Now, in the case the Court will decide, the Section 230 issue isn't that there was content that incited immediate violence (which wouldn't have been a legal problem for the website), the issue is that the website's algorithm actively promoted that content to some of its users.
So now that's no longer just user generated content, but website generated promotion of content inciting violence, and that's a liability issue for the website now, which the Court will probably hold isn't covered under Section 230.
Section 230 is for websites that publish user-generated content (including the one we're on right now) and states that these sites cannot be held legally liable for the content that was generated by their users, be that in terms of defamation, copyright infringement, or (in this case) incitement to violence.
Why would i think that?
Because that is exactly the situation big tech is in.
There is a that is exploring the core question of whether companies should be legally responsible for the content their users post.
If the current model was perfectly compatible, this question wouldn't be in front of the Supreme Court.
Your comment implies there is no real tension in upholding these two core principles - of not being accountable for content but also having the right to moderate content.
The facts say otherwise.