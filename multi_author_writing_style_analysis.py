# -*- coding: utf-8 -*-
"""Multi-Author Writing Style Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10HSKr8ka7Nivxhipu6fg1TovVV8j4hOc

# Data Preparation
"""

# Install necessary dependencies
!pip install nltk pandas tqdm requests

import os
import json
import pandas as pd
import requests
import zipfile
from tqdm import tqdm
from pathlib import Path

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Define dataset URL and paths
ZIP_URL = "https://zenodo.org/records/14891299/files/pan25-multi-author-analysis.zip?download=1"
ZIP_PATH = "pan25-multi-author-analysis.zip"
EXTRACT_FOLDER = "pan25_data"

# Download the ZIP file
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get("content-length", 0))

    with open(save_path, "wb") as file, tqdm(
        desc="Downloading Dataset",
        total=total_size,
        unit="B",
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for chunk in response.iter_content(chunk_size=1024):
            file.write(chunk)
            bar.update(len(chunk))

# Unzip the dataset
def unzip_file(zip_path, extract_folder):
    os.makedirs(extract_folder, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_folder)
    print(f"Extracted to {extract_folder}")

# Download and unzip dataset
download_file(ZIP_URL, ZIP_PATH)
unzip_file(ZIP_PATH, EXTRACT_FOLDER)

# Define the root directory where the extracted folders exist
DATASET_ROOT = Path(EXTRACT_FOLDER)

# Define dataset splits
SPLITS = ["train", "validation"]
DIFFICULTY_LEVELS = ["easy", "medium", "hard"]

# Read and process the dataset
def read_problem_data(problem_file):
    text_path = problem_file
    json_path = text_path.with_name(f"truth-{text_path.stem}.json")

    # Read text
    with open(text_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    # Read labels
    with open(json_path, "r", encoding="utf-8") as f:
        truth = json.load(f)

    return text, truth["changes"]

# Define the file path
file_path = "pan25_data/hard/train/problem-3207.txt"

# New content to write
new_content = """Intelligence agencies aren't spying about pandemics folks.
The world has many interacting medical based organizations that monitor pandemics all the time.
Looking to the DoD or CIA or any other intelligence org to be the source of pandemic intelligence other than from a defense angle is a not their primary role.
Again, by rule, the NCMI is the lead agency on pandemic intelligence for the United States.
It's not an exception.
Additionally, the NCMI doesn't solely view medical intelligence from the lens of a "defense angle" as you believe.
But regardless, my comment originally rebutted your comment "Intelligence agencies aren't spying about pandemics folks."
Yes it does.
It's called the NCMI.
a.
There will be a unified Defense community for medical intelligence activities; those activities will be executed in a coordinated and coherent manner to effectively respond to U.S. intelligence priorities in support of national security objectives, and in accordance with all applicable laws, Presidential directives, DoD issuances, and Director of National Intelligence (DNI) guidelines.
From multiple angles; not solely based on a defense posture.
Where's your DOD policy citation that it's solely for defense posture?
"""

# Overwrite the file
with open(file_path, "w", encoding="utf-8") as f:
    f.write(new_content)

print(f"Successfully updated {file_path}")

def process_dataset(split):
    dataset = []
    total_mismatched_pairs = 0  # Counter for total mismatched pairs

    for difficulty in DIFFICULTY_LEVELS:
        print(f"Processing {split} set - {difficulty} difficulty...")
        folder_path = DATASET_ROOT / difficulty / split

        for problem_file in tqdm(folder_path.glob("problem-*.txt")):
            with open(problem_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Strip whitespace and remove empty lines
            sentences = [line.strip() for line in lines if line.strip()]

            # Read corresponding labels
            _, labels = read_problem_data(problem_file)

            # Check for label mismatch
            if len(labels) != len(sentences) - 1:
                mismatch_count = abs(len(labels) - (len(sentences) - 1))
                total_mismatched_pairs += mismatch_count

                print(f"\n Label mismatch in {problem_file}")
                print(f" Total Sentences (Lines): {len(sentences)} | Expected Labels: {len(labels)}")
                print(f" Mismatched Pairs in This File: {mismatch_count}")

                # Print mismatched sentence-label pairs
                print("\n--- Mismatched Sentence Pairs ---")
                for i in range(min(len(sentences) - 1, len(labels), 5)):
                    print(f"[{i}] {sentences[i]} || {sentences[i+1]} --> Label: {labels[i]}")

                print("--- End of Mismatch ---\n")
                continue

            # Merge consecutive sentences for training
            for i in range(len(sentences) - 1):
                dataset.append({
                    "difficulty": difficulty,
                    "split": split,
                    "problem_id": problem_file.stem,
                    "sentence1": sentences[i],
                    "sentence2": sentences[i + 1],
                    "label": labels[i]
                })

    print(f"\n Total Mismatched Sentence-Label Pairs Across All Files: {total_mismatched_pairs}")
    return dataset
# Process both training and validation sets
train_data = process_dataset("train")
valid_data = process_dataset("validation")

# Convert to DataFrame and save as CSV
train_df = pd.DataFrame(train_data)
valid_df = pd.DataFrame(valid_data)

train_df.to_csv("train_dataset.csv", index=False)
valid_df.to_csv("valid_dataset.csv", index=False)

print("Training and validation datasets saved successfully!")

# Load processed datasets
train_df = pd.read_csv("train_dataset.csv")
valid_df = pd.read_csv("valid_dataset.csv")

# Show unique label counts
print("\n Label Distribution in Training Set:")
print(train_df['label'].value_counts())

print("\n Label Distribution in Validation Set:")
print(valid_df['label'].value_counts())

"""# Feature Extraction"""

import spacy
import textstat
import numpy as np
from collections import Counter

# Load SpaCy NLP model
nlp = spacy.load("en_core_web_sm")

# Feature extraction function
def extract_features(text):
    doc = nlp(text)

    # Lexical Features
    word_lengths = [len(token.text) for token in doc if token.is_alpha]
    avg_word_length = np.mean(word_lengths) if word_lengths else 0

    function_words = set(["the", "is", "and", "but", "or", "because", "as", "that"])
    function_word_count = sum(1 for token in doc if token.text.lower() in function_words)

    # Syntactic Features
    pos_counts = Counter(token.pos_ for token in doc)
    num_nouns = pos_counts.get("NOUN", 0)
    num_verbs = pos_counts.get("VERB", 0)

    sentence_lengths = [len(sent.text.split()) for sent in doc.sents]
    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0

    # Stylometric Features
    punctuation_count = sum(1 for token in text if token in ".,!?;:-")
    readability_score = textstat.flesch_reading_ease(text)

    return [
        avg_word_length, function_word_count, num_nouns, num_verbs,
        avg_sentence_length, punctuation_count, readability_score
    ]

# Apply feature extraction to dataset
train_df["features1"] = train_df["sentence1"].apply(extract_features)
train_df["features2"] = train_df["sentence2"].apply(extract_features)
valid_df["features1"] = valid_df["sentence1"].apply(extract_features)
valid_df["features2"] = valid_df["sentence2"].apply(extract_features)

print(" Feature extraction completed!")

import pickle

# Save extracted features along with dataset
train_df.to_csv("train_features.csv", index=False)
valid_df.to_csv("valid_features.csv", index=False)

# Save as Pickle for faster loading
with open("train_features.pkl", "wb") as f:
    pickle.dump(train_df, f)

with open("valid_features.pkl", "wb") as f:
    pickle.dump(valid_df, f)

from google.colab import drive
drive.mount('/content/drive')
train_df.to_pickle("/content/drive/My Drive/train_features.pkl")
valid_df.to_pickle("/content/drive/My Drive/valid_features.pkl")

"""# Model Enhenced"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.01  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

# Evaluate on validation set
valid_acc, valid_f1 = evaluate_model_batched(model, valid_df, dataset_name="Validation")

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Official Evaluator"""

import requests
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define save path in your Drive
save_path = "/content/drive/My Drive/evaluator.py"

# GitHub raw URL of the evaluator script
url = "https://raw.githubusercontent.com/pan-webis-de/pan-code/master/clef25/multi-author-analysis/evaluator/evaluator.py"

# Download and save
response = requests.get(url)
with open(save_path, "w") as f:
    f.write(response.text)

print("evaluator.py saved to:", save_path)

import os
import requests
import zipfile

url = "https://zenodo.org/records/14891299/files/pan25-multi-author-analysis.zip?download=1"
zip_path = "/content/drive/My Drive/pan25.zip"
extract_path = "/content/drive/My Drive/pan25_data"

print("Downloading PAN25 dataset...")
r = requests.get(url)
with open(zip_path, "wb") as f:
    f.write(r.content)

print("Unzipping...")
with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(extract_path)

print("Extracted to", extract_path)
os.listdir(extract_path)

import os
import json
import torch
import pickle
import numpy as np
import pandas as pd
from tqdm import tqdm
import spacy
import textstat
from collections import Counter
from transformers import AutoTokenizer, AutoModel
from google.colab import drive

drive.mount('/content/drive')

# Load handcrafted features
with open("/content/drive/My Drive/train_features.pkl", "rb") as f:
    train_df = pickle.load(f)

with open("/content/drive/My Drive/valid_features.pkl", "rb") as f:
    valid_df = pickle.load(f)

# Load tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load SpaCy for feature extraction
nlp = spacy.load("en_core_web_sm")

#HANDCRAFTED FEATURE FUNCTION
def extract_features(text):
    doc = nlp(text)
    word_lengths = [len(token.text) for token in doc if token.is_alpha]
    avg_word_length = np.mean(word_lengths) if word_lengths else 0
    function_words = {"the", "is", "and", "but", "or", "because", "as", "that"}
    function_word_count = sum(1 for token in doc if token.text.lower() in function_words)
    pos_counts = Counter(token.pos_ for token in doc)
    num_nouns = pos_counts.get("NOUN", 0)
    num_verbs = pos_counts.get("VERB", 0)
    sentence_lengths = [len(sent.text.split()) for sent in doc.sents]
    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
    punctuation_count = sum(1 for token in text if token in ".,!?;:-")
    readability_score = textstat.flesch_reading_ease(text)
    return [avg_word_length, function_word_count, num_nouns, num_verbs,
            avg_sentence_length, punctuation_count, readability_score]

#MODEL DEFINITION
import torch.nn as nn
import torch.nn.functional as F

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)
        self.classifier = nn.Linear(style_dim * 2, 1)
        self.ortho_lambda = 0.01

    def forward(self, input_ids, attention_mask, features1, features2):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = outputs.last_hidden_state[:, 0, :]
        content_embedding = self.content_proj(embeddings)
        style_embedding = self.style_proj(embeddings)
        combined_features = torch.cat([features1, features2], dim=-1)
        feature_embedding = self.feature_proj(combined_features)
        style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
        content_weight = 1 - style_weight
        content_embedding = content_embedding * content_weight
        style_embedding = style_embedding * style_weight
        content_norm = F.normalize(content_embedding, dim=-1)
        style_norm = F.normalize(style_embedding, dim=-1)
        ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2
        style_feature_loss = 1 - F.cosine_similarity(style_norm, F.normalize(feature_embedding, dim=-1)).mean()
        combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
        logits = self.classifier(combined_rep).squeeze(-1)
        return logits, ortho_loss, style_feature_loss

#LOAD TRAINED MODEL
model = FactorizedAttentionModel().to("cuda" if torch.cuda.is_available() else "cpu")
checkpoint = torch.load("/content/drive/My Drive/enhenced_factorized_attention_model.pt", map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
print("Model loaded.")

#PREDICTION + JSON OUTPUT
def predict_and_save(input_folder, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    for file_name in tqdm(sorted(os.listdir(input_folder))):
        if not file_name.startswith("problem-") or not file_name.endswith(".txt"): continue
        problem_id = file_name[8:-4]
        with open(os.path.join(input_folder, file_name), "r") as f:
            sentences = [line.strip() for line in f if line.strip()]

        changes = []
        for i in range(len(sentences) - 1):
            sent1, sent2 = sentences[i], sentences[i + 1]
            encoding = tokenizer(sent1, sent2, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
            f1 = torch.tensor([extract_features(sent1)], dtype=torch.float32).to(device)
            f2 = torch.tensor([extract_features(sent2)], dtype=torch.float32).to(device)
            logits, _, _ = model(encoding["input_ids"], encoding["attention_mask"], f1, f2)
            pred = torch.sigmoid(logits).item() > 0.5
            changes.append(int(pred))

        out_path = os.path.join(output_folder, f"solution-problem-{problem_id}.json")
        with open(out_path, "w") as f:
            json.dump({"changes": changes}, f, indent=2)

    print(f"Predictions saved to: {output_folder}")

predict_and_save("/content/drive/My Drive/pan25_data/easy/validation", "/content/drive/My Drive/pan25_output/easy")
predict_and_save("/content/drive/My Drive/pan25_data/medium/validation", "/content/drive/My Drive/pan25_output/medium")
predict_and_save("/content/drive/My Drive/pan25_data/hard/validation", "/content/drive/My Drive/pan25_output/hard")

import shutil

os.makedirs("/content/drive/My Drive/eval_result", exist_ok=True)

src_dir = "/content/drive/My Drive/pan25_data/easy/validation"
dest_dir = "/content/drive/My Drive/pan25_data/easy"

for filename in os.listdir(src_dir):
    if filename.startswith("truth-problem-"):
        shutil.move(
            os.path.join(src_dir, filename),
            os.path.join(dest_dir, filename)
        )

src_dir = "/content/drive/My Drive/pan25_data/medium/validation"
dest_dir = "/content/drive/My Drive/pan25_data/medium"

for filename in os.listdir(src_dir):
    if filename.startswith("truth-problem-"):
        shutil.move(
            os.path.join(src_dir, filename),
            os.path.join(dest_dir, filename)
        )

src_dir = "/content/drive/My Drive/pan25_data/hard/validation"
dest_dir = "/content/drive/My Drive/pan25_data/hard"

for filename in os.listdir(src_dir):
    if filename.startswith("truth-problem-"):
        shutil.move(
            os.path.join(src_dir, filename),
            os.path.join(dest_dir, filename)
        )

!python "/content/drive/My Drive/evaluator.py" \
  --predictions "/content/drive/My Drive/pan25_output" \
  --truth "/content/drive/My Drive/pan25_data" \
  --output "/content/drive/My Drive/eval_result"

"""# Tuning ortho_lambda = 0.1"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model0.1.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model0.1.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Tuning ortho_lambda = 1"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model1.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model1.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Tuning alpha = 2"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.01  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   2 * style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model2.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model2.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Ablation Study CLS-only"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class SimpleCLSClassifier(nn.Module):
    def __init__(self, model_name="microsoft/deberta-v3-base", hidden_dim=768):
        super(SimpleCLSClassifier, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, input_ids, attention_mask, *args, **kwargs):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(cls_embedding).squeeze(-1)
        return logits

from torch.optim import AdamW
import torch.nn.functional as F

def train_simple_classifier(model, train_loader, device, epochs=3, lr=1e-5):
    model.train()
    optimizer = AdamW(model.parameters(), lr=lr)

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].float().to(device)

            logits = model(input_ids, attention_mask)
            loss = F.binary_cross_entropy_with_logits(logits, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize model
model = SimpleCLSClassifier().to(device)

# Train
train_simple_classifier(model, train_loader, device, epochs=3)

from torch.optim import AdamW

# Make sure 'model' is already defined
optimizer = AdamW(model.parameters(), lr=1e-5)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/basic_model.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/basic_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = SimpleCLSClassifier().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score

def evaluate_simple_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = (torch.sigmoid(logits) > 0.5).cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_simple_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Plotting"""

import matplotlib.pyplot as plt
from google.colab import files

lambdas = [0.01, 0.1, 1]
f1_easy = [0.9224, 0.9034, 0.9341]
f1_medium = [0.8134, 0.8208, 0.8069]
f1_hard = [0.7808, 0.7904, 0.7719]

plt.figure(figsize=(8, 5))
plt.plot(lambdas, f1_easy, marker='o', label='Easy')
plt.plot(lambdas, f1_medium, marker='o', label='Medium')
plt.plot(lambdas, f1_hard, marker='o', label='Hard')

plt.xscale('log')
plt.xticks(lambdas, labels=["0.01", "0.1", "1"])
plt.xlabel('λ (Orthogonality Loss)')
plt.ylabel('Macro F1-score')
plt.title('Effect of Orthogonality Weight on Macro F1')
plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()


filename = 'orthogonality_f1_plot.png'
plt.savefig(filename)
files.download(filename)

import matplotlib.pyplot as plt

dims = [64, 128, 256, 512]
f1_easy = [0.9365, 0.9034, 0.9242, 0.9295]
f1_medium = [0.7954, 0.8208, 0.8276, 0.8084]
f1_hard = [0.7595, 0.7904, 0.7841, 0.7589]

plt.figure(figsize=(8, 5))
plt.plot(dims, f1_easy, marker='o', label='Easy')
plt.plot(dims, f1_medium, marker='o', label='Medium')
plt.plot(dims, f1_hard, marker='o', label='Hard')

plt.xticks(dims, labels=["64", "128", "256", "512"])
plt.xlabel('dimension of style embedding')
plt.ylabel('Macro F1-score')
plt.title('Effect of style dimension on Macro F1')
plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.tight_layout()

filename = 'style_dim_f1_plot.png'
plt.savefig(filename)
files.download(filename)

import matplotlib.pyplot as plt
import numpy as np

f1_scores = [
    [0.9034, 0.8208, 0.7904],
    [0.9058, 0.7981, 0.7274],
    [0.9113, 0.8184, 0.7580],
]


methods = ['Feature Set 1', 'Feature Set 2', 'Feature Set 3']
difficulty = ['Easy', 'Medium', 'Hard']
colors = ['#66c2a5', '#fc8d62', '#8da0cb']


x = np.arange(len(methods))
width = 0.25


plt.figure(figsize=(8, 5))
for i in range(len(difficulty)):
    f1 = [method[i] for method in f1_scores]
    plt.bar(x + i*width - width, f1, width, label=difficulty[i], color=colors[i])

plt.ylabel('Macro F1-score')
plt.title('Macro F1-score for Different Feature Extraction Methods')
plt.xticks(x, methods)
plt.ylim(0.7, 1)
plt.legend(title='Validation Set')
plt.grid(axis='y', linestyle='--', linewidth=0.5)
plt.tight_layout()

filename = 'feature_extraction_f1_plot.png'
plt.savefig(filename)
files.download(filename)

!apt-get update
!apt-get install -y texlive-latex-base texlive-fonts-recommended texlive-latex-extra

!apt-get install -y texlive-fonts-recommended texlive-fonts-extra texlive-latex-extra

import pandas as pd
import matplotlib.pyplot as plt
from google.colab import files
import matplotlib as mpl
import numpy as np

mpl.use('pgf')

mpl.rcParams.update({
    "pgf.texsystem": "pdflatex",
    "text.usetex": True,
    "font.family": "serif",
    "font.serif": ["Times New Roman"],

    "pgf.preamble": "\\usepackage{newtxtext}\\usepackage{newtxmath}",

    "axes.labelsize": 8,
    "font.size": 9,
    "legend.fontsize": 8,
    "xtick.labelsize": 10,
    "ytick.labelsize": 10,

    "figure.figsize": (4.5, 4),
    "lines.linewidth": 1.0,
    "grid.linestyle": '--',
    "grid.linewidth": 0.5,
})


f1_scores = [
    [0.9034, 0.8208, 0.7904],
    [0.9058, 0.7981, 0.7274],
    [0.9113, 0.8184, 0.7580],
]
methods = ['Feature set 1', 'Feature set 2', 'Feature set 3']
difficulty = ['Easy', 'Medium', 'Hard']
colors = ['#66c2a5', '#fc8d62', '#8da0cb']
hatches = ['///', '+++', '|||']


y = np.arange(len(methods))
width = 0.25

fig, ax = plt.subplots()
for i, level in enumerate(difficulty):
    values = [scores[i] for scores in f1_scores]
    bars = ax.bar(y + i*width - width, values, width, label=rf'\textbf{{{level}}}',
                  color=colors[i], hatch=hatches[i], edgecolor='black')


ax.set_ylabel(r'Macro F$_1$-score', fontsize=10)
ax.set_title(r'Macro F$_1$-score for Different Feature Extraction Methods', fontsize=12)
ax.set_xticks(y)
ax.set_xticklabels(methods)
ax.set_ylim(0.7, 1)
ax.legend(title=r'Validation set', loc='best')
ax.grid(axis='y')
plt.tight_layout()

plt.savefig('feature_extraction_f1_plot.pgf')
plt.savefig('feature_extraction_f1_plot.pdf')
files.download('feature_extraction_f1_plot.pdf')

gmpl_backend = 'pgf'
mpl.use(gmpl_backend)


mpl.rcParams.update({

    "pgf.texsystem": "pdflatex",
    "text.usetex": True,
    "font.family": "serif",
    "font.serif": ["Times New Roman"],

    "pgf.preamble": "\\usepackage{newtxtext}\\usepackage{newtxmath}",

    "axes.labelsize": 8,
    "font.size": 8,
    "legend.fontsize": 6,
    "xtick.labelsize": 6,
    "ytick.labelsize": 6,

    "figure.figsize": (3.39, 2.53),
    "lines.linewidth": 1.0,
    "grid.linestyle": '--',
    "grid.linewidth": 0.5,
})


lambdas = [0.01, 0.1, 1]
f1_easy   = [0.9224, 0.9034, 0.9341]
f1_medium = [0.8134, 0.8208, 0.8069]
f1_hard   = [0.7808, 0.7904, 0.7719]


ticks = [0.01, 0.1, 1]
plt.plot(lambdas, f1_easy,   'o-', label="Easy")
plt.plot(lambdas, f1_medium, 's--', label="Medium")
plt.plot(lambdas, f1_hard,   'd-.', label="Hard")


plt.xscale('log')
plt.xticks(ticks, labels=["0.01", "0.1", "1"])


plt.xlabel(r'$\lambda$ (Orthogonality Loss)')
plt.ylabel(r'Macro F$_1$-score')
plt.title(r'Effect of Orthogonality Weight on Macro F$_1$')


plt.legend(loc='best')
plt.grid(True, which='both')
plt.tight_layout()


plt.savefig('orthogonality_f1_plot.pgf')
plt.savefig('orthogonality_f1_plot.pdf')
files.download('orthogonality_f1_plot.pdf')

"""# McNemar’s test"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

def evaluate_model_and_collect_preds(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            output = model(input_ids, attention_mask, features1, features2)

            # Check if model returns logits only or (logits, loss1, loss2)
            if isinstance(output, tuple):
                logits = output[0]
            else:
                logits = output

            preds = (torch.sigmoid(logits) > 0.5).long().cpu().numpy()

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")

    return all_preds, all_labels

import torch.nn as nn
from transformers import AutoModel

class SimpleCLSClassifier(nn.Module):
    def __init__(self, model_name="microsoft/deberta-v3-base", hidden_dim=768):
        super(SimpleCLSClassifier, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, input_ids, attention_mask, *args, **kwargs):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(cls_embedding).squeeze(-1)
        return logits

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/basic_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
simple_model = SimpleCLSClassifier().to(device)
optimizer = AdamW(simple_model.parameters(), lr=1e-5)

# Load saved weights
simple_model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

simple_model.eval()

print(" Model loaded successfully!")

import torch.nn as nn
from transformers import AutoModel
import torch.nn.functional as F

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model0.1.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
factorized_model = FactorizedAttentionModel().to(device)
optimizer = AdamW(factorized_model.parameters(), lr=1e-5)

# Load saved weights
factorized_model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

factorized_model.eval()

print(" Model loaded successfully!")

# Collect predictions
simple_model_preds, y_true = evaluate_model_and_collect_preds(simple_model, valid_df, dataset_name="Simple Model")
factorized_preds, _ = evaluate_model_and_collect_preds(factorized_model, valid_df, dataset_name="Factorized Model")

# Run McNemar’s test
from statsmodels.stats.contingency_tables import mcnemar
import numpy as np

y_true = np.array(y_true)
simple_preds = np.array(simple_model_preds)
factorized_preds = np.array(factorized_preds)

b = np.sum((simple_preds != y_true) & (factorized_preds == y_true))
c = np.sum((simple_preds == y_true) & (factorized_preds != y_true))
table = [[0, b], [c, 0]]

result = mcnemar(table, exact=True)
print("McNemar’s test p-value:", result.pvalue)

from statsmodels.stats.contingency_tables import mcnemar
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def run_mcnemar(simple_model, factorized_model, df, dataset_name):
    print(f"\n {dataset_name} Set")

    # Get predictions and true labels
    simple_preds, y_true = evaluate_model_and_collect_preds(simple_model, df, dataset_name="Simple")
    factorized_preds, _ = evaluate_model_and_collect_preds(factorized_model, df, dataset_name="Factorized")

    y_true = np.array(y_true)
    simple_preds = np.array(simple_preds)
    factorized_preds = np.array(factorized_preds)

    # McNemar's contingency table
    a1 = np.sum(y_true == factorized_preds)
    a2 = np.sum(y_true != factorized_preds)
    b = np.sum((simple_preds != y_true) & (factorized_preds == y_true))
    c = np.sum((simple_preds == y_true) & (factorized_preds != y_true))
    table = [[0, b], [c, 0]]

    # Run McNemar's Test
    result = mcnemar(table, exact=True)

    # Classification metrics
    for name, preds in [("Simple", simple_preds), ("Factorized", factorized_preds)]:
        precision = precision_score(y_true, preds)
        recall = recall_score(y_true, preds)
        print(f" {name} Model — Precision: {precision:.4f}, Recall: {recall:.4f}")

    print(f" McNemar’s Test: a1 = {a1}, a2 = {a2}, b = {b}, c = {c}, p-value = {result.pvalue:.4e}")
    return result.pvalue

# Subsets
easy_df = valid_df[valid_df["difficulty"] == "easy"]
medium_df = valid_df[valid_df["difficulty"] == "medium"]
hard_df = valid_df[valid_df["difficulty"] == "hard"]

# Run McNemar’s test per difficulty level
p_easy = run_mcnemar(simple_model, factorized_model, easy_df, "Easy")
p_medium = run_mcnemar(simple_model, factorized_model, medium_df, "Medium")
p_hard = run_mcnemar(simple_model, factorized_model, hard_df, "Hard")

"""# Tuning style_dim = 64"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=64, content_dim=64, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model64.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model64.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Tuning style_dim = 256"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=256, content_dim=256, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model256.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model256.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Tuning style_dim = 512"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=512, content_dim=512, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced_factorized_attention_model512.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model512.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Enhanced 2.0"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name="microsoft/deberta-v3-base", hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # DeBERTa backbone
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized projections
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Handcrafted feature projection
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Learnable gating mechanism for style-content separation
        self.gate_layer = nn.Linear(feature_dim * 2, 1)

        # Classification head
        self.classifier = nn.Linear(style_dim * 2, 1)

        # Regularization layers
        self.dropout = nn.Dropout(0.2)
        self.norm = nn.LayerNorm(style_dim)

        # Loss weights
        self.ortho_lambda = 0.1
        self.contrastive_lambda = 0.01
        self.style_feature_lambda = 1.0

    def forward(self, input_ids, attention_mask, features1, features2, labels=None):
        # Encode input
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = outputs.last_hidden_state[:, 0, :]

        # Project to factorized embeddings
        content_embedding = self.content_proj(embeddings)
        style_embedding = self.style_proj(embeddings)

        # Handcrafted feature processing
        combined_features = torch.cat([features1, features2], dim=-1)
        feature_embedding = self.feature_proj(combined_features)

        # Learnable gate: controls style vs. content emphasis
        gate = torch.sigmoid(self.gate_layer(combined_features))
        content_embedding = content_embedding * (1 - gate)
        style_embedding = style_embedding * gate

        # Apply dropout and normalization
        style_embedding = self.norm(self.dropout(style_embedding))
        content_embedding = self.norm(self.dropout(content_embedding))

        # Orthogonality loss between content and style
        content_norm = F.normalize(content_embedding, dim=-1)
        style_norm = F.normalize(style_embedding, dim=-1)
        ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

        # Feature-style alignment loss
        style_feature_loss = 1 - F.cosine_similarity(
            F.normalize(style_embedding, dim=-1),
            F.normalize(feature_embedding, dim=-1)
        ).mean()

        # Final classification
        combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
        logits = self.classifier(combined_rep).squeeze(-1)

        # Contrastive loss (optional, only if labels are provided)
        contrastive_loss = torch.tensor(0.0, device=logits.device)
        if labels is not None:
            # Assume binary labels: 1 if same author, 0 if not
            contrastive_loss = self.compute_contrastive_loss(style_embedding, labels)

        return logits, ortho_loss, style_feature_loss, contrastive_loss

    def compute_contrastive_loss(self, embeddings, labels, margin=1.0):
        # Pairwise contrastive loss over batch
        batch_size = embeddings.size(0)
        loss = 0.0
        pairs = 0

        for i in range(batch_size):
            for j in range(i + 1, batch_size):
                dist = F.pairwise_distance(embeddings[i:i+1], embeddings[j:j+1])
                target = (labels[i] == labels[j]).float()
                loss += (1 - target) * dist.pow(2) + target * F.relu(margin - dist).pow(2)
                pairs += 1

        return loss / pairs if pairs > 0 else torch.tensor(0.0, device=embeddings.device)

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

def train_model(model, train_loader, optimizer, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0
        total_contrastive_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Move data to device
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss, contrastive_loss = model(
                input_ids, attention_mask, features1, features2, labels
            )

            # Compute losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (
                cls_loss
                + model.ortho_lambda * ortho_loss
                + model.style_feature_lambda * style_feature_loss
                + model.contrastive_lambda * contrastive_loss
            )

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Track loss values
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()
            total_contrastive_loss += contrastive_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss:              {total_loss / num_batches:.4f}")
        print(f"Classification Loss:     {total_cls_loss / num_batches:.4f}")
        print(f"Orthogonality Loss:      {total_ortho_loss / num_batches:.4f}")
        print(f"Feature Alignment Loss:  {total_feat_loss / num_batches:.4f}")
        print(f"Contrastive Loss:        {total_contrastive_loss / num_batches:.4f}\n")


train_model(model, train_loader, optimizer, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/enhenced2.0_factorized_attention_model.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced2.0_factorized_attention_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            logits, _, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Save weights only"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

model_name = "microsoft/deberta-v3-base"
class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model0.1.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/small_factorized_attention_model.pt"

# Save model weights
torch.save(model.state_dict(), model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = FactorizedAttentionModel().to(device)
model.load_state_dict(torch.load("/content/drive/My Drive/small_factorized_attention_model.pt"))
model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Content only"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel, AutoConfig

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)
        hidden_dim = AutoConfig.from_pretrained(model_name).hidden_size

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.01  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([content_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            # Forward pass
            logits, ortho_loss, style_feature_loss = model(
                input_ids, attention_mask, features1, features2
            )

            # Calculate losses
            cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
            loss = (cls_loss +
                   model.ortho_lambda * ortho_loss +
                   style_feature_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/content_only_factorized_attention_model.pt"

# Save model weights
torch.save(model.state_dict(), model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = FactorizedAttentionModel().to(device)
model.load_state_dict(torch.load("/content/drive/My Drive/content_only_factorized_attention_model.pt"))
model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Feature Re-extraction"""

# Install necessary dependencies
!pip install nltk pandas tqdm requests

import os
import json
import pandas as pd
import requests
import zipfile
from tqdm import tqdm
from pathlib import Path

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Define dataset URL and paths
ZIP_URL = "https://zenodo.org/records/14891299/files/pan25-multi-author-analysis.zip?download=1"
ZIP_PATH = "pan25-multi-author-analysis.zip"
EXTRACT_FOLDER = "pan25_data"

# Download the ZIP file
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get("content-length", 0))

    with open(save_path, "wb") as file, tqdm(
        desc="Downloading Dataset",
        total=total_size,
        unit="B",
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for chunk in response.iter_content(chunk_size=1024):
            file.write(chunk)
            bar.update(len(chunk))

# Unzip the dataset
def unzip_file(zip_path, extract_folder):
    os.makedirs(extract_folder, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_folder)
    print(f"Extracted to {extract_folder}")

# Download and unzip dataset
download_file(ZIP_URL, ZIP_PATH)
unzip_file(ZIP_PATH, EXTRACT_FOLDER)

# Define the root directory where the extracted folders exist
DATASET_ROOT = Path(EXTRACT_FOLDER)

# Define dataset splits
SPLITS = ["train", "validation"]
DIFFICULTY_LEVELS = ["easy", "medium", "hard"]

# Read and process the dataset
def read_problem_data(problem_file):
    text_path = problem_file
    json_path = text_path.with_name(f"truth-{text_path.stem}.json")

    # Read text
    with open(text_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    # Read labels
    with open(json_path, "r", encoding="utf-8") as f:
        truth = json.load(f)

    return text, truth["changes"]

# Define the file path
file_path = "pan25_data/hard/train/problem-3207.txt"

# New content to write
new_content = """Intelligence agencies aren't spying about pandemics folks.
The world has many interacting medical based organizations that monitor pandemics all the time.
Looking to the DoD or CIA or any other intelligence org to be the source of pandemic intelligence other than from a defense angle is a not their primary role.
Again, by rule, the NCMI is the lead agency on pandemic intelligence for the United States.
It's not an exception.
Additionally, the NCMI doesn't solely view medical intelligence from the lens of a "defense angle" as you believe.
But regardless, my comment originally rebutted your comment "Intelligence agencies aren't spying about pandemics folks."
Yes it does.
It's called the NCMI.
a.
There will be a unified Defense community for medical intelligence activities; those activities will be executed in a coordinated and coherent manner to effectively respond to U.S. intelligence priorities in support of national security objectives, and in accordance with all applicable laws, Presidential directives, DoD issuances, and Director of National Intelligence (DNI) guidelines.
From multiple angles; not solely based on a defense posture.
Where's your DOD policy citation that it's solely for defense posture?
"""

# Overwrite the file
with open(file_path, "w", encoding="utf-8") as f:
    f.write(new_content)

print(f"Successfully updated {file_path}")

def process_dataset(split):
    dataset = []
    total_mismatched_pairs = 0  # Counter for total mismatched pairs

    for difficulty in DIFFICULTY_LEVELS:
        print(f"Processing {split} set - {difficulty} difficulty...")
        folder_path = DATASET_ROOT / difficulty / split

        for problem_file in tqdm(folder_path.glob("problem-*.txt")):
            with open(problem_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Strip whitespace and remove empty lines
            sentences = [line.strip() for line in lines if line.strip()]

            # Read corresponding labels
            _, labels = read_problem_data(problem_file)

            # Check for label mismatch
            if len(labels) != len(sentences) - 1:
                mismatch_count = abs(len(labels) - (len(sentences) - 1))
                total_mismatched_pairs += mismatch_count

                print(f"\n Label mismatch in {problem_file}")
                print(f" Total Sentences (Lines): {len(sentences)} | Expected Labels: {len(labels)}")
                print(f" Mismatched Pairs in This File: {mismatch_count}")

                # Print mismatched sentence-label pairs
                print("\n--- Mismatched Sentence Pairs ---")
                for i in range(min(len(sentences) - 1, len(labels), 5)):
                    print(f"[{i}] {sentences[i]} || {sentences[i+1]} --> Label: {labels[i]}")

                print("--- End of Mismatch ---\n")
                continue

            # Merge consecutive sentences for training
            for i in range(len(sentences) - 1):
                dataset.append({
                    "difficulty": difficulty,
                    "split": split,
                    "problem_id": problem_file.stem,
                    "sentence1": sentences[i],
                    "sentence2": sentences[i + 1],
                    "label": labels[i]
                })

    print(f"\n Total Mismatched Sentence-Label Pairs Across All Files: {total_mismatched_pairs}")
    return dataset
# Process both training and validation sets
train_data = process_dataset("train")
valid_data = process_dataset("validation")

# Convert to DataFrame and save as CSV
train_df = pd.DataFrame(train_data)
valid_df = pd.DataFrame(valid_data)

train_df.to_csv("train_dataset.csv", index=False)
valid_df.to_csv("valid_dataset.csv", index=False)

print("Training and validation datasets saved successfully!")

# Load processed datasets
train_df = pd.read_csv("train_dataset.csv")
valid_df = pd.read_csv("valid_dataset.csv")

# Show unique label counts
print("\n Label Distribution in Training Set:")
print(train_df['label'].value_counts())

print("\n Label Distribution in Validation Set:")
print(valid_df['label'].value_counts())

import spacy
import textstat
import numpy as np
from collections import Counter
import re

# Load SpaCy English model
nlp = spacy.load("en_core_web_sm")

# Define Reddit-style elements
slang = {"lol", "brb", "idk", "imo", "smh", "tbh", "rofl", "lmao"}
uppercase_pattern = re.compile(r"[A-Z]{2,}")
url_pattern = re.compile(r"https?://|www\.")

# Define common pronouns used as subjects
subject_pronouns = {"i", "we", "you", "he", "she", "they", "it"}

def extract_features(text):
    doc = nlp(text)

    # Stylometric & Reddit-style
    punctuation_count = sum(1 for char in text if char in ".,!?;:-")
    uppercase_ratio = len(uppercase_pattern.findall(text)) / (len(text) + 1)
    url_count = len(url_pattern.findall(text))
    slang_count = sum(1 for token in doc if token.text.lower() in slang)

    # Lexical
    tokens = [token.text.lower() for token in doc if token.is_alpha]
    type_token_ratio = len(set(tokens)) / (len(tokens) + 1) if tokens else 0

    # Function word usage
    function_words = {"the", "is", "and", "but", "or", "because", "as", "that"}
    function_word_count = sum(1 for token in doc if token.text.lower() in function_words)

    # Sentence complexity
    sentence_lengths = [len(sent) for sent in doc.sents]
    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
    readability_score = textstat.flesch_reading_ease(text)

    # Subject structure features
    has_subject = 0
    is_pronoun_subject = 0
    is_noun_subject = 0
    for token in doc:
        if token.dep_ in {"nsubj", "nsubjpass"}:
            has_subject = 1
            if token.text.lower() in subject_pronouns:
                is_pronoun_subject = 1
            else:
                is_noun_subject = 1
            break

    # Final feature vector
    return [
        function_word_count,
        punctuation_count,
        type_token_ratio,
        avg_sentence_length,
        readability_score,
        uppercase_ratio,
        slang_count,
        url_count,
        has_subject,
        is_pronoun_subject,
        is_noun_subject
    ]

# Apply feature extraction to dataset
train_df["features1"] = train_df["sentence1"].apply(extract_features)
train_df["features2"] = train_df["sentence2"].apply(extract_features)
valid_df["features1"] = valid_df["sentence1"].apply(extract_features)
valid_df["features2"] = valid_df["sentence2"].apply(extract_features)

print(" Feature extraction completed!")

import pickle

# Save extracted features along with dataset
train_df.to_csv("enhanced_train_features.csv", index=False)
valid_df.to_csv("enhanced_valid_features.csv", index=False)

# Save as Pickle for faster loading
with open("enhanced_train_features.pkl", "wb") as f:
    pickle.dump(train_df, f)

with open("valid_features.pkl", "wb") as f:
    pickle.dump(valid_df, f)

from google.colab import drive
drive.mount('/content/drive')
train_df.to_pickle("/content/drive/My Drive/enhanced_train_features.pkl")
valid_df.to_pickle("/content/drive/My Drive/enhanced_valid_features.pkl")

"""# Save weights only using new features"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/enhanced_train_features.pkl"
valid_features_path = "/content/drive/My Drive/enhanced_valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

model_name = "microsoft/deberta-v3-base"
class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=11):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            optimizer.zero_grad()
            with autocast():  # mixed precision
                logits, ortho_loss, style_feature_loss = model(
                    input_ids, attention_mask, features1, features2
                )
                cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
                loss = cls_loss + model.ortho_lambda * ortho_loss + style_feature_loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/small_factorized_attention_model1.pt"

# Save model weights
torch.save(model.state_dict(), model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = FactorizedAttentionModel().to(device)
model.load_state_dict(torch.load("/content/drive/My Drive/small_factorized_attention_model1.pt"))
model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Ablation Study CLS + Orthogonality Loss"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel

class CLSOrthogonalModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, ortho_lambda=0.1):
        super(CLSOrthogonalModel, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)

        # Style & content heads
        self.style_proj = nn.Linear(hidden_dim, style_dim)
        self.content_proj = nn.Linear(hidden_dim, content_dim)

        # Classifier: style + content
        self.classifier = nn.Linear(style_dim + content_dim, 1)
        self.ortho_lambda = ortho_lambda

    def forward(self, input_ids, attention_mask, *args, **kwargs):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]

        # Project heads
        style_embedding = self.style_proj(cls_embedding)
        content_embedding = self.content_proj(cls_embedding)

        # Orthogonality loss
        style_norm = F.normalize(style_embedding, dim=-1)
        content_norm = F.normalize(content_embedding, dim=-1)
        ortho_loss = torch.norm(torch.matmul(style_norm, content_norm.T)) ** 2

        # Combine & classify
        combined = torch.cat([style_embedding, content_embedding], dim=-1)
        logits = self.classifier(combined).squeeze(-1)

        return logits, ortho_loss

from torch.optim import AdamW
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLSOrthogonalModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            optimizer.zero_grad()
            with autocast():  # mixed precision
                logits, ortho_loss = model(
                    input_ids, attention_mask, features1, features2
                )
                cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
                loss = cls_loss + model.ortho_lambda * ortho_loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/CLS_Orthogonality_model.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/CLS_Orthogonality_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = CLSOrthogonalModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            logits, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Ablation Study CLS + Style Features"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/train_features.pkl"
valid_features_path = "/content/drive/My Drive/valid_features.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel

class CLSStyleFeatureModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, feature_dim=7):
        super(CLSStyleFeatureModel, self).__init__()
        self.encoder = AutoModel.from_pretrained(model_name)

        # Project handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classifier: [CLS] + style features
        self.classifier = nn.Linear(hidden_dim + style_dim, 1)

    def forward(self, input_ids, attention_mask, features1, features2):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]

        # Handcrafted feature projection
        combined_features = torch.cat([features1, features2], dim=-1)
        feature_embedding = self.feature_proj(combined_features)

        # Concatenate and classify
        combined_rep = torch.cat([cls_embedding, feature_embedding], dim=-1)
        logits = self.classifier(combined_rep).squeeze(-1)

        return logits

from torch.optim import AdamW
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CLSStyleFeatureModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            optimizer.zero_grad()
            with autocast():  # mixed precision
                logits = model(
                    input_ids, attention_mask, features1, features2
                )
                cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
                loss = cls_loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()


        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/CLS_Features_model.pt"

# Save model weights
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
}, model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/CLS_Features_model.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = CLSStyleFeatureModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            logits = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Feature Re-extraction 2.0"""

# Install necessary dependencies
!pip install nltk pandas tqdm requests

import os
import json
import pandas as pd
import requests
import zipfile
from tqdm import tqdm
from pathlib import Path

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

# Define dataset URL and paths
ZIP_URL = "https://zenodo.org/records/14891299/files/pan25-multi-author-analysis.zip?download=1"
ZIP_PATH = "pan25-multi-author-analysis.zip"
EXTRACT_FOLDER = "pan25_data"

# Download the ZIP file
def download_file(url, save_path):
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get("content-length", 0))

    with open(save_path, "wb") as file, tqdm(
        desc="Downloading Dataset",
        total=total_size,
        unit="B",
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for chunk in response.iter_content(chunk_size=1024):
            file.write(chunk)
            bar.update(len(chunk))

# Unzip the dataset
def unzip_file(zip_path, extract_folder):
    os.makedirs(extract_folder, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_folder)
    print(f"Extracted to {extract_folder}")

# Download and unzip dataset
download_file(ZIP_URL, ZIP_PATH)
unzip_file(ZIP_PATH, EXTRACT_FOLDER)

# Define the root directory where the extracted folders exist
DATASET_ROOT = Path(EXTRACT_FOLDER)

# Define dataset splits
SPLITS = ["train", "validation"]
DIFFICULTY_LEVELS = ["easy", "medium", "hard"]

# Read and process the dataset
def read_problem_data(problem_file):
    text_path = problem_file
    json_path = text_path.with_name(f"truth-{text_path.stem}.json")

    # Read text
    with open(text_path, "r", encoding="utf-8") as f:
        text = f.read().strip()

    # Read labels
    with open(json_path, "r", encoding="utf-8") as f:
        truth = json.load(f)

    return text, truth["changes"]

# Define the file path
file_path = "pan25_data/hard/train/problem-3207.txt"

# New content to write
new_content = """Intelligence agencies aren't spying about pandemics folks.
The world has many interacting medical based organizations that monitor pandemics all the time.
Looking to the DoD or CIA or any other intelligence org to be the source of pandemic intelligence other than from a defense angle is a not their primary role.
Again, by rule, the NCMI is the lead agency on pandemic intelligence for the United States.
It's not an exception.
Additionally, the NCMI doesn't solely view medical intelligence from the lens of a "defense angle" as you believe.
But regardless, my comment originally rebutted your comment "Intelligence agencies aren't spying about pandemics folks."
Yes it does.
It's called the NCMI.
a.
There will be a unified Defense community for medical intelligence activities; those activities will be executed in a coordinated and coherent manner to effectively respond to U.S. intelligence priorities in support of national security objectives, and in accordance with all applicable laws, Presidential directives, DoD issuances, and Director of National Intelligence (DNI) guidelines.
From multiple angles; not solely based on a defense posture.
Where's your DOD policy citation that it's solely for defense posture?
"""

# Overwrite the file
with open(file_path, "w", encoding="utf-8") as f:
    f.write(new_content)

print(f"Successfully updated {file_path}")

def process_dataset(split):
    dataset = []
    total_mismatched_pairs = 0  # Counter for total mismatched pairs

    for difficulty in DIFFICULTY_LEVELS:
        print(f"Processing {split} set - {difficulty} difficulty...")
        folder_path = DATASET_ROOT / difficulty / split

        for problem_file in tqdm(folder_path.glob("problem-*.txt")):
            with open(problem_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Strip whitespace and remove empty lines
            sentences = [line.strip() for line in lines if line.strip()]

            # Read corresponding labels
            _, labels = read_problem_data(problem_file)

            # Check for label mismatch
            if len(labels) != len(sentences) - 1:
                mismatch_count = abs(len(labels) - (len(sentences) - 1))
                total_mismatched_pairs += mismatch_count

                print(f"\n Label mismatch in {problem_file}")
                print(f" Total Sentences (Lines): {len(sentences)} | Expected Labels: {len(labels)}")
                print(f" Mismatched Pairs in This File: {mismatch_count}")

                # Print mismatched sentence-label pairs
                print("\n--- Mismatched Sentence Pairs ---")
                for i in range(min(len(sentences) - 1, len(labels), 5)):
                    print(f"[{i}] {sentences[i]} || {sentences[i+1]} --> Label: {labels[i]}")

                print("--- End of Mismatch ---\n")
                continue

            # Merge consecutive sentences for training
            for i in range(len(sentences) - 1):
                dataset.append({
                    "difficulty": difficulty,
                    "split": split,
                    "problem_id": problem_file.stem,
                    "sentence1": sentences[i],
                    "sentence2": sentences[i + 1],
                    "label": labels[i]
                })

    print(f"\n Total Mismatched Sentence-Label Pairs Across All Files: {total_mismatched_pairs}")
    return dataset
# Process both training and validation sets
train_data = process_dataset("train")
valid_data = process_dataset("validation")

# Convert to DataFrame and save as CSV
train_df = pd.DataFrame(train_data)
valid_df = pd.DataFrame(valid_data)

train_df.to_csv("train_dataset.csv", index=False)
valid_df.to_csv("valid_dataset.csv", index=False)

print("Training and validation datasets saved successfully!")

# Load processed datasets
train_df = pd.read_csv("train_dataset.csv")
valid_df = pd.read_csv("valid_dataset.csv")

# Show unique label counts
print("\n Label Distribution in Training Set:")
print(train_df['label'].value_counts())

print("\n Label Distribution in Validation Set:")
print(valid_df['label'].value_counts())

pip install textstat

import spacy
import textstat
import numpy as np
from collections import Counter
import re

# Load SpaCy English model
nlp = spacy.load("en_core_web_sm")

# Define Reddit-style elements
uppercase_pattern = re.compile(r"[A-Z]{2,}")

# Define common pronouns used as subjects
subject_pronouns = {"i", "we", "you", "he", "she", "they", "it"}

POS_NGRAMS = [
    "NOUN-VERB", "PRON-VERB", "ADJ-NOUN", "VERB-ADV",
    "PRON-AUX-VERB", "ADP-DET-NOUN", "DET-ADJ-NOUN"
]

def get_pos_ngrams(pos_tags, n):
    return ['-'.join(pos_tags[i:i+n]) for i in range(len(pos_tags)-n+1)]

def extract_features(text):
    doc = nlp(text)

    # Stylometric & Reddit-style
    punctuation_count = sum(1 for char in text if char in ".,!?;:-")
    uppercase_ratio = len(uppercase_pattern.findall(text)) / (len(text) + 1)

    # Lexical
    tokens = [token.text.lower() for token in doc if token.is_alpha]
    type_token_ratio = len(set(tokens)) / (len(tokens) + 1) if tokens else 0

    # Function word usage
    function_words = {"the", "is", "and", "but", "or", "because", "as", "that"}
    function_word_count = sum(1 for token in doc if token.text.lower() in function_words)

    # Sentence complexity
    sentence_lengths = [len(sent) for sent in doc.sents]
    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0
    readability_score = textstat.flesch_reading_ease(text)

    # Subject structure features
    has_subject = 0
    is_pronoun_subject = 0
    is_noun_subject = 0
    for token in doc:
        if token.dep_ in {"nsubj", "nsubjpass"}:
            has_subject = 1
            if token.text.lower() in subject_pronouns:
                is_pronoun_subject = 1
            else:
                is_noun_subject = 1
            break

    # POS n-gram features
    pos_tags = [token.pos_ for token in doc]
    pos_bigrams = get_pos_ngrams(pos_tags, 2)
    pos_trigrams = get_pos_ngrams(pos_tags, 3)
    all_ngrams = pos_bigrams + pos_trigrams

    pos_ngram_counts = Counter(all_ngrams)
    pos_ngram_features = [pos_ngram_counts.get(ngram, 0) for ngram in POS_NGRAMS]

    # Final feature vector
    return [
        function_word_count,
        punctuation_count,
        type_token_ratio,
        avg_sentence_length,
        readability_score,
        uppercase_ratio,
        has_subject,
        is_pronoun_subject,
        is_noun_subject,
        *pos_ngram_features
    ]

# Apply feature extraction to dataset
train_df["features1"] = train_df["sentence1"].apply(extract_features)
train_df["features2"] = train_df["sentence2"].apply(extract_features)
valid_df["features1"] = valid_df["sentence1"].apply(extract_features)
valid_df["features2"] = valid_df["sentence2"].apply(extract_features)

print(" Feature extraction completed!")

import pickle

# Save extracted features along with dataset
train_df.to_csv("enhanced_train_features2.0.csv", index=False)
valid_df.to_csv("enhanced_valid_features2.0.csv", index=False)

# Save as Pickle for faster loading
with open("enhanced_train_features.pkl", "wb") as f:
    pickle.dump(train_df, f)

with open("valid_features.pkl", "wb") as f:
    pickle.dump(valid_df, f)

from google.colab import drive
drive.mount('/content/drive')
train_df.to_pickle("/content/drive/My Drive/enhanced_train_features2.0.pkl")
valid_df.to_pickle("/content/drive/My Drive/enhanced_valid_features2.0.pkl")

sample = extract_features("This is a simple example.")
print("feature_dim =", len(sample))

"""# Save weights only using new features"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
train_features_path = "/content/drive/My Drive/enhanced_train_features2.0.pkl"
valid_features_path = "/content/drive/My Drive/enhanced_valid_features2.0.pkl"

# Load training features
with open(train_features_path, "rb") as f:
    train_df = pickle.load(f)

# Load validation features
with open(valid_features_path, "rb") as f:
    valid_df = pickle.load(f)

print(train_df.head())
print(valid_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]

        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)


        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
train_dataset = StyleChangeDataset(train_df, tokenizer)
valid_dataset = StyleChangeDataset(valid_df, tokenizer)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, pin_memory=True)
valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

model_name = "microsoft/deberta-v3-base"
class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=16):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 1

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

from torch.optim import AdamW
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Initialize model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FactorizedAttentionModel().to(device)

# Optimizer with weight decay
optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)

# Enhanced training loop
def train_model(model, train_loader, epochs=3):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        total_cls_loss = 0
        total_ortho_loss = 0
        total_feat_loss = 0

        for batch_idx, batch in enumerate(train_loader):
            # Prepare batch data
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            features1 = batch["features1"].to(device)
            features2 = batch["features2"].to(device)
            labels = batch["label"].to(device).float()

            optimizer.zero_grad()
            with autocast():  # mixed precision
                logits, ortho_loss, style_feature_loss = model(
                    input_ids, attention_mask, features1, features2
                )
                cls_loss = F.binary_cross_entropy_with_logits(logits, labels)
                loss = cls_loss + model.ortho_lambda * ortho_loss + style_feature_loss

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # Accumulate stats
            total_loss += loss.item()
            total_cls_loss += cls_loss.item()
            total_ortho_loss += ortho_loss.item()
            total_feat_loss += style_feature_loss.item()

        # Epoch statistics
        num_batches = len(train_loader)
        print(f"\nEpoch {epoch+1}/{epochs}:")
        print(f"Total Loss: {total_loss/num_batches:.4f}")
        print(f"Classification Loss: {total_cls_loss/num_batches:.4f}")
        print(f"Orthogonality Loss: {total_ortho_loss/num_batches:.4f}")
        print(f"Feature Alignment Loss: {total_feat_loss/num_batches:.4f}\n")

train_model(model, train_loader, epochs=3)

import torch

# Define save path in Google Drive
model_save_path = "/content/drive/My Drive/small_factorized_attention_model2.pt"

# Save model weights
torch.save(model.state_dict(), model_save_path)

print(f" Model saved to {model_save_path}")

import torch
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = FactorizedAttentionModel().to(device)
model.load_state_dict(torch.load("/content/drive/My Drive/small_factorized_attention_model2.pt"))
model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="Validation"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
for difficulty in ["easy", "medium", "hard"]:
    subset = valid_df[valid_df["difficulty"] == difficulty]
    evaluate_model_batched(model, subset, dataset_name=f"{difficulty.capitalize()} Validation")

"""# Test on more data"""

import os
import zipfile
from google.colab import drive

DOWNLOADS = {
    "2025": {
        "url": "https://zenodo.org/records/14891240/files/pan25-multi-author-analysis.zip?download=1",
        "zip": "pan25-multi-author-analysis.zip",
        "extract": "pan25"
    }
}
DIFFICULTY_LEVELS = ["easy", "medium", "hard"]

def download_and_extract(download_info):
    if not os.path.exists(download_info['zip']):
        !wget -O {download_info['zip']} {download_info['url']}
    with zipfile.ZipFile(download_info['zip'], 'r') as zip_ref:
        zip_ref.extractall(download_info['extract'])

def merge_datasets():
    os.makedirs('test_merged', exist_ok=True)

    for level in DIFFICULTY_LEVELS:
        os.makedirs(f'test_merged/{level}', exist_ok=True)
        pan25_val_path = f"pan25/{level}/validation"
        pan25_test_path = f"pan25/{level}/test"

        for source_path in [pan25_val_path, pan25_test_path]:
            if os.path.exists(source_path):
                for file in os.listdir(source_path):
                    src = os.path.join(source_path, file)
                    dst = os.path.join(f'test_merged/{level}', file)
                    if os.path.exists(dst):
                        base, ext = os.path.splitext(file)
                        counter = 1
                        while os.path.exists(dst):
                            new_name = f"{base}_{counter}{ext}"
                            dst = os.path.join(f'test_merged/{level}', new_name)
                            counter += 1
                    os.rename(src, dst)

for dataset in DOWNLOADS.values():
    download_and_extract(dataset)

merge_datasets()

import os

def check_folder_structure():
    for level in DIFFICULTY_LEVELS:
        folder_path = f'test_merged/{level}'
        if os.path.exists(folder_path):
            files = os.listdir(folder_path)
            txt_files = [f for f in files if f.endswith('.txt')]
            json_files = [f for f in files if f.endswith('.json')]
            print(f"{level}:")
            print(f"  Total files: {len(files)}")
            print(f"  Text files: {len(txt_files)}")
            print(f"  JSON files: {len(json_files)}")
            print(f"  Matching pairs: {min(len(txt_files), len(json_files))}")
        else:
            print(f"{level}: Folder not found")

check_folder_structure()

from google.colab import files

uploaded = files.upload()

import zipfile

with zipfile.ZipFile("dataset3.zip", 'r') as zip_ref:
    zip_ref.extractall("test_merged")  # 可自定义目标文件夹名称

import json
import pandas as pd
from tqdm import tqdm
from pathlib import Path
import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

def read_problem_data(problem_file):
    text_path = problem_file
    json_path = text_path.with_name(f"truth-{text_path.stem}.json")

    with open(text_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    with open(json_path, "r", encoding="utf-8") as f:
        truth = json.load(f)

    return lines, truth["changes"]



def process_dataset(root_path):
    dataset = []
    total_mismatched_pairs = 0

    for split in ["train", "validation"]:
        print(f"Processing {split} set")
        folder_path = Path(root_path) / split

        for problem_file in tqdm(folder_path.glob("problem-*.txt")):
            sentences, labels = read_problem_data(problem_file)

            if len(labels) != len(sentences) - 1:
                mismatch_count = abs(len(labels) - (len(sentences) - 1))
                total_mismatched_pairs += mismatch_count

                print(f"\nLabel mismatch in {problem_file}")
                print(f"Sentences: {len(sentences)} | Labels: {len(labels)}")
                print(f"Mismatched Pairs: {mismatch_count}")
                continue

            for i in range(len(sentences) - 1):
                dataset.append({
                    "split": split,
                    "problem_id": problem_file.stem,
                    "sentence1": sentences[i],
                    "sentence2": sentences[i + 1],
                    "label": labels[i]
                })

    print(f"\nTotal mismatched sentence-label pairs: {total_mismatched_pairs}")
    return dataset


test_data = process_dataset("test_merged/dataset3")


test_df = pd.DataFrame(test_data)
test_df.to_csv("test_dataset.csv", index=False)

print("Test datasets saved successfully!")

# Load processed datasets
test = pd.read_csv("test_dataset.csv")

# Show unique label counts
print("\n Label Distribution in Test Set:")
print(test_df['label'].value_counts())

"""# Feature Extraction"""

pip install textstat

import spacy
import textstat
import numpy as np
from collections import Counter

# Load SpaCy NLP model
nlp = spacy.load("en_core_web_sm")

# Feature extraction function
def extract_features(text):
    doc = nlp(text)

    # Lexical Features
    word_lengths = [len(token.text) for token in doc if token.is_alpha]
    avg_word_length = np.mean(word_lengths) if word_lengths else 0

    function_words = set(["the", "is", "and", "but", "or", "because", "as", "that"])
    function_word_count = sum(1 for token in doc if token.text.lower() in function_words)

    # Syntactic Features
    pos_counts = Counter(token.pos_ for token in doc)
    num_nouns = pos_counts.get("NOUN", 0)
    num_verbs = pos_counts.get("VERB", 0)

    sentence_lengths = [len(sent.text.split()) for sent in doc.sents]
    avg_sentence_length = np.mean(sentence_lengths) if sentence_lengths else 0

    # Stylometric Features
    punctuation_count = sum(1 for token in text if token in ".,!?;:-")
    readability_score = textstat.flesch_reading_ease(text)

    return [
        avg_word_length, function_word_count, num_nouns, num_verbs,
        avg_sentence_length, punctuation_count, readability_score
    ]

# Apply feature extraction to dataset
test_df["features1"] = test_df["sentence1"].apply(extract_features)
test_df["features2"] = test_df["sentence2"].apply(extract_features)

print(" Feature extraction completed!")

import pickle

# Save extracted features along with dataset
test_df.to_csv("test_features.csv", index=False)


# Save as Pickle for faster loading
with open("test_features.pkl", "wb") as f:
    pickle.dump(test_df, f)

from google.colab import drive
drive.mount('/content/drive')
test_df.to_pickle("/content/drive/My Drive/test_features.pkl")

"""# Test"""

import pickle
from google.colab import drive
drive.mount('/content/drive')

# Define file paths
test_features_path = "/content/drive/My Drive/test_features.pkl"

# Load training features
with open(test_features_path, "rb") as f:
    test_df = pickle.load(f)


print(test_df.head())

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer

# Load DeBERTa tokenizer
model_name = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Custom PyTorch dataset
class StyleChangeDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length=128):
        self.dataframe = dataframe
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, index):
        row = self.dataframe.iloc[index]
        sentence1 = row["sentence1"]
        sentence2 = row["sentence2"]
        label = row["label"]
        features1 = torch.tensor(row["features1"], dtype=torch.float32)
        features2 = torch.tensor(row["features2"], dtype=torch.float32)

        encoding = self.tokenizer(
            sentence1, sentence2,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "features1": features1,
            "features2": features2,
            "label": torch.tensor(label, dtype=torch.long)
        }

# Create PyTorch dataloaders
test_dataset = StyleChangeDataset(test_df, tokenizer)

test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)

print(" Data successfully loaded into PyTorch Dataloaders!")

import torch.nn as nn
from transformers import AutoModel

class FactorizedAttentionModel(nn.Module):
    def __init__(self, model_name=model_name, hidden_dim=768, style_dim=128, content_dim=128, feature_dim=7):
        super(FactorizedAttentionModel, self).__init__()

        # Load DeBERTa model
        self.encoder = AutoModel.from_pretrained(model_name)

        # Factorized Attention Heads
        self.content_proj = nn.Linear(hidden_dim, content_dim)
        self.style_proj = nn.Linear(hidden_dim, style_dim)

        # Additional MLP for handcrafted features
        self.feature_proj = nn.Linear(feature_dim * 2, style_dim)

        # Classification head (combines style embeddings & extracted features)
        self.classifier = nn.Linear(style_dim * 2, 1)

        self.ortho_lambda = 0.1  # Regularization for orthogonality loss

    def forward(self, input_ids, attention_mask, features1, features2):
      # Get transformer embeddings
      outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
      embeddings = outputs.last_hidden_state[:, 0, :]

      # Separate content and style representations
      content_embedding = self.content_proj(embeddings)
      style_embedding = self.style_proj(embeddings)

      # Process handcrafted features
      combined_features = torch.cat([features1, features2], dim=-1)
      feature_embedding = self.feature_proj(combined_features)

      # Feature-guided masking
      style_weight = torch.sigmoid(torch.norm(combined_features, dim=-1, keepdim=True))
      content_weight = 1 - style_weight
      content_embedding = content_embedding * content_weight
      style_embedding = style_embedding * style_weight

      # Orthogonality loss
      content_norm = F.normalize(content_embedding, dim=-1)
      style_norm = F.normalize(style_embedding, dim=-1)
      ortho_loss = torch.norm(torch.matmul(content_norm, style_norm.T)) ** 2

      # Feature alignment loss
      style_feature_loss = 1 - F.cosine_similarity(
          style_norm,
          F.normalize(feature_embedding, dim=-1)
      ).mean()

      # Final prediction
      combined_rep = torch.cat([style_embedding, feature_embedding], dim=-1)
      logits = self.classifier(combined_rep).squeeze(-1)

      return logits, ortho_loss, style_feature_loss

import torch
from torch.optim import AdamW
from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define model path
model_load_path = "/content/drive/My Drive/enhenced_factorized_attention_model0.1.pt"

# Load checkpoint
checkpoint = torch.load(model_load_path, map_location=device)

# Initialize model and optimizer
model = FactorizedAttentionModel().to(device)
optimizer = AdamW(model.parameters(), lr=1e-5)

# Load saved weights
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

model.eval()

print(" Model loaded successfully!")

from sklearn.metrics import accuracy_score, f1_score
def evaluate_model_batched(model, dataset, batch_size=64, dataset_name="test"):
    model.eval()
    all_preds, all_labels = [], []

    for i in range(0, len(dataset), batch_size):
        batch = dataset.iloc[i:i+batch_size]

        encoding = tokenizer(
            batch["sentence1"].tolist(),
            batch["sentence2"].tolist(),
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors="pt"
        )

        input_ids = encoding["input_ids"].to(device)
        attention_mask = encoding["attention_mask"].to(device)
        features1 = torch.tensor(batch["features1"].tolist(), dtype=torch.float32).to(device)
        features2 = torch.tensor(batch["features2"].tolist(), dtype=torch.float32).to(device)
        labels = torch.tensor(batch["label"].tolist(), dtype=torch.float32).to(device)

        with torch.no_grad():
            # Updated to handle three return values
            logits, _, _ = model(input_ids, attention_mask, features1, features2)
            preds = torch.sigmoid(logits).cpu().numpy() > 0.5

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"{dataset_name} Set - Accuracy: {accuracy:.4f}, Macro F1-Score: {f1:.4f}")
    return accuracy, f1

import torch.nn.functional as F
subset = test_df
evaluate_model_batched(model, subset)