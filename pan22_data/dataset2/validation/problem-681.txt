Index data is not stored in the dump file, only index definitions. You can have a 14 GB dump file with 14 GB table data and 0 index definitions or hundreds of index definitions, and building those indexes may consume a lot more storage than the base tables, or just a fraction of it.
Assuming you are not using bigfile tablespaces and assuming you have no standard for how large datafiles should be or how empty they should be, you should create a tablespace with one 14G datafile and allow it to autoextend to a reasonable percentage of the overall free space you have available.  If you have plenty of free space, you should set the maximum to unlimited (which is actually a finite amount dependent on your block size).  By doing this you can set an amount for the autoextention that reflects your balance between system performance due to frequent autoextending and the space benefits of allocating only as much space as the objects need.
Using a combination of MASTER_ONLY and KEEP_MASTER switches, it's possible to get Data Pump import to do the dependency tracking, calculations, and estimations of sizes that it does at the beginning, create the master table that it uses to track everything, and then stop.
Remember that when importing data, if importing to a schema/user which has a default tablespace of "XYZ", then the data will go into that tablespace no matter what tablespace it came from on the originating database.