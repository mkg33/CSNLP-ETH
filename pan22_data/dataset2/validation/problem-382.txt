Rasterization (filling out triangles on screen) takes some time. If you do it on the CPU, you essentially take that time away from game logic, especially if it's not optimized well.
With software rendering, using the CPU, the renderer will be looping, one by one, over all the pixels on a bitmap and issue orders to show each on the screen. So if you're rendering a 1000x1000 sized image, that 1,000,000 loops for your CPU to go over. They are designed with control in mind after all; lots of if conditions, jumping from one set of instructions to another and a strict direction of flow of control. However, a GPU is designed with knowledge that it'll be doing a lot of similar looping over pixels on the screen. A GPU would take a for loop with a 1000000 iterations and divide the work over its huge number of cores for each to work** in parallel, and independent from one another**. So unlike the CPU, every time a GPU comes across an if-else condition, it'll handle both code branches to two cores of itself AND THEN, at the very end, it'll look at what the condition evaluates to and discards the result of the unneeded branch (that's why lots of if-else conditions in GPU shaders are frowned upon; they're always responsible for a waste).
It's not just about speed of execution, but also about simplicity. Although the software rendering used in this example would be a lot slower than using hardware acceleration (i.e. a GPU), drawing a few bitmaps on screen is such a trivial task that you would not notice the performance drop.
Engines do much more that just draw a picture to the screen. They handle lighting, shadows, input , collision detection. Even just the rendering part is way more complex than just pushing a buffer onto the screen. For 3d scenes especially you need to do a lot of calculations on far more complex data than a bitmap. Let me give you a analogy with a car: What you are describing as simple is the exhaust of the car. You just make a pipe with the right size and then you push the gas from one end to the other. However this is far from the only thing happening in the mechanism of the car.  
However, low-level activity like triangle rasterisation, depth sort and the like are well-understood concepts that the GPU can handle implicitly with a few commands. Re-implementing those in software mode is essentially reinventing the wheel. This is fine if you want to gain a low-level understanding of how rendering is done, I myself wrote a 3D software renderer just to explore it a bit, but for most circumstances it's a waste of time when OpenGL can do it faster out of the box.
You're drawing the line of what's "acceptable" or not at an arbitrary point in the toolchain. You could just as easily say "why use C++ when you could do the same thing in assembly?", or "why rely on the keyboard drivers when you could just as easily read the voltages coming off its wires and calculate it yourself?" There aren't enough hours in the day, or years in a lifetime for everyone to do everything themselves. 
Somebody created the hardware and the machine languages that run on it. Somebody else creates higher level languages and compilers, drivers and operating systems, graphics libraries and on and on. We each build upon the work of our predecessors. That's not only "okay", it's a requirement. 
While the answers from others are more correct than any answer I could give, I want to point out the fundamental misunderstanding about how software development works that I think underlies your question. While it's always possible to do things "by yourself" without a framework, and there's often great educational benefit from doing so, the reality is that's not how modern software is created. 
The example you've given sounds extremely basic, just drawing a single image on the screen, hence the implementation is easy. Once you start layering on complexity though, you'll find it becomes increasingly complicated to get everything rendering correctly. The stuff people had to do back in the Quake days of 3D software rendering was insane, though I appreciate you're not going THAT far (yet).
And doesn't matter, how small the image is, he needs to allocate certain amount of memory for it. GPUs have a video memory for this.
This doesn't apply just to software development, but to modern life in general. Have you ever heard of the guy that built a toaster himself, from scratch? http://www.thomasthwaites.com/the-toaster-project/. It took a really long time and a whole lot of effort. For a toaster. Try building everything that's required to actualize a video game out of the ether all on your own!
So yes, GPUs are built around parallelism. That makes working on pixels for them much faster compared to CPUs.
The above answers are excellent, but none really goes over the most important reason as to why OpenGL and such are preferred. The main reason is to make use of dedicated hardware designed especially to work with things like rendering millions of pixels on a screen, the GPU. 