Coming to your question about pi-calculus, I think pi-calculus is still a bit too new to be finding applications in language design.  The wikipedia page on pi-calculus does mention BPML and occam-pi as language designs using pi-calculus.  But you might also look at the pages of its predecessor CCS, and other process calculi such as CSP, join calculus and others, which have been used in many programming language designs.  You might also look at the "Objects and pi-calculus" section of the Sangiorgi and Walker book to see how pi-calculus relates to existing programming languages.
Wide-scale industrial adoption of a language is a matter for sociologists to study, and that science is even more in its infancy. Market considerations are an important factor — if Sun, Microsoft or Apple pushes a language, this has a lot more impact than any number of POPL and PLDI papers. Even for a programmer who has a choice, library availability is usually far more important than language design. Which is not to say that language design isn't important: having a well-designed language is a relief! It just usually isn't the deciding factor.
Process calculi do have practical implications. A lot of computations out there are distributed — they involve clients talking to servers, servers talking to other servers, etc. Even local computations are very often multithreaded to take advantage of parallelism over multiple processors and to react to environmental concurrency (communication with independent programs and with the user).
This is just fine from the POV type theory, but it's awkward when programming. The reason is that programmers end up managing not just their function calls, but also the call stack. (Indeed, encodings of lambda calculus into pi calculus typically end up looking like CPS transforms.) Now, typing ensures that they will never screw this up, but nevertheless it's a lot of bookkeeping foisted onto the programmer. 
The science of programming language design is very much in its infancy. Theory (the study of what programs mean and of the expressivity of a language) and empiricism (what programmers manage or don't manage to do) give a lot of qualitative arguments to weigh one way or another when designing a language. But we rarely have any quantitative reason to decide.
Are research advances needed to make better software? After all there's a billion-dollar industry out there that can't tell the pi calculus from a pie in the sky. Then again, that industry spends billions of dollars fixing bugs.
From my point of view, the No. 1 purpose of theory is to provide understanding, which can be in reasoning about existing programming languages as well as the programs written in them.  In my spare time, I maintain a large piece of software, an email client, written ages ago in Lisp.  All the PL theory I know such as Hoare logic, Separation Logic, data abstraction, relational parametricity and contextual equivalence etc. does come in handy in daily work.  For instance, if I am extending the software with a new feature, I know that it still has to preserve the original functionality, which means that it should behave the same way under all the old contexts even though it is going to do something new in new contexts.  If I didn't know anything about contextual equivalence, I probably wouldn't even be able to frame the issue in that way.
Now, the natural type discipline of pi-calculus is some variant of classical linear logic. See, for instance, Abramsky's paper Process Realizability, which shows how you interpret simple concurrent programs as proofs of propositions from linear logic. (The literature contains a lot of work on session types for typing pi-calculus programs, but session types and linear types are very closely related.) 
This is a tricky question! I'll tell you my personal opinion, and I emphasize that this is my opinion. 
You say that "the true end goal would be to use the theory to actually build a PL."  So, you presumably admit that there are other goals?
There is a delay between the time some theory stabilizes enough for an innovation to be usable in a practical programming language, and the time this innovation begins to appear in “mainstream” languages. For example, automatic memory management with garbage collection can be said to have been mature for industrial use in the mid-1960s, but to have only reached mainstream with Java in 1995. Parametric polymorphism was well-understood in the late 1970s, and made it into Java in the mid-200s. On the scale of a researcher's career, 30 years is a long time.
I do not think pi-calculus is directly suitable as a notation for concurrent programming. However, I think you should definitely study it before designing a concurrent programming language. The reason is that pi-calculus gives a low-level --- but importantly, compositional! --- account of concurrency. As a result, it can express everything you want, but not always conveniently. 
This is not a problem unique to concurrency theory --- the mu-calculus gives a good proof-theoretic account of sequential control operators like call/cc, but at the price of making the stack explicit, which makes it an awkward programming language.  
I like to search for practical implementations of process calculi in the wild :) (besides reading about the theory).
However, I said that the natural type discipline of pi-calculus is classical linear logic, and this is the source of the difficulty in using it directly as a programming language. In most presentations of classical linear logic, the (linear) function type $A \multimap B$ is not a primitive. Instead, you encode function types using de Morgan duality $A^\bot ⅋  B$. 
Explaining this comment requires thinking a bit about types. First, useful programming languages generally need some kind of type discipline in order to build abstractions. In particular, you need some kind of function type to make use of procedural abstractions when building software.
So when designing a concurrent programming language, my opinion is that you should design your language with higher-level abstractions than the raw pi-calculus, but you should make sure that it cleanly translated into a sensible typed process calculus. (A nice recent example of this is Tonhino, Caires and Pfenning's Higher-Order Processes, Functions and Sessions: A Monadic Integration.)
Process calculi are still at the stage where the theory hasn't stabilized. We believe that we understand sequential calculations — all the models of things that we like to call sequential calculation are equivalent (that's the Church-Turing thesis). This does not hold for concurrency: different process calculi tend to have subtle differences in expressivity. 
“Will they ever be needed” is never a worthwhile question in research. It is impossible to predict in advance what will have long-term consequences. I would even go further and say that it is a safe assumption that any research will have consequences one day — we just don't know at the time whether that day will come next year or next millennium.
For a while now, I have been very interested in programming language theory and process calculi and have started to study them. To be honest, it something that I wouldn't mind going into for a career. I find the theory to be incredibly fascinating. One constant question I keep running into is if either PL Theory or Process Calculi have any importance at all in modern programming language development. I see so many variants on the Pi-calculus out there and there is a lot of active research, but will they ever be needed or have important applications? The reason why I ask is because I love developing programming languages and the true end goal would be to use the theory to actually build a PL. For the stuff I have written, there really has not been any correlation to theory at all.