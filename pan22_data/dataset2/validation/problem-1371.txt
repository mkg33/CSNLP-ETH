Most commonly, I've added a feature to the dataset that is actually a sort of proxy of the target variable. That is, I've made a silly mistake.
Not sure if this is going to be a satisfactory answer... 1st thing that comes to mind whenever I get a 100% accuracy on test data is "I must have done something wrong".
But sometimes it is not a silly mistake, like adding a feature. I don't really remember the source thus I cannot link to it. But I heard in a podcast about some guys that were trying to classify patients of cancer from some features (I mean not from images of cancer cells or anything like that), and they built a model that was pretty simple and surprisingly good (not 100% accuracy though). The point was they included some kind of id of the patient as a feature, and that id somehow contained information about the hospital that treated them. There were a few hospitals that treated the very bad cases, thus the model was learning that anybody going to those few hospitals was really sick, and not really learning about who was sick.