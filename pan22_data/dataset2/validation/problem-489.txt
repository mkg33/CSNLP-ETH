This is all quite informal reasoning. It can be made formal by considering the question of the cost of sampling from a distribution $X$ in the so called "Random Bit Model" --- essentially how many (unbiased, independent) random bits do you need to exactly generate a sample from $X$? Knuth and Yao gave a construction for which the average number of bits required is provably in $[H(X), H(X) +2]$. By a similar "scaling up" argument you can convert their sampler to one which uses $H(X)$ random bits on average, therefore justifying the "average number of yes/no questions" intuition.
If you want to apply your intuition for "how many questions to ask", it only makes sense for integral values of entropy. So instead of your random variable $X$, consider the random variable:
In the following I will use that $1/0.46899\approx 2.13$ as motivation to set $N = 213$. You can set $N$ to be a better decimal approximation to this quantity to get a more exact argument.
Using your heuristic, we can determine a value of $Y$ using roughly 100 yes/no questions. Each value of $Y$ corresponds to $N$ values of $X$ though, so we can convert this value of $Y$ (which we used roughly 100 "questions" on) to $N$ values of $X$, so the "number of questions" we use to generate each value of $X$ is $100/N\approx H(X)$.