Additionally, serial devices (and thus modems - which is how data was transmitted over long distance initially), had different ways of representing data (for example N81 - No parity, 8 data bits, 1 stop bit, so a "byte" of data was 9 bits in this example).
Then, of-course, there is compression.   If you were/are sending standard text, you could get a lot more bytes through the line then the stated bit rate.
Then along came the Internet and grouped data into packets - with additional overheads for each packet.  Depending on the size of the packet and encapsulation, the packet overhead can be significant.
Once you need to deal with congestion, the game changes - Congestion usually means packet loss, which signals the system to slow down.  Protocols which break up into multiple streams will provide better throughput (eg bittorrent, some implementations of HTTP downloads), and, of-course compression.
David Schwartz is right that bits/s predate the Internet - and this is part of the answer - old systems did not always use 8 bits to represent data - For example, ASCII is only 7 bits (Extended ASCII is 8 bits).  
There is no single correct answer to this question.  If the file can't be compressed, and the channel is not congested and distances are short, FTP is pretty good.
That said, there is also tuning which can sometimes make a significant difference which sits below the TCP level at which FTP works.  (This is an expert topic, but includes things like larger MTU's, more packet buffering, QoS tagging etc - and the performance will only be as good as these underlying optimisations will allow.