in short, rsync works really, really well.  Any error could better be attributed to your hardware and/or filesystem.  bpbkar wouldn't perform any better facing the same failures.
If your backups take 18 hours to run normally, deprioritising them probably isn't going to solve the problem (unless you want to run your backups for a couple of days at a time).  I'd be inclined to setup a disk replication mechanism to another machine (I like DRBD, myself) and then use LVM to take a point-in-time snapshot, backup that, and move on.  Because it's running on a separate machine, (a) it can hammer as hard as it likes without affecting the live app, and (b) it won't be contending with the live app for disk IO, meaning it'll probably run a whole lot faster as well.
bpbkar is Veritas Netbackups backup client. It supports throttling, so the combination of normal I/O and backup I/O doesn't saturate your disks. Look at here:
We have a system wher we rsync live servers to backup servers (which are built out of cheap 1TB SATA discs) then take full tape backups of the backup servers. It's excellent:
You might also want to look into synthetic full backups so you don't need to do as many full backups.
Another vote for rsync.  I use it to daily backup 9TB of a very heavy used fileserver.  never had an issue.
If you're concerned about 'point in time', create an LVM snapshot, mount, rsync, umount, destroy.  Somewhat higher load on the server, but still far (far!) less time than a full copy.
An anectode from testing: when we approached the 8TB limit of ext3, made some 'pull the plug' tests to determine how possible is to corrupt a file by hardware failure while copying.  pulled the plug on the server, the storage boxes, and the SAN wiring.  copied tens of millions of files.
One thing I can say for sure: anything you do on the same machine is going to completely bone your disk cache -- as the backup process reads all the data off the disk to be backed up (even if it just checks mtimes rather than reading and checksumming all the files), that's still a lot of metadata blocks running into your cache, and those will be kicking out useful data from the cache and causing more disk IO than is otherwise warranted.
If the administrator says that it must positively, absolutely be bpbkar, first do an rsync to a less used system, and then run bpbkar from it.  No need to hog your production system.
Get your NetBackup admins to schedule the backups better - do full backups on alternating weeks for each RAID array. 
Is there anything stopping you doing full backups at the weekend, as you say the system is mostly busy weekdays, and incremental backups during the week? That'd help you get the backup done during the quiet slot between 2300 and 0900