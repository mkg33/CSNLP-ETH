Force detach the system volume.  Then you will be able to stop instance.  Re-attach volume and start instance.  Viola!
Where the value supplied to region= is the region that contains the environment to be shut down. On that page, one can easily deactivate multiple environments swiftly:
I ran into a related situation after I created an Elastic Beanstalk environment within an ec2 instance. To terminate my ec2 instance, I had to terminate my eb environment first, which I did by visiting:
on reboot internal IPs and public DNS change, so if you dont have elastic IP associated with your system then this might be the reason why you cant SSH in to the system.
After a routine reboot of a Linux ec2 instance with 1 small root volume and 1 small attached volume I was not able to ssh into the instance.  It is not clear why rebooting the instance caused it to be inaccessible from ssh.  The instance showed as running in the AWS console but ssh, http,etc was not responsive.  I tried to create an AMI from this running instance.  However, the AMI was never created.  Instead I just saw "pending" in the AMI section of the AWS console for hours.  Eventually I de-registered the AMI.  Next I tried to stop the ec2 instance.  However I am not able to stop the instance - it has been stuck in the stopping state for hours. I also tried force stopping the instance with no success.  I then tried to detach the volumes but they constantly report "detaching"  Does anyone have any suggestions about how to handle this?  It seems that Amazon does not offer any kind of email or phone support unless you are a premium member.  Thanks very much for your help.
I know this post is old, but you can also click stop again, and the dashboard will ask you if you want to do a forced stop. Sometimes, I know this from experience, you have to do two or three forced stops to get it to work.