2) If you use a big number of layers/neurones, then the model will learn  too much representations/features that are specific to the training data and donâ€™t generalize to data in the real-world and outside of your training set (overfitting). 
Working with neural networks since two years ago, this is a problem I always have each time I wan't to model a new system. The best approach I've found is the following:
Adding to the previous answers, there are approaches where the topology of the neural network emerges endogenously, as part of the training. Most prominently, you have Neuroevolution of Augmenting Topologies (NEAT) where you start with a basic network without hidden layers and then use a genetic algorithm to "complexify" the network structure. NEAT is implemented in many ML frameworks. Here is a pretty accessible article on an implementation to learn Mario: CrAIg: Using Neural Networks to learn Mario
1) If you use a few number of layers/neurones, then the model will just learn a few useful representations/features of your data and lose some important ones, because the capacity of middle layers are very limited (underfitting).
The general approach is to try different architectures, compare results and take the best configuration. Experience gives you more intuition in the first architecture guess.
Choosing the right number of layers can only be achievable with practice. There is no general answer to this question yet. By choosing a network architecture, you constrain your space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. In a DeepNN each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the problem at hand, this information can never be recovered by later layers. This is usually referred as "Information Bottleneck".