As you can see in the lower graph in this image, my wifi signal is even lower than the noise level. Hence, the "quality" graph in the middle, indicating the SNR, is negative. The upper graph shows the actual data rate is zero. In other words, I have a really, really bad wifi connection at the moment, and my computer knows it. And yet the indicator in the menu bar at the top shows full strength, 5 out of 5 bars! 
Here's the thing: Every laptop and wifi device has a signal indicator like this. Users look at it to answer a very specific question: "How good is my connection going to be right here, right now?" 
So why is it that my Mac (and in fact, every other device I know of) shows RSSI, which doesn't answer that question (and isn't really useful to end-users), instead of SNR, which does (and is)? Wouldn't a signal quality indicator be much more useful than a signal strength indicator?
It seems that the menu bar indicator is showing the RSSI, i.e. the "signal strength," instead of the SNR, a much better indicator of actual signal quality. As far as I can tell, my phone does the same thing - this would explain why in crowded cities, I can have 4-5 bars and poor reception, while in flat, empty rural areas I can usually connect just fine with 1-2 bars.