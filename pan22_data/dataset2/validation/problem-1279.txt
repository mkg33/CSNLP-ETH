If you are still nearing 100% utilization, talk to your programmer and see if he can improve things by making more efficient DB queries. If this isn't an option, then throw money into some faster disk hardware (move to RAID0 or SSD disks).
Investigate what else is running at this time (DB dumps running? backups? slocate/mlocate?) that may be causing I/O bottlenecks.
I'm going to assume this is a SATA disk system so 181 TPS is about the limit. You can try iostat -x 1 to get some extended statistics (example below). Note the %util column. This will tell you how much the read/write load the disk is under. I would guess this to be nearing 100% and causing the database problems.  Another metric is the svctm. This tells you how long it's taking the disk to complete any given I/O operation. The higher the number, the worse the situation.
Disk utilization can also be caused by the system having too little available RAM. As soon as memory becomes tight, disk swapping starts and everything comes to a crawl. 