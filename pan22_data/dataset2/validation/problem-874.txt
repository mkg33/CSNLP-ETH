If you have access to N different training algorithms and you can use cross validation to optimize them and choose the best one ( the one that leads to a very low bias error), then there is a high probability that you don't need to use stacking. But stacking is very beneficial when all these N algorithms can not lead to 0 bias error (if you have variance error use bagging), it is a case that may confuse the cross validation algorithm. The other thing is using cross validation to optimize algorithms may lead to different algorithms that have similar biases, which reduces the benefits from the stacking technique.
After you finish generating the meta data you can now use all the data the 5 folds to train the base learners. The higher the convergence between the final set of base classifiers and the ones you used to generate the meta data the better. This is why the less the data you have the larger the k you need to go with and vice versa.
Here is one last note to think about: using cross validation to optimize base learners my not be very beneficial, but why?
You can use the 4 folds ( training data) to optimize the base classifiers. You can also find the best hyper parameters by applying cross validation on your training data, re-train using all training data ( the 4 folds) and then test using the last fold to generate the meta data.
you need to know that stacking, is one of the most tricky ensembles and this is why it is not well studied in literature in compare to bagging and boosting. In the other hand it is proven that it is very important, especially for practical purposes.