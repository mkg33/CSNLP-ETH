I'm going to assume your using python and scikit-learn mostly because it has a method for providing model metrics.
The most common methods are Platt scaling and isotonic regression.  There is a third and more recent method, beta calibration, and there are a few more exotic ones around.  The three ones I've named all fit to a new dataset a univariate function with inputs your model's scores and outputs the actual observed labels.  Platt scaling fits a sigmoid function, beta calibration fits a parametric model that is more general than sigmoid, and isotonic fits a nonparametric, arbitrary non-decreasing function.  XGBoost's outputs are biased away from 0 and 1, so the sigmoid is generally ill-suited, so in this case go with beta or isotonic (or find something else to your liking).  Isotonic, being more well-known, has more open-source implementations.
And the classification report goes into more depth about how good the model is predicting each class more info here crossvalidated SE explanation.
You included that probability-calibration tag, which is prescient: there are a few techniques, all called "probability calibration," which adjust the scores output by a model to better fit observed probabilities.  After this, the scores should be close to representing real probabilities, and should therefore be directly comparable.