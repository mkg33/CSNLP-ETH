So what you're seeing is perfectly normal, if slightly misleading, behaviour: the time recorded for glDrawArrays is not actually the time taken by glDrawArrays alone, but also includes time for actually committing a whole bunch of previous GL calls.
There's a discussion of this for D3D available here but the same basic principle will apply to any modern implementation of either API.  The key point to take away from it is: "in general, trying to profile your GPU by timing the CPU is going to be confusing and misleading".
Naive implementation of just writing to a command buffer on every gl call makes little sense, when the driver can possibly optimize out state changes. Thus, it makes sense to postpone processing as late as possible, especially on a tiled/binned rendering architecture, which most OpenGL ES implementations are.
On some architectures it actually makes sense to do some shader processing as late as on a glDrawArrays call. (And yes, even your ES1.x hardware likely has some form of shaders under the hood).
Calling glFinish/glFlush like doug65536 suggests won't help on tiled/binned architectures, as it causes a total pipeline flush which has a very, very, VERY heavy overhead on such - causing the rendering of everything queued so far to every tile and forcing a unresolve on the next render cycle.
The way most drivers operate is by using a "lazy state changes" model.  What this means is that the vast majority of your gl* calls will actually do nothing much more than recording a state, storing off some parameters, then return immediately.  This works perfectly fine up until a gl* call is made that actually needs to do something with all that state (or that depends on the result of some other previously made call); at that point in time all the previously buffered-up state needs to be gathered together and flushed before the call can be made.