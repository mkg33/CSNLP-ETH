This is just an example, it could be that 5% is fine, or even too much, to hold out for testing. It depends mainly on the size of your dataset, and in case of classification, for example, it matter if one class has very few examples, so that perhaps in your 5% for testing there may not be a single example of that class.
K-fold cross-validation is a technique you use when you have too little training data. You use it to expand your training dataset, leaving only a tiny bit for testing (for example, 5%). Then you use k-fold cross-validation to select a different 5% for testing each time, and a different 95% of the full dataset for training each time.  That way, you can get a good idea of the performance of your model, even if you only kept 5% of the data apart for training.
What you're trying to do is called dimensionality reduction. The go-to technique for that is PCA. Make sure you normalize your features first (subtract the mean and divide by the variance), and then drop features until you still have, for example, 95% of the variance of your data explained. If in the end you find that you cannot get good performance from your model, it could be that you have lost too much information. In that case, you may have to increase this to 99%, adding features back in.
RSS is ok to reduce the number of training examples you have in your dataset, but it is only necessary if the order of examples is not random already. If it's random already, then you can just train on the first 10%, or whatever you need, and save yourself the trouble.