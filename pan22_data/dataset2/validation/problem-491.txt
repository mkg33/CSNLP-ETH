You always size for the peaks, unless it's the kind of workload that can afford to have high latency when it's pushing a lot of IO. That is part of why wide striping is so popular- you can put together a bunch of workloads and size for the peak of their aggregate usage- different parts will peak at different times, so you're able to use cheaper disks to provide the same capacity.
The closest I can come to answering your question, is to note that if you cannot handle the instantaneous iops at any given time, you will simply increase latency.  If latency isnt important, then buying storage based on your projected growth in average iops is not a bad place to start.
Unfortunately, there is no easy answer to that question.  First, consider your needs.  How much money are you willing/able to spend?  How much redundancy do you need?  How much total storage do you need? How much latency can you tolerate?  How much growth will you have over the amount of time you want the system to last (both growth in size, and in iops)?  Do you have time to maintain and prune you data to keep size down?  
Let's say that I have gathered Disk Transfers per second data for 2x24 hours period, i.e., instantaneous sampling of data every 15 seconds. What statistical analysis can/should I apply to the samples if I want to use the data to, for instance, provision a storage?
NOTE: Redundancy is not a backup solution, so plan for backups as well.  Backups can (should) be isolated from your live data by time and space.
Should I simply use the peak value (which happens less than 1% of the time)? Should I user mean/average value? Or a formula involving the mean and the deviation?
Wide striping assumes that this is on some sort of centralized storage. If it's local, of course you can't aggregate workload that way. 