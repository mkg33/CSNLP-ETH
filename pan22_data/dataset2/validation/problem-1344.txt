Can anyone suggest a tool/method to simulate various scenarios when using WSUS?  Or am I asking the impossible?  I'm curious to know if I deploy X number of patches over slow links is there any way of determining when all of the machines on the other end of the wire will be updated?  
Virtual machines in a test environment that allows you to implement rate-limiting (to simulate various link speeds) is the best solution. But, as Andrew noted, there are considerable costs associated with this.
Not knowing anything about how your network is engineered makes this a difficult question to answer concisely.
If you're concerned about bandwidth and remote WSUS servers isn't an option, then you can also tune your BITS settings in a GPO for those sites; the clients may get the patches slower but better than not at all.
Another approach might be to simply break out your pencil and perform some back-of-the-napkin calculations. What's the link speed, how large is the set of patches, can they be multicast, how many machines on the remote end, and so on. Once you've considered these variables you might decide that it's simpler to deliver a single set of patches to a second WSUS server on the far end and move on with life.
Do you have a bunch of mobile users, and you want to know when they would all be updated? But you don't know when they will connect and over what speed links?
Apart from doing a dry run, which is very costly in time, there isn't an easy way to model this. Perhaps it would be better for management to insist that these machines connect for a certain period within which patches can be deployed?