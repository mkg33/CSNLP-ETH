And on the topic of disk failures, the issue RAID6 has is the same as RAID5 - parity bit calculations are complex, and take a lot of time, resources and I/O to do, meaning that you're actually at a bigger risk of array failure during a rebuild than you'd think, and I've actually seen the nightmare scenario of a RAID6 array (with a hot-spare, even) failing during rebuild - not terribly shocking considering all the I/O a rebuild takes, but terribly devastating if it happens to you.
And you'll need true backups with some kind of retention for the same reason.  Not having any backup means that data corruption could take out the whole thing as well, because if you're replicating the one system to the other, you're likely to replicate any corruption that occurs as well.
If you decide to go with ZFS, then go whole hog and set up NexentaStor. That's probably about the safest you'll get with your data in its intended, uh, home.
You don't need the best possible RAID — simple RAID5 would suffice. RAID is a high-availability solution, not a data protection solution. What you really need is a very good backup solution — one that will backup automatically, often, keep several old backups and backup periodically offsite (this is not automatical — it can be a for example a set of disks in a deposit box in a bank).
If your data is truly that important that losing it means bathing with the electrical appliances, then there's no way around needing a second server on which to mirror the devices, and having true backups.  Anything you do on just one server has at least one single point of failure.  (At a minimum, the server itself - for the most basic example, what if there's a fire?  Or a the PSU goes out and takes everything else with it?  Etc.)
As a final note, you don't want to kill yourself by electrocution.  It's quite excruciating, so you'd probably be better served by a different method of suicide.
And since no one's said it here yet, RAID6 and big SATA disks is not a good combination.  It does provide more redundancy than a RAID5 setup, but not by enough. On such large disks, you're almost certain to have a read error on one of your parity stripes, which really means that on one of the disks, somewhere (but you won't know where), you're effectively running in RAID5.  Which is all well and good until a disk fails.  Then you're running on failed array RAID, which is too big a risk with critical data.
If you're going to go the RAID 6 route, then do yourself a favor: Visit eBay and pick up a couple of PERC 6/i cards (one as a spare) and two SFF-8484 to 4xSFF-8482 cables to hook up the drives. You'll thank me later.
Given some of the trade offs you listed ditching DASD (Direct Attached Storage Device) might be a good idea.  You may find looking at something like GlusterFS (Red Hat Storage) or CEPH more useful.  Gluster for instance replicates data over a cluster of file servers, where that cluster can lose one or more disks, and potentially one more more nodes and maintain data integrity.
Overall your list lends itself to two or more solutions.  You may need to use fast DASD or SAN for a RDBMS, in which case you will want to spend time and money investing in mirroring and so forth.  Other use cases may be very well suited by something like Gluster which can allow for easier scaling and can abstract you from needing to know the intricacies of each RAID type.
And think very carefully about a backup strategy; neither RAID nor ZFS are backup. Backup of terabytes of data is not easy or cheap; it's not usually something you can do over the Internet, and terabytes of "cloud" storage would set you back a lot more than the drives anyway.
Anyways, just before using RAID6 take drills for loosing 1, and 2 disks. Go even further — take out 3 or 4, then make it working back. With LSR it's quite possible. And piece of advice — use its bitmaps, it would save hell-a-lot of the time for you.
The simpler is the better. LVM on LinuxSoftRAID-6 looks like quite reasonable. Another important thing left to consider is FS on top. In Linux world up to this moment only Btrfs has built-in data verification mechanisms. But I doubt its developers would share your attitude regarding bath-n-toaster in case. ;)
I'd recommend running it in 2xRAID10, for what it's worth - you get performance benefits, as well as some measure of redundancy.  Rebuilding a failed disk in a mirror RAID is a lot faster, and therefore safer, than you'll find in any parity-based system.  If it matters, I have a 12 disk RAID6 array at home (12x2TB near-line SAS), so I definitely do like RAID6, but it's just not safe enough for critical data.