This is a method of backup.  There are multiple solutions.  Some people do like to do things very barebones.  Particularly in Unix, they may often just use "tar" to create a data stream, and instead of storing that data stream to a file, they may send the data stream over a pipe.  That pipe might compress the data and create a new pipe.  A pipe may send data over a network using netcat.  There are various creative things people can do (like sending a pipe through "pv" for monitoring progress).  Other people prefer to use a bit more advanced of a solution such as rsync.  Other people may prefer full commercial suites, purchasing an expensive backup program.  There is no one single "right" way, so choose an option that suits you, and make it work.
You're probably best off to not create one 7-Zip archive file before copying over the network.  One bad bit might cause issues with not just one file, but the entire archive, thereby losing all data.  (So, your data storage just won't be very resilient.)  It will also take a long time, and may exceed the archive format filesize limitations (possibly depending on what version of software you use).
This can depend on various factors.  Historically, you would definitely want to compress before sending the file, to minimize transfer time.  However, these days networking equipment can be even faster than disk storage equipment.  So what will be fastest can depend on the states of various types of technology, which are details that are subject to change over time.
This answer may be partial, but I'm doing so since there were so many questions that addressing just some of them may take multiple comments.
Do remember, though, you probably do want the ability to restore a single file, instead of needing to copy an entire archive that has lots of files.
Often it is best to have workstations to work that workstations can do, so that a centralized server isn't overburdened with too much work.  However, if a server is more powerful, it might be able to just get a job done rather quickly, while the job might take longer on a workstation and may have more visible effects that annoyingly slow down a computer that somebody actually uses.  So, once again, the answer can vary.
Do not just use 7-Zip to create the archive remotely.  7-Zip may consider data multiple times, possibly as part of the compression process and possibly for the purpose of just verifying data.  To avoid unnecessary slowdown, you don't want to have data transferred over the network once, not re-reading data over the network.  (In practice, not necessarily a problem when creating small data files.)
Be warned that 7-Zip has been known to release versions that write incorrect data that cannot be extracted.  I've experienced this when creating ZIP files (from 7-Zip).  If you really care about the data, don't trust the compressed data until you've verified it.  That's probably always good advice, but I especially consider it important for 7-Zip.  (I love 7-Zip.  Free, easy to use, versatile with various compression formats, and creates ZIP files at are very close to the minimum size I can make when using a variety of specialized software.  However, it does not good to sugarcoat reality, so it is best to be aware of the limitations it does have.)