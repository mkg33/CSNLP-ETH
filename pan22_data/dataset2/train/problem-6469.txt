So the result is you need to create swap space as big as you need your /tmp to be (even if you have to create a swapfile) and use that space to mount a tmpfs of the required size onto /tmp. 
The tmpfs uses memory and swapspace as it's backing store a filesystem uses a specific area of disk, neither is limited in the size the filesystem can be, it is quite possible to have a 200GB tmpfs on a machine with less than a GB of ram if you have enough swapspace.
If you are creating a program that requires a virtual memory heap that should be mapped to virtual memory.  This goes double so if you need multiple processes or threads to be able to safely access that memory.
Unless, you're using it for very small (probably mmap'd) IPC files and you are sure that it exists (it's not a standard) and the machine has more than enough memory + swap available.
The difference is in when data is written to the disk. For a tmpfs the data is written ONLY when memory gets too full or the data unlikely to be used soon. OTOH most normal Linux filesystems are designed to always have a more or less consistent set of data on the disk so if the user pulls the plug they don't lose everything. 
Use /tmp/ for temporary files. Use /dev/shm/ when you want shared memory (ie, interprocess communication through files).
In other languages, not having gigether everywhere, to avoid the starts and stops in network transfer (working locally on a file that is located on a server in a client-server atmosphere), using a batch file of some type, I will copy the whole (300-900MB) file at once to /dev/shm, run the program with output to /dev/shm, write the results back to the server, and delete from /dev/shm
Personally, I'm used to having operating systems that don't crash and UPS systems (eg: laptop batteries) so I think the ext2/3 filesystems are too paranoid with their 5-10 second checkpoint interval. The ext4 filesystem is better with a 10 minute checkpoint, except it treats user data as second class and doesn't protect it. (ext3 is the same but you don't notice it because of the 5 second checkpoint) 
The fact is that just because the driver uses a special version of tmpfs for it, doesn't mean you should use it as a generic tmpfs partition.  Instead, you should just create another tmpfs partition if you want one for your temporary directory.
Another time when you should use /dev/shm (for Linux 2.6 and above) is when you need a guaranteed tmpfs file system because you don't know if you can write to disk.
A monitoring system I'm familiar with needs to write out temporary files while building its report for submission to a central server. It's far more likely in practice that something will prevent writes to a file system (either out of disk space or an underlying RAID failure has pushed the system into a hardware read-only mode) but you'll still be able to limp along to alert about it than if something spirals all available memory such that tmpfs will be unusable (and the box won't be dead). In cases like this, a monitoring system will prefer writing out to RAM so as to potentially be able to send an alert about a full disk or dead/dying hardware.
Naturally, if I had less RAM, I would not be doing this. Ordinarily, the in-memory file system of /dev/shm reads as a size being one half of your available RAM. However, ordinary use of RAM is constant. So you really couldn't do this on a device with 2GB or less. To turn paraphrase to hyperbole, there is often things in RAM that even the system doesn't report well.
This frequent checkpointing means that unnecessary data is being continually written to disk, even for /tmp.
In PERL, having 8GB minimum on any machine (all running Linux Mint), I am of what I think is a good habit of doing DB_File-based (data structure in a file) complex algorithms with millions of reads and writes using /dev/shm