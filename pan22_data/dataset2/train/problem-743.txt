I was hoping to open a debate about quantitative techniques that can evaluate explainers. What do you think? Also, speaking of 'altering' the features, what would be the appropriate way to do so (when working with tabular data)? I'm open for comments and thoughts!
That makes sense, too. So I am planning to build up on that and focus on tabular data. The idea would be to, for each instance (row) in our test data:
The thing is, those feature importance values differ from explainer to explainer, and it is rather tricky to evaluate which explainer actually performs best and produces the most sensible outcome. The authors of SHAP have tested the explainers' consistency with human intuition, assuming that:
I'm working on a project where multiple machine learning explainers (LIME and SHAP, potentially more coming) are applied to pre-trained models (neural networks) to help explain the predictions of those 'black boxes'. The explainers assign each feature an importance value for each prediction. 
Some authors have recently attempted to come up with evaluation metrics. I found one approach to be interesting. The authors worked with the following intuition:
Right, that makes sense. But how to evaluate the performances of different explainers in a more generic and scalable way, without the need for human intervention?
Then, the explainer for which the necessary amount of alteration to obtain a change of outcome was the highest (aggregated across all instances), would be the best.
I would just like to add Microsoft's interpret to the list of explainers which uses boosting, and has some really cool graphics for explaining a model's behavior.