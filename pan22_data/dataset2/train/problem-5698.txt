If you are considering Gluster (which in my experience is usable with many small files but YMMV), you might want to have a look at Ceph as well http://ceph.com/
DRBD is not an option here anyway - it doesn't scale. If anything, I'd look at other object storage projects (Swift for example), if Gluster doesn't work well enough, but none of them are extremely performance oriented
I would also recomend Glustet ad the easyest, and fastest to setup , configure. Also it scales out very very well. The problem is speed, and my 2c ia that you need to choose between easy configuration and easy scaleout, and between some technical dificulties (some fancy drbd / ocfs2 / Glances block storage) with the gain of speed. But..how much speed you gain? Yoi need to make some stress test and choose.. 
I need to build a solution to host internal git repositories. It needs to supports hundreds of thousands (or more) repositories.
This is a classic scale-out use case, and IMO GlusterFS should fit the bill. You can give it a try - just bring a few VMs up, set up a few bricks to be used for repository storage and run a stress test. 
But if you need your solution as fast as possible GlusterFS probably isn't for you. Other options would be commercial products like Git Clustering(haven't tested that, though) or you could build your own solution with free tools like gitmirror.
My first thought was to use GlusterFS for that, but I've read it doesn't handle well with small files. I'm also thinking of replicating everything myself using DRBD, but this requires more setup and seems more complicated when comparing to GlusterFS.
The problem you will have with Gluster is mainly latency between the nodes. I would advise you give it a try and see if it handles your workload. If your servers are are powerful enough and have a fast interconnect you should get a fairly good performance. Also you might want to try the built-in NFS server, which, talking from my experience, handles small files a little bit faster.
I plan on using multiple "dumb" servers with a shared storage, so basically when a client is trying to access a repository - it will be redirected by the load-balancer to any of the available servers. Any change to the repository - will be replicated across all nodes.
Which one of the two provides better performances? Basically the problem I'm trying to solve is that when any of the servers goes down - I want others to still be able to serve the data.