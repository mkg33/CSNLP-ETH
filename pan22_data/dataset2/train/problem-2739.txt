Blasphemy to propose an application change in a database forum... but the ultimate database optimization is to not use the database at all.
That query plan isn't necessarily bad, but SQL Server is scanning all ten million rows just to return 80 distinct ones. There's a more efficient algorithm available but it takes some effort to get. It's very fast to get the minimum value from an index. It's also very fast to get the next value from an index. Both are just a handful of logical reads. Instead of reading the entire index what if we could effectively read just the distinct values and skip ahead to the next ones?
Once you have a clean copy, import the data into the tables you decided upon. Set up a test environment and test the new tables thoroughly, fine-tuning with indexes etc. When the tests are successful, offer you users the opportunity to test out the new setup, or keep on using the old one. Once the majority of your users prefer the new data, discontinue the use of the old table but keep it around.
So, updates, deletes and inserts into the main table would become somewhat slower, but querying the indexed view would be instant, because it will not scan 10M rows of the main table. In any case, the engine is smart enough not to scan the whole 10M row table when it is updated to adjust the values stored in the indexed view.
You need to determine the requirement, specifically your target latency for a new distinct PortName to appear in the UI dropdown.
So, someone just copied the data and dumped it into a table. But why does it need to stay that way? This is a neat little project: Sit down with your teams your team and discuss which tables and relationships would serve your users best. Then take your time cleaning up a copy of the imported table.
Somehow, nobody mentioned an indexed view. A very brief intro to the indexed views can be found at What You Can (and Can’t) Do With Indexed Views.
In essence it is a cache, which is maintained by the engine automatically behind the scenes. Indexed view is stored on disk and updated automatically when the underlying table changes.
The other comments and answers by Flourid, RBarryYoung, and DForck42 have already highlighted that this looks like a normalization issue.  You should not be querying millions of rows to pull out just 80 unique records.  Normalize the data first so you can populate the drop downs from the 80 rows.  Then apply indexing as a performance enhancement if warranted.  Then apply caching so that you can populate the drop downs from the cache if warranted.
You should still optimize your query using the other answers provided, and perhaps including a where clause so that you only inspect new records added since the last refresh and then merge the results in your application code.  
If I increase the number of rows in the table to 100 million, the original query now takes 17203 ms of CPU time. The recursive query still takes 0 ms of CPU time. You can solve your query performance problem just by creating an index and writing some fancy code. In this case, there's no real need to implement caching in some other layer.
That can be done with recursive SQL. Paul White describes the approach here. Below is T-SQL written against your table:
This also makes it impossible to insert a row with a misspelled port, since it has to be linked to an existing row in the second table.
When you are notified that the other system has made updates, use the old table to pinpoint the changes and add them to your new data structure.
However, if you don't want to change up your database architecture, you could create a new table with just the name of the port (e.g.: CREATE TABLE portnames (name varchar(50));). Then you fill it with content from your first table (INSERT INTO portnames (SELECT DISTINCT PortName FROM Ports);). Now you can query this table instead! Remember, if you want to keep it updated you have to recreate (or truncate/insert) it everytime you add an entry to the first table .
Besides, the question title says "Alternatives to running query for rarely changed data", so I assume that this large table doesn't change often anyway. I think, indexed view would be perfect here.
This sounds like you have a table where each entry has a port, but only from a small pool of ports. In this case it is usually good practice to create a second table that contains every port once and link it via foreign key. Then you can query this much smaller table.
A query to return only 80 distinct values can finish almost instantaneously with the right index and the right physical implementation strategy. The run time of the query is determined by the number of distinct values as opposed to the size of the table.
Normalization seems the answer, but there's your comment "The main issue regarding normalization is that I'm sadly not entitled to change the database structure itself in this project. It's basically a replicated and indexed version coming out of another system. :) –"
Note: there is an edge case that when the server is restarted it will not have a last_user_update until the first insert/update/delete occurs.
That said (and the reason this is an answer rather than a comment), if you don't have control of the schema (normalization) or infrastructure (caching) to implement those changes... look at sys.dm_db_index_usage_stats.last_user_update.  This should be a relatively inexpensive query you can use to check the last insert/update/delete for the table.  Wrap your expensive query with the cheap one so that you only refresh from the table if the data has changed.  It's effectively a poor man's cache.
You mention running the query can take 10-15 seconds, but suggest "running some script at certain intervals... to offload querying".  This stands out as meeting two very different requirements: