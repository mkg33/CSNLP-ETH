In this way you would double your SAN bandwidth and should assure no disaterish cluster behavior. If you cannot use QOS I would choose this approach:
You don't specify if the system has 3 nics built-in to the motherboard or if they are add-in cards that are using up all available slots. If it is the latter case, I highly suggest you get replacement nic cards with more than one port instead of doing this vlan hackery. 2-port and 4-port server nic cards are readily available from Intel and 2-port gigabit nic is not too expensive.
If you are intent on going down the vlan path also see if your nics support QOS. In this way you can assign priority queues to traffic on different vlans. With QOS, the best scenario I can think of with 3 nics would be:
Why not to consider to move to 10G NICs for links you need to have increased bandwidth? The key idea your server isn't just for fun (or it is so?) so the more mess you create with VLANs, the more you (or your colleagues) will face later, increasing the risk of breaking the whole connectivity scheme.
All of this is possible doing exactly what you say using .1q, it's what it's designed for and works well. Can't add much more as you've got this tied up.
10G NICs are more and more affordable these days, and you'll also will be able to use one 10G to carry traffic for several VLANs later if you need.
In any case, really investigate getting the multi-port nic cards if you can because with vlan and, in particular qos features, you have a much greater complexity in the network card driver and IP software stack. There can be bugs...
Although the VLAN option will work it is important to keep the heartbeat separate. Although it is a low bandwidth connection it is latency sensitive. Combining the heartbeat and iSCSI on the same physical media would be a particularly bad idea since iSCSI tends to have bursty traffic that can saturate the link for short periods. The iSCSI traffic is also latency sensitive. However, if latency increases, you will only suffer decreased I/O performance instead of the potential of unnecessary and unintended cluster fail-over events. 
Here you may see performance anomolies but it should be a stable configuration. I'm not sure if you are accessing your SAN from standalone servers, Hyper-V, or VMWare ESX. We have found out ESXi 5 will not load balance iSCSI correctly unless the 2 connections are in different vlans, so maybe in the latter scenario above you would have NIC3 -> iSCSI(VLAN3)