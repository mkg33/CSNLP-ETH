The second distance function is mutual information (see wikipedia page here). The idea is similar with Kulback-Leibler divergence, however the KL distance is an oriented measure (measures how a distribution can be expressed through another one). The MI distance is a measure of mutual independence between two distributions. In your case the two distributions are two clusters, and the MI measures how dependent they are. I give formula just to illustrate the idea behind it. 
You can see from the formula that when the two variables are independent, the quantity inside the logarithm is $1$, and as a consequence, the MI is $0$. As the difference grows, the function also grows. I will not derive what p means, see what I explained already for chi square test, since is very similar. 
Note also that all those tests can't prove that two clusters are the same. The logic behind the tests can be formulated as "we consider the distributions the same, as a natural state, accepted without other proofs, and if we see some exceptionally obvious differences, we can bring that as a proof that they are different indeed". 
First distance is the chi square distance. This type of distance gives a measure to the distance between two normalized histograms (or, put it simply, between two vectors of probabilities of the same size). Thus, if we denote with $cluster_i = (n_{i1}/n_i, n_{i2}/n_i, .., n_{ik}/n_i] = (p_{i1},p_{i2},..p_{ik})$, the distance between cluster i and j would be: 
where $N_c$ is the number of clusters (here 3), $n_{object_i,cluster_k}$ is the number of elements of object $i$ that belong to cluster $k$ (here $n_{object_2,cluster_2}=200$) and $n_{object_i}$ is the total number of elements in object $i$ (sum of $i^{th}$ row).
To answer to the same type of question you can also employ G-test (wikipedia page here) or Fisher exact test (wikipedia page here).  
$S(object_i,object_j) = 1 - \frac{\sum_{k=1}^{N_c} |n_{object_i,cluster_k}-n_{object_j,cluster_k}|}{n_{object_i}+n_{object_j}}$
A different approach would be to build a distance between those two distributions. The advantage of a distance function would be that it could be used for comparisons and eventually some further algorithms based on distances. The drawback of a distance function is that it gives you no hint to answer the first type of questions like "are the differences between 2 distributions small enough in order to be explained by the natural noise in data?". Thus I have two candidates in mind. 
This distance function has the advantage that it is symmetric regarding $i$ and $j$. To be honest I rarely used it since I did not fully understand it's meaning. In general I use another function, described below.
You can consider that the clusters are defined by a multinomial distribution with 4 classes. Thus you would identify the cluster as a vector of 4 probabilities, one for each class, and the sum of probabilities being 1. 
You can ask questions like "is the cluster 1 is different than the cluster 2?". In order to do that you can employ hypothesis testing, and specifically chi square independence test (wikipedia page here). You select from your table only the first 2 columns, compute the marginal totals, compute the estimated counts as $E_{ij} = rowsum_i * colsum_j / grandtotal$. Denoting with $O_{ij}$ the cell counts you can compute a test statistic with 
This is known to has a chi square distribution with $(rows-1)(cols-1)=3$ degrees of freedom. Thus using the cumulative distribution of chi square you can compute the p-value as $1-cdf_{\chi_{3}^2}(chi)$. Now if p-value is smaller than any critical value you choose before test (like 0.05, 0.01 or so), you have a statistical proof that the clusters have a different distribution (this follows from the fact that your procedure tests for the independence of cluster and object distributions, which with short derivation is the same as saying the clusters have the same distribution). 