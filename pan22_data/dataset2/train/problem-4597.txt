How can I create a dual encoder though? Do I use two different neural networks? Or does output just contain one neuron that outputs similarity?
With universal sentence encoder I can first pre-encode all sentences and put them in the database. When user wants to perform query, input will too be converted in 512-dimensional vector, and we will perform sequential search on whole database by comparing cosine similarity (highest similarity vector is picked). But this is extremely slow...
One of the simpler methods they use for transforming a sentence into embedding vector is DAN (deep averaging neural network).
From what I understand, sentence is split in words, which are converted into vectors (word2vec), then we get average of all vectors that we have obtained. Average value is finally passed to one or more hidden feedforward layers, and eventually output layer that has softmax activation functions and has 512 neurons.
My apologies if anything ignorant has been mentioned here, I'm quite confused with the concept of projecting lower dimensional space to 512 dimensional vector space.
I've been trying to grasp the concept of Google's semantic experiences. By using it, I'm planning to implement a semantic query tool.