The piece of hardware is actually one of several clusters of load balancers, geographically dispersed, and you were directed to the one closest to you, a decision made by the DNS server. 
Anycast can also be used for TCP connections, assuming the connections are short-lived so the routes do not change during the connection lifetime. This is a good assumption with HTTP connections (especially if Connection: Keep-Alive is kept to a short timeout or disabled).
The piece of hardware is actually a cluster of load balancers, all of which are configured to pull from shared storage so they're all identically configured with identical material. 
Those "clusters" in this datacenter consist now of several different machines (DB server, web server, load balancer, etc.) Depending on what you are providing with your website you have maybe some servers for the static content etc.
Igor, your question is great, and like so many innocent questions, there are many, many answers, all at different levels of details. 
Larger sites use several different techniques together. Those websites you mentioned do all have in almost every country several server. Based on the IP address of the website visitor the DNS server is giving back an IP address of the cluster which is the nearest to the visitor. Akamai is providing such a service (click on the picture on this website for more information.)
Many CDNs (CacheFly, MaxCDN, and probably many others) actually use anycast for TCP connections (HTTP), and not just DNS. When you resolve a hostname on CacheFly, you get the same IP address around the world, it is simply routed to the "closest" CacheFly cluster. "Closest" here would be in terms of BGP path length and metrics, which is usually a better way to measure network latency than simple geographic distance.
Massive sites like Google almost certainly design their own hardware. Large sites would probably use a multi-layer switch to load balance connections to multiple actual servers.