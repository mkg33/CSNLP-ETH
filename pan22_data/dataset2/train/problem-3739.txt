Are there any benefits to calculating the time complexity of an algorithm using lambda calculus?  Or is there another system designed for this purpose?
There is a very interesting line of work based on linear logic, called implicit complexity theory, which characterizes various complexity classes by imposing various type disciplines on the lambda calculus. IIRC, this work began when Bellantoni and Cook, and Leivant figured out how to use the type system to bound primitive recursion to capture various complexity classes. 
In general, the attraction to working with lambda calculi is that it is sometimes possible to find more extensional (ie, more mathematically tractable) characterizations of various intensional features that give models like Turing machines their power. For example, one difference between Turing machines and pure lambda calculus is that since Turing receive codes of programs, a client can manually implement timeouts, to implement dovetailing -- and hence can compute parallel-or. However, timeouts can also be modelled metrically, and Escardo has conjectured (I don't know its status) that metric space models of the lambda calculus are fully abstract for PCF + timeouts. Metric spaces are very well-studied mathematical objects, and it is very nice to be able to make use of that body of theory.
However, the difficulty of using lambda calculus is that it forces you to confront higher-order phenomena right from the starting gate. This can be very subtle, since the Church-Turing thesis fails at higher type -- natural models of computation differ at higher type, since they differ in what you are permitted to do with the representations of computations. (Parallel-or is a simple example of this phenomenon, since it exhibits a difference between LC and TMs.) Moreover, there isn't even a strict inclusion between the different models, since the contravariance of the function space means that more expressive power at one order implies less expressive power one order higher. 