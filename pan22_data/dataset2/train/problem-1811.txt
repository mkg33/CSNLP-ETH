I don't know if you should organize any of this info into a lecture though, as like you said it all comes off as a bit "back in my day we had to walk uphill to the mainframe- both ways!" Instead, maybe try to start a discussion about how much technology has changed in the students' lives, and then work backwards from there.
The problem with teaching the trajectory of computing technology is with basic numeracy.  The orders of magnitude are just too big for many students to comprehend without more numeracy and feeling for log() scales.  Many of the examples here would just go over their heads as random magical numbers.
If you don't have a museums/datacenters like this in your area, you could maybe bring in props of some kind -- maybe bring in an old tape drive of some sort along with a USB and contrast how much data each of them can store.
I was born in 1968 and used dial-up to mainframes back before PCs were available. We got an Apple ][+ when they came out and thought 48K was a lot of memory. 
I live in constant wonder at the world we are now in, which seems like science fiction to my childhood self. How can I convey to students born around the turn of the millennium how amazing this growth has been? 
Then I show my students a picture of a typical rack of servers from 2008, which could perform 2-4 TFLOPs.  In a decade, the size had shrunk from a warehouse to a single rack, the computing power had doubled, and the cost was about $200K.
Engelbart's "Mother of all Demos" (also YouTube) might be another good one, though it always felt a bit dry to me, so maybe not.
You could also perhaps try showing them clips of seminal tech demos. I personally like Steve Job's original iPhone demo (YouTube link warning) -- I stumbled across this recently, and it was so incredibly bizarre and surreal to me to watch people go crazy over features I take for granted.
That is in less than 20 years.  Extrapolating from that back another 20 years only gets you to 1978; you have to go 10 years beyond that to reach 1968.  It's really hard to communicate the extent of this change.
I tell them about Moore's Law, of course, and that, when I was young, we paid 25 cents to play Asteroids, back when 25 cents was a lot of money, but I know I sound like Grampa Simpson. I share that a single Google search uses more compute power than all of the Apollo missions, but even I can't wrap my head around that.
I don't usually go back as far as the 60s.  I show my students a picture of the ASCI RED supercomputer from ~1998, which was the first supercomputer to be able to perform 1 trillion floating point operations in a second (1 TFLOP).  It's basically the size of a warehouse floor, with another floor consumed by its climate control system. It cost ~46 million dollars.
Then they might be impressed that you started out in the Paleolithic period of computing.  Or as a protozoa equivalent.
(One of my professors actually did this and took us all to the Living Computer Museum in Seattle, and made us write a program of some kind on a retro computer of our choice. It took me a while to figure out how to even use one of the computers -- it certainly gave me a deeper appreciation for how dramatically HCI has advanced over just a few decades!)
This is really difficult to communicate to anyone who hasn't lived through it (and even to those of us who have).
You could also perhaps contrast this by following this up with a field-trip to a local datacenter (if you have one).
How do you convey to students some of the wonder that you feel (as part of motivating them to appreciate and learn computer science)?
I would teach this by not talking about computers, at first, but start with one of those science picture books that shows a proton, then orders magnitudes to the height of a humun, then orders of magnitudes to the size of the known universe.  Also one of those books that show all of human history as the last fraction of a second on a 24 hour clock mapped to the age of the Earth.
Finally, I show my students a picture of a Nvidia K80 GPU card, which can perform over 2 TFLOPs.  In less than a decade, the form factor has shrunk to a card you can put inside a desktop computer, with the same power. Launch price was about $5K. 
It's pretty easy to compare the CPU power, RAM, and hard drive space of a phone to an early computer. Maybe pick a few computers from each decade and plot them out.
But something that was really effective for me was hearing stories from my (older) coworkers. About how they had to store their code on punch cards and run their programs at midnight every night because it was the only time the mainframe (read: only computer in the computer science deparatment) was not being used. About needing to wait another 24 hours to run their code again (which they fixed using tape and hole punches?). Compare that to modern development, where my IDE updates several times a second whenever I'm typing code.
Then, after students go wow over these scales, maybe an appreciation for them, only then overlay computing technology: gears to relays to tubes to discrete transistors to ICs to VLSI to 10 nm finfets SOCs in their iPhones, and put each on top of the one of pages of those order-of-magnitude books.  Give them a better sense of Moore's law scaling as it relates to the universe of scaling. 
One idea is to take your students on a field trip to a "computer museum" of some sort, if your local area has one. Being able to physically look at and maybe even interact with machines can maybe help give your students a more "visceral" feel for how computing has advanced in general.