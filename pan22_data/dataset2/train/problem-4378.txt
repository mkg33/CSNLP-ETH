In the second case, texture coordinates are computed from the position of the vertices in eye space.
In a realtime renderer, you tend to always have your artist-authored texture coordinates fixed as vertex attributes up-front. Specialist modes like view/world-sourced texture coordinates don't really occur, unless such an effect is explicitly designed into a shader.
I have been spending hours trying to understand why and how the effects would be different in both cases (why are the stripes different in both cases?). In addition, is there any particular case when we need to perform shading in the model space coordinates and likewise only in eye space coordinates?  
The use of 'shading' here is a bit off from the typical renderer meaning. The author appears to talk about how texture coordinates are computed.
What this looks to me is just sampling form a texture using texcoords.xy and then texcoords.yx or using world space coords like worldpos.xz or worldpos.xy.
The author goes on with a brief explanation claiming that the image on the left is a result of shading in model space coordinates because the stripes follow the vx value running from the tip of the spout to the handle while the image on the right is based in eye space coordinates, with the stripes following the vx value from right to left.
While reading on shading, I came across a section in which the artist provides 2 different kinds of fragment shading: