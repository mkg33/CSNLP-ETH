With all of these configurations, I never get more than ~25Mbyte/s throughput. The thing is, that if I start multiple parallel copy streams, like 3 times an rsync, i almost reach 90Mbyte/s. I also did some IOMeter tests and found out that the "chunksize" makes a huge difference, but is normally not tunable with the given tools (or is it?).
What are the bottlenecks you would think about? Do you have similar experiences? Are these the expected "natural" values?
My understanding is, that a 1 Gbit link should provide 125Mbyte/s - of course this is theory. But at least it should reach ~100Mbyte/s.
I've a question which bothers me since quite some time now. I have several environments and servers. All of them are connected to 1 Gbit ethernet swiches (e.g. Cisco 3560).
Jumbo frames are not enabled but I'm unsure if it would make a difference. TOE is enabled on all NICs.
In my experience the bottleneck are always the disks. I never used ISCSI or SAN, thus for met the only way to increase performance is using RAID0 with a dedicated raid card.