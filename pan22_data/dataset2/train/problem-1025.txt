Otherwise, a network based filesystem might make sense, as the changes would be reflected at all locations.  Nevertheless, I suspect you are transferring over the Internet, which will limit options here.
Unison is good for this, but still needs to copy files locally and it cannot detect a move/rename if also the file content changed even a little.
This should work well if all you do is create files, rename them and remove them. It would be a lot harder to detect things like copies and edits, or to optimize away the creation of a file followed by its deletion.
The idea is to have the volume manager keep track of operations between backups. Instead of making a backup of the filesystem, take a snapshot with the volume manager and back up the snapshot expressed as a diff from the previous snapshot.
You might be able to use a host based IDS such as AIDE and write a wrapper script using its output.  You would likely have to write more complex logic considering the checksums.
The idea is to have the filesystem keep track of operations between backups. Instead of making a backup of the filesystem, back up the filesystem journal (and optionally replay the changes on the backup machine, if you want a ready-to-use backup). A filesystem journal naturally expresses moves and deletions in a few bytes.
Given your workflow, I wonder if working at the file level (like what others have proposed so far) is the best solution. You could work...
With that list of files, you can use rsync to only sync those files. rsync has an option to read in a file list. Here's a test showing this example:
I made a simple Python script to detect renamed/moved files and directories using inode numbers (*nix only) and replay these changes on the synchronized machine. You can use it by itself or as a "renaming preprocessor" for Unison or rsync. It can be found here
With this solution, it would be worthwhile to have some form of journal compression. For example, if a file has been overwritten 10 times, only keep its last update in the journal. Another worthwhile optimization would be to recognize copy operations, and even better, edits (i.e., creating a file that is mostly but not completely identical to another file). I don't know if anybody has implemented this. For your workflow, I don't think it would matter much anyway.
interesting suggestions here. Also thought of using filesystem capabilities ie ZFS. Found it strange that there is no tool which does that simple thing. Unison option does not work in most cases as people report, not for me either.
I'm not sure if there's an existing tool that does this for you, but you could write a simple script that just runs a find on the base directory where mtime is newer than the last backup. This will get you a list of all files that have been modified. If a file was simply moved, it will not appear in the list. Unfortunately, this list will include the directories that the files moved into, since the directory gets updated when a file is added/removed.
Please note that I waited approximately 1 minute between running each find command. From this, it shows that when initially creating the file, it gets listed by find. If I move the file into another directory and re-run the find command, it only displays the directory I moved the file into, and not the file itself. You can use a combination of find and rsync commands to only list the files you want, it can probably achieve your goal.
Fuse makes it relatively easy to design a filesystem with specific requirements that sits on top of a “real filesystem”. I've never used it, but LoggedFS looks promising.
I want the feature to keep backup of my movie collection on second hard disk in sync when rearraring folders. 