If you are looking for ways to improve performance, maybe testing different activation functions would be a good place to start.
Your results sound like your classifier is not working at all assuming your classes are evenly distributed.
I have a binary car sound classifier. I have a feature set that is extracted from audio with size of 48. I have a model(multi layer neural network) that has around %90 accuracy on test and validation sets. (without normalization or Standardization)
Are you applying the regularization to the entire data set or the fields that have the larger magnitude? If to the whole data set, I would only apply to the fields with the greater magnitude.
You should make sure you understand your raw data and the assumptions of each feature scaling technique before application. Accuracy-driven machine learning might lead to the wrong conclusions.
I see that the feature values are mostly around [-10, +10] But there are certain features with a mean of 4000. Seeing unproportional values within features I though some feature scaling might improve things. So using scikit-learn tools I tried: 
While some NNs are sensitive to magnitude differences, I personally donâ€™t find data regularization necessarily that helpful. 
There could a skewed power envelope or non-stationary data. As a result, off-the-shelf feature scaling could attenuate the signal.
There are feature scaling techniques that tend to work better for audio signals, examples include: RMS level (Root Mean Square Level), Cepstral Mean Subtraction (CMS), RelAtive SpecTrAl (RASTA), kernel filtering, short time gaussianization, stochastic matching, and feature warping. 