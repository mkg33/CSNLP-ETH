A permutation of this idea is to get the embedding for the interesting words, and then get a distance measure (I assume cosine distance) to the labels you have. Then you have a problem, as they may be distributed evenly across labels in terms of distance, but if there is a group of words that are close to a particular label, that can be interpreted as informative. 
I am sure there is a better technique to do this, so I am curious as well :). You may want to check these out (but they use additional modeling to get salience)
A NEW METHOD OF REGION EMBEDDING FOR TEXT CLASSIFICATION - this deals with the exact problems of how to identify the useful bits of an input text 
First thought (which is not the best method, but could be worth a quick try if your examples are smallish) - Run the text with all words through fastText, and get the baseline most likely labels with probabilities (predict-prob function). Then remove each word that you are interested in testing, (maybe removing common words like 'a' 'the' etc), and compare the predicted probabilities of each obtained set of predicted_probs. The ones that give the greatest difference between baseline and missing wordX can be interpreted as the most informative. 