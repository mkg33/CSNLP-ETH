I don't think you can do this in the browser though as you don't get enough control. You could run a local proxy that cut off the connection after a set amount of data or time though. Try Fiddler if you are on Windows.
In Firefox I often open logs from our CI server. Usually these have few hundred kB, but sometimes these can be up to several gigabytes in size. In this case, Firefox gets stuck (all tabs) and I have to wait few minutes until it processes the Back button press.
The way round this is to have more "smarts" on the server. You need a log browser that is capable of doing a "tail" on the log file. It should then refresh every few seconds.
So the browser is behaving as expected since the file it is loading is enormous and must fully load before the browser knows it has the end of it - unlike an HTML page where rendering can start early if enough information is available.
Naturally, I don't want to read through such pages (I rather ssh to the server and read them using better tool such as less). The server may not properly inform about the page size in headers, the logs are updated via AJAX when the test is running.
You could roll this yourself using PERL, PHP, Python or Node.JS very easily if you are allowed to add such things to a web server with access to the log files. There are also packages available for all of those languages.
Is there any setting/plugin which would block downloading/rendering the pages which are over certain size/after timeout? Or warning me that the page is pretty large, if it can't process it in chunks.