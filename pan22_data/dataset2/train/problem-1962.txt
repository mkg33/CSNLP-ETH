One thing to be very careful about is that the pagesplits/sec counters Aren't really reliable. They keep track of all page splits. (sql 2012 is better) What I mean by all is that when you insert a record at the very end of an index, effectively you need a new page added to the index to continue. The adding of the new data page is counted as a pagesplit as well. But in reality nothing is split. So in order inserts on a cluster index will still have page splits when you look at the pagesplit/sec counter, but they aren't bad. So only look at this counter in relation with other facts. For example: Unexplainable high trans log load together with high  out of order inserts together with high page splits/sec. 
Page plits cause extra load on the log file (potentially a lot). Extra writes to the data files (higher checkpoint peaks). And read ahead reads become less effective because of fragmentation. However, low fill factors, eat up buffer space, increase read load. 
One last word of warning. Especially on large tables. Always check what the current page density is of your index and calculate what the size impact would be when you rebuild the index with a different fill factor. (Rebuilding an index with a 100% page density  with a new 50% fill factor will effectively double the size of the index)
The basic idea is that you want your pages as full as possible. The more rows are packed into a data page, the less pages you need to read and keep in your buffer pool.
Keep in mind, that you should evaluate this over time. Your workload, (data mod) might change over time. 
All of the above will cause fragmentation.  Now that's bad, but in a strange way good. Because that is about the only thing that can properly help you decide on the proper fill factor. I'll explain why.