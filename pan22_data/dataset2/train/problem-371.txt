CNN are (I think) invariant to small translations of the input image, i.e., they will classify to the same class an image $X$ and an image $X'$ such that all pixels have been translated along a vector $\mathbf{v}$ by some "small" distance $d$. They're not, however, invariant to rotation. I.e., if $X'$ is obtained by $X$ applying a rotation of arbitrary degree $\theta$ around an axis $z$ orthogonal to the plane of the image, they won't necessarily classify it to the same class as $X$. Practitioners usually solve this by data augmentation, but this is unnecessarily wasteful and anyway not an option in my case. Is there a way to make the NN architecture invariant to both isometries? I don't have to use a CNN, but since CNN already enjoy some sort of translation invariance, I figured it would be easier to modify them to get rotational invariance , than to use a completely different architecture.
“Transformationally Identical and Invariant Convolutional Neural Networks by Combining Symmetric Operations or Input Vectors”
Please see 3 recent pre-prints (6-8/2018) about transformation-identical CNN (TI-CNN) and geared rotation-identical CNN (GRI-CNN). They were expanded and derived from the circular path CNN (SPIE MI 1998 and IEEE TMI 2002) initiated by the same medical AI research team at Georgetown University medical center who also independently developed the CNN and the wavelet CNN in early 1990s for cancer research.
“Transformationally Identical and Invariant Convolutional Neural Networks through Symmetric Element Operators”