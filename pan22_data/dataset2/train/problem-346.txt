While it is very hard to gauge performance in advance, you can do some testing on an operational rig to determine whether a GPU upgrade is going to be useful. Theoretically, if your GPU usage never reaches 100%, a faster model would probably just have more idle time, whereas full usage indicates an upgrade would allow the GPU to do more work. This reasoning holds for the central processing unit as well. There are plenty of ways of getting these statistics during load, an example guide can be found here.
There's a catch. Having a GPU do more work does not necessarily result in a perceived or useful performance increase. For example, it could boost the average framerate from 250 fps to 500 fps, but not prevent frequent framedrops that are caused by a bottleneck elsewhere. The GPU could also be doing busy work, throwing most of the calculations away. Keep an eye out for other stats as well. Knowing the CPU, HDD, RAM etcetera are not bottlenecks, will help you determine whether the GPU is.
But it's simple to max out a GPU. Just get a "gaming CPU", for example the Core i5 2500K is the king of that field. With it's cheap price, and kickass power, it will surely max out any GPU.
There are no lookup tables to determine what GPU is the best fit for your system. You mentioned 'Arma 2' as something being very CPU intensive. Let's go one step further and take a look at Dwarf Fortress. Although the game is not coded to support it, in terms of processing power, it could run on a machine with no GPU at all, as long as the system can output text to a display. Cards cannot really be ranked on a vague term as processing power anyway. To give a very general indication, some cards excel at handling complex scenes, while others may cope better with large textures and high display resolutions, or offer a fancier set of features.