However, what you want (one model to solve both tasks) is most certainly possible and is usually referred to as Multi-Task Learning. In this framework there usually is a large network that branches off to $M$ smaller ones (one for each task). The shared layers are supposed to learn from all $M$ tasks and outperform networks trained individually. If you would like to see some research done in the area I would suggest following MIT's affective computing group (example MTL applications: 1, 2, 3, 4, 5).
In your case you would have a network with two outputs, one for time series prediction and one for classification. Many machine learning libraries allow you to define two cost functions (one for each task) and even have a hyper-parameter you can tune to select the importance of each task. I've done a similar thing with keras' functional api.
I think the most intuitive solution would be to have two networks (i.e. one for predicting the next values in the time series and one for classifying if it is or isn't a seizure), because these are two very different tasks and there are different models that excel at each.
The classification network could even be on top of the time series predictor (include the predicted values to assess if it is a seizure or not).