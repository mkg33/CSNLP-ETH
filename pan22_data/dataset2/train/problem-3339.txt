These kernel modules are available to certain OSes (Linux, BSD, and Windows). With these drivers installed in your VM, the kernel in your VM has special access to the underlying hardware through the kernel that's running on your hypervisor.
For example, here I have 2 servers that are running on 2 different hypervisors. Using iperf on both servers:
KVM is more akin to a type-2, but has some elements, such as virtio_*, that make it behave and perform more like a type-1, by exposing to virtualization the underlying Linux kernel of the hypervisor in such a way that VMs can have semi-direct access to it. 
Given you're running on a paravirtualized hypervisor you have to go onto the actual hypervisor to find out your NIC's theoretical speed using ethtool. In lieu of that can only find out by doing something like using iperf to benchmark the NIC under load, and experimentally find out what the NICs speed appears to be.
To expand a bit on this because I too recently came into this and was also semi-confused by the lack of speed details when running ethtool on a VM:
Remember that with hypervisors there's 2 distinct types. ESX/vsphere are considered type-1. Reminder on the types:
This is telling us that the device driver being used for this VM is virtualized, in this case this is a VM running on KVM and so the VM is using the virtio_* drivers for all its interactions with "hardware".