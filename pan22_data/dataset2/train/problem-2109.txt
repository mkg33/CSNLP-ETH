As pointed in the comments, there is a distinction to make between the importance of a feature for the model, i.e., how many times it is used, and the importance for the accuracy of the prediction. These two points might be unrelated.
I don't think there is a definite answer on how to rank features with respect to the accuracy either, as the information contained in one feature can be a duplicate from another one, or be useful only if you have another feature to complement it.
One simple method to approach this problem is the reverse of iterative feature selection, which you can read more on Feature Selection (Wikipedia). Given a model with 10 features, you can build 10 models with 9 features, removing a different one each time, and compute their loss on a validation set. If the 2nd model out of your 10 models performs the worst, then the 2nd feature is the most important. If the 9th model, without feature #9, is still as good as the full model, then feature #9 is not important, and you can use the difference in your loss metric to measure the importance to accuracy. However, it might be the case that feature #9 becomes useful if you add a new feature and that feature #2 can be built by a combination of other features, which you'd catch if you allowed your model to be more complex.
For example, consider that you want to predict a continuous target $y$ from two features, $x_1, x_2$, where the ground truth is $y = x_1 + x_2$. Assume that $x_1$ can either be $0$ or $100$ and $x_2$ is a continuous variable ranging from $0$ to $1$. Training a model that fits exactly your training set will have only one split for $x_1$ and as many splits on $x_2$ than distinct training values of $x_2$. In this case, you can make $x_2$ arbitrarily more important that $x_1$ by increasing the number of distinct training points, but $x_2$ is mostly irrelevant to the accuracy of the prediction when compared to $x_1$. This problem makes it impossible to judge the importance of a feature based on the number of time it is used, in the general case.