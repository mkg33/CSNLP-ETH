Or should I just be aware that less-than-average performance should be a database check and going back to the proverbial chalkboard?
In a perfect world everything would be designed perfectly from the start and developed using a logical build sequence.  In the real world there are constraints on budget and time and your test data might not end up looking like your production data.  For this reason, I say use common sense to avoid problems proactively but concentrate your limited resources tuning the things that turn out to be real problems instead of spending time and money you probably don't have on looking for imaginary or potential problems.
IMHO, this shouldn't be a reactive process at all - you should never wait until a change causes a performance problem in production to start reacting to it. When you make the change in dev/test etc., you should be testing those changes with similar data on similar hardware with the same apps and similar usage patterns. Don't let these changes get rushed out to production and surprise you. This will almost always happen when it isn't convenient to spend a day tuning - budget for that tuning time well in advance.
Your planning for retroactive tuning is very pragmatic.  When you are testing you should document expected timings and throughput and at times should actually build in analysis that lets you know when production processes are not meeting design specifications.  In this way you may be able to identify in advance what code needs to be tuned.  You can then determine not just what the problem is, but why you didn't catch it in the design/test phase.
When you find out that something isn't performing up to your design specs, or if something is falling into the bottom 10% or 20% of your profiler's list of response times, then invest the time you need to tweak whatever it is that is broken.
My question is, should query tuning be proactive or reactive?  In other words, a few weeks after some heavy code/database modification, should I just set aside a day to check out query performance and tune based off of that?  Even if it seems to be running okay?
But, just like any other software developer, there is added functionality, bugs, and just change of requirements that demands altered/created database objects.
After that, time's (and money's) a-wasting so get on with it and deliver your product.  Instead of taking a ton of design time tuning queries that may or may never turn out to be bottle necks, use that time for extra testing, including load testing.
In a perfect world all tuning would be done in the design phase proactively and nothing would be reactive, but the world isn't perfect.  You will find that test data sometimes isn't representative, test cases will have been missed, loads will be unexpectedly different, and there will be bugs that cause performance issues.  These situations may require some reactive tuning, but that doesn't mean reactive tuning is preferred.  The goal should always be to catch these up front.
OK, I'll bite and take a contrarian view.  First off, I would say you should never start by doing something that you know will lead you into trouble.  If you'd like to call this applying best practices, go ahead.  This is as far as being proactive should go.
As a software developer and an aspiring DBA, I try to encorporate best practices when I design my SQL Server databases (99% of the time my software sits on top of SQL Server).  I make the best possible design prior to and during development.
Query tuning can take up a lot of time, and depending on the initial database design it could be minimal benefit.  I'm curious as to the accepted modus operandi.
For me, performance testing has always been part of the development process. Want to change this table, alter this report, add this feature? As part of testing you make sure that you can compare individual and overall performance to known baselines and/or against the requirements (e.g. some reports run in the background or are otherwise automated, so performance - or rather speed - of every single query in the system isn't always the top priority).