My solution for easy access to logs is an S3 event notification on my log-collecting buckets, which sends a message into an SQS queue.  The queue consumer runs on an EC2 instance with an EBS Cold Storage (sc1) volume.  When each log file is written to the bucket, the queue consumer fetches the file, and derives the date from the filename.  It then parses the log events to determine their HTTP status class, e.g. 2XX, 3XX, 4XX, 5XX, or other/unmatched, and appends each record to a master daily file.  The records with 4xx, 5xx, or unmatched/unexpected are appended to smaller daily files with errors only.  Searching these local files with a tool like grep then becomes a trivial task.
Logs for troubleshooting are often needed quickly after the events occur, so it often desirable to receive them as soon as practical, and that is what S3 tends to do.
Objects in S3 are immutable -- it isn't possible to directly append data to an S3 object, and doing so requires an emulation operation: the bytes of the object must be copied into a new object, followed by the additional data.  This would make logging into a single "growing" daily log file nearly impossible to do at any scale.  The log files are standard S3 objects, so this is likely another reason why the individual files are written as they are.
It isn't one file per request, although it can certainly seem like that on a bucket with low traffic.  Essentially, each log file contains records created prior to its timestamp, but not necessarily records since the last log was written -- a log file can occasionally contain records from hours, days, or weeks ago that have been stranded somewhere inside S3 and have finally been released.  This is rare, but a documented possibility.
S3 is a distributed system, and this is at least one factor in the large numbers of log files it generates.