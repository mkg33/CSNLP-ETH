Another common reason for denormalization is to permit some change in storage structures or allow some other physical optimization that the DBMS wouldn't otherwise permit. According to the principle of Physical Data Independence a DBMS ought to have the means to configure internal storage structures without needlessly altering the logical representation of data in the database. Unfortunately many DBMSs are very restrictive of the physical implementation options available for any given database schema. They tend to compromise physical database independence by only supporting a sub-optimal implementation of the desired logical model.
It ought to be obvious but it still needs to be said: in all cases it is only changes in physical implementation features that can dictate performance - features such as internal data structures, files, indexing, hardware and so forth. Normalization and denormalization have nothing to do with performance or storage optimization.
One potentially sensible reason to apply controlled denormalization is if it enables you to apply some integrity constraint to the data that wouldn't otherwise be possible. Most SQL DBMSs have extremely limited support for multi-table constraints. In SQL sometimes the only effective way to implement certain constraints is to ensure that the attributes involved in the constraint are all present in the same table - even when normalization would dictate that they belong in separate tables. 
Controlled denormalization means that mechanisms are implemented to ensure that inconsistences can't arise due to redundant data. The cost of these extra controls and the risk of inconsistent data need to be considered when deciding whether denormalization is worthwhile.