The point is - if there's one script only for all users (the idea of shared libraries) it's not only easier to maintain as pointed out earlier here, but the built-in caching mechanisms should be more efficient (at least from my understanding of the subject). One file to read, easy way to cache in memory. Multiple identical files (not sym-/hardlinked, different inodes) - this is more problematic. Or am I underestimating Linux capabilities here? Please correct me if I'm wrong - my knowledge of the internals is not that profound, but I think this would result in independent seeks/buffering. 
Another point - using opcode caches. I'm not sure about how it looks in different implementations, but in most cases using multiple identical scripts would result in unnecessary cache entries. However, there's one potential problem with shared scripts - when caching gets too aggressive (like caching external configuration files between different users). I had recently encountered this kind of behaviour with XCache and hardlinks. With proper configuration and non-buggy implementations it's not a problem though.
If it's feasible, I would use shared installations both for convenience and performance. However, in some situations the difference in performance can be negligible, so it's probably "what fits your situation better".
I think it's the other way round. It shouldn't be slower. The question is, "will running it shared make it faster"? As to that point - theoretically, yes. But the practical difference may be indiscernible.