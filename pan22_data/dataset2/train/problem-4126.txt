Summing up, failure rates for disks doesn't drive the rules for how many spindles are in a RAID group, performance does. For the most part.
The number of spindles in an array doesn't really worry me over much, as it's pretty simple to get to 6.5TB with 500GB drives on U320. If doing that, it would be a good idea to put half of the drives on one channel and half on the other just to reduce I/O contention on the bus side. SATA-2 speeds are such that even just two disks transferring at max-rate can saturate a bus/channel. 
SAS disks have a lower MTBF rate than SATA (again, this is beginning to change) so the rules are less firm there.
RAID 5 I'd say 0 drives per array. See http://baarf.com/ or similar rants by any number of other sources. 
I use 7 as a "magic" maximum number. For me, it's a good compromise between space lost for redundancy (n this case, ~14%) and time to rebuild (even if the LUN is available while rebuilding) or increase size, and MTBF.
In any of the RAID levels that use striping, increasing the number of physical disks usually increases performance, but also increase the chance of any one disk in the set failing. I have this idea that I shouldn't use more than about 6-8 disks in a given RAID set but that's more just passed down knowledge and not hard fact from experience. Can anyone give me good rules with reasons behind them for the max number of disks in a set?
The recommended maximum number of disks in a RAID system varies a lot. It depends on a variety of things:
SATA-2 is 300MB/S interface spec, SCSI Ultra 320 would be more consistent. You're talking 6-10 disks because you won't hit peak too often.
For SATA-based RAID, you don't want to have more than about 6.5TB of raw disk if you're using RAID5. Go past than and RAID6 is a much better idea. This is due to the non-recoverable read error rate. If the size of the array is too large, the chances of a non-recoverable read error occurring during the array rebuild after a loss get higher and higher. If that happens, it's very bad. Having RAID6 greatly reduces this exposure. However, SATA drives have been improving in quality lately, so this may not hold true for much longer. 
All-in-all, 5-7 has worked for me. Sorry, no scientific data from me either, just experience with RAID systems since 2001.
RAID 6 I'd say 5 drives + 1 for each hot spare per array. Any less and you might as well do RAID 10, any more and you are pushing the risk factor and should go to RAID 10.   
Obviously, this has worked great for me when working with SAN 14-disk enclosures. Two of our clients had 10-disk enclosures, and the magic number 7 was reduced to 5.
There are FC arrays that use SATA drives internally. The RAID controllers there are very sophisticated, which muddies the rules of thumb. For instance, the HP EVA line of arrays groups disks into 'disk groups' on which LUNs are laid out. The controllers purposefully place blocks for the LUNs in non-sequential locations, and perform load-leveling on the blocks behind the scenes to minimize hot-spotting. Which is a long way of saying that they do a lot of the heavy lifting for you with regards to multiple channel I/O, spindles involved in a LUN, and dealing with redundancy.