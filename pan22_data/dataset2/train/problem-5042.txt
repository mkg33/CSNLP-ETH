The concept of potential is related to many different areas of physics. In cs, potential is used in amortized analysis of data structures. We can look at how each step affects the entropy of the system and  therefore get an average (amortized) cost of an operation with a given data structure. This has given rise to many theoretically better data structures like the fibonacci heap.
A result of Pour-El and Richards Adv. Math. 39 215 (1981) gives the existence of noncomputable solutions to the 3D wave equation for computable initial conditions by using the wave to simulate a universal Turing machine. 
Much of the math that we use was originally invented to solve physics problems. Examples include calculus (Newtonian gravity) and Fourier series (heat equation).
A very old example (which could be subsumed by Suresh's answer, however, this is a different tack) is the influence of the theory of electrical networks, e.g. Kirchhoff's circuit laws, on combinatorics, graph theory, and probability.
Statistical physics has given computer scientists a novel way of looking at SAT, as overviewed here. The idea is that as the ratio of clauses to variables involved in a 3-SAT formula increases from around 4 to around 5 we go from being able to solve the vast majority of 3-SAT instances to being able to solve very few. This transition is regarded as a "phase change" in SAT.