Shell scripts aren't the best way to handle filenames with spaces in them: you can do it, but it gets clunky.
Using find -print0 combined with xargs -0 is completely robust against legal file names, and is one of the most extensible methods available. For example, say you wanted a listing of every PDF file within the current directory. You could write
I reset IFS after its use in find, mostly because it looks nice, I think. I haven't seen any problems in having it set to newline, but I think this is "cleaner".
Use find -print0 and pipe it to xargs -0, or write your own little C program and pipe it to your little C program.  This is what -print0 and -0 were invented for.
This will find every PDF (via -iname '*.pdf') in the current directory (.) and any sub-directory, and pass each of them as an argument to the echo command. Because we specified the -n 1 option, xargs will only pass one argument at a time to echo. Had we omitted that option, xargs would have passed as many as possible to echo. (You can echo short input | xargs --show-limits to see how many bytes are allowed in a command line.)
We can clearly see the effect xargs has on its input — and the effect of -n in particular — by using a script which echoes its arguments in a more precise manner than echo.
Another method, depending on what you want to do with the output from find, is to either directly use -exec with the find command, or use -print0 and pipe it into xargs -0. In the first case find takes care of the file name escaping. In the -print0 case, find prints its output with a null separator, and then xargs splits on this. Since no file name can contain that character (what I know of), this is always safe as well. This it mostly useful in simple cases; and usually is not a great substitute for a full for loop.
You can set the "internal field separator" (IFS) to something else than space for the loop argument splitting, e.g.