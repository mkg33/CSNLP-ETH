I will add my $.02 because there is a key area that seems not to have been addressed in all of the previous posts - machine learning on big data!
For big data, scalability is key, and R is insufficient. Further, languages like Python and R are only useful for interfacing with scalable solutions which are usually written in other languages. I make this distinction not because I want to disparage those using them, but only because its so important for members of the data science community to understand what truly scalable machine learning solutions look like.
I do most of my work with big data on distributed memory clusters. That is, I don't just use one 16 core machine (4 quad core processors on a single motherboard sharing the memory of that motherboard), I use a small cluster of 64 16 core machines. The requirements are very different for these distributed memory clusters than for shared memory environments and big data machine learning requires scalable solutions within distributed memory environments in many cases.
We also use C and C++ everywhere within a proprietary database product. All of our high level stuff is handled in C++ and MPI, but the low level stuff that touches the data is all longs and C style character arrays to keep the product very very fast. The convenience of std strings are simply not worth the computational cost.
If you want to go beyond R, I'd recommend learning python. There are many libraries available such as scikit-learn for machine learning algorithms or PyBrain for building Neural Networks etc. (and use pylab/matplotlib for plotting and iPython notebooks to develop your analyses). Again, C/C++ is useful to implement time critical algorithms as python extensions.
Now talking about C, C++ or even Java.  They are good popular languages. Wether you need them or will need them depend on the type of job or projects you have.  From personal experience, there are so many tools out there for data scientist that you will always feel like you constantly need to be learning.  
Not sure whether it's been mentioned yet, but there's also vowpal wabbit but it might be specific to certain kinds of problem only.
Take a look at Intel DAAL which is underway. It's highly optimised for Intel CPU architecture and supports distributed computations. 
You can add Python or Matlab to things to learn if you want and keep adding.  The best way to learn is to take on a work project using other tools that you are not comfortable with.  If I were you, I would learn Python before C.  It is more used in the community than C.  But learning C is not a waste of your time. 
Do the opposite: learn C/C++ to write R extensions. Use C/C++ only for the performance critical sections of your new algorithms, use R to build your analysis, import data, make plots etc.
There not many C++ libraries available which offer distributed, scalable machine learning capabilities - MLPACK. 