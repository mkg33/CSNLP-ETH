Short Stroking a traditional hard drive reduces latency for random reads/writes. "Short Stroking" a SSD gives the drive controller more unused blocks for garbage collection/wear leveling routines so it won't speed up the drive but it will increase longevity and prevent the speed loss that is seen when a SSD fills up.
I would say typically 15%, however with how large hard drives are now adays, as long as you have enough for your temp files and swap file, technically you are safe.  But as a safe practice, once you hit 15%, time to start thinking of doing some major cleanup and/or purchasing a larger/second hard drive.
SSDs add a new layer to this. Wear Leveling and Write Amplification. For these reasons you want more free space than you absolutely need on traditional hard drives.
Yes, depends on usage and underlying storage system. Some systems, like high end SAN based disk arrays laugh at file fragmentation making the only system impact of fragmentation is OS overhead in scattering things all over hither and yon. Other systems, like laptop, drives are another story all together. And that doesn't get into newer file systems, such as ZFS, where the concept of a hard limit to space is nebulous at best. 
NTFS is its own beast, of course. These days I give C:\ a total size of 15GB for XP systems, and haven't played with Vista/Win7 enough to know what to recommend there. You really don't want to get much below a GB free on C:. Using Shadow Copies means you should keep more 'empty' space around than you otherwise would, and I'd say 20% free-space is the marker for when more needs to be added or a clean-up needs to happen. 
But for data storage however it's only a matter of data growth rate which needs to be monitored and/or estimated when setting up the monitoring... if the data doesn't grow very fast on for example a 1TB volume - a few % for the warning threshold would be fine and I'd be comfortable with 90-95% utilization. If the growth rate is higher, adjust it down to get notified in time... fragmentation can often be dealt with if the data isn't growing and just changing with scheduled defrags.
I'd always try and keep around 50% free on system volumes of any kind, and possibly smaller data volumes. Sizing 2 for 1 if possible and I'd set a warning threshold at 75% or something.
I try to keep the used space under 80%. Above this number the filesystem generally has to work harder to place data on the disk, leading to fragmentation.
For plain old NTFS data volumes, I get worried when it gets under 15%. However, if the drive is a 1TB drive, 15% is still a LOT of space to work with and allocate new files into (the converse being that it takes a lot longer to defrag). 
You still don't want to fill the drive but with SSDs the immediate effect isn't there and the reason why is different.