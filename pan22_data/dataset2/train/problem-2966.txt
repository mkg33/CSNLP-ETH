The behavior you describe is determined by the sending client (software).  A common misconception about TCP & UDP implementations is that there is only one way to code, but there are choices to make when coding...and not all programmers make the same choices.  The RFCs are your friend.
"When an application issues a series of SEND calls without setting the PUSH flag, the TCP MAY aggregate the data internally without sending it. Similarly, when a series of segments is received without the PSH bit, a TCP MAY queue the data internally without passing it to the receiving application."
The MAY in that sentence offers the programmer (of the sending client) the choice of whether or not to set the PSH bit.  In the case you describe, the programmer chose to prioritize sending the payload up the stack to the application over buffering a series of them to send up the stack at once.
"I think ... this behaviour is just because every chunk of the transaction is big and must be sent to the upper layers as fast as possible to avoid a huge buffer and that this behaviour is used in real-time and interactive applications"