If it was that easy then there wouldn't be any need for high resolution input. You're interpolating data that isn't there and that causes the blurriness. It's the same as taking a low resolution image and increasing the resolution.
If you want your user interface and text to be larger you need to change the scaling setting rather than lowering the input resolution of the display. The scaling setting will render the interface and text differently so that more pixels are used to render the same menu/button/text/...
I'm also looking at buying a high-DPI external display (24" 4K) so the question could also apply to that (attached to either a GTX 970 or Intel Iris 540).
Unfortunately, scaling implementations are not always perfect and not all software will respond well to it.
When I try it though, the image is very blurry. So I have two questions. Firstly, is it the display or the GPU responsible for adding this blur (or impossible to say)? And secondly, is there anything that can be done to prevent it?
I have a high-DPI display (13" 3200x1800) and figured that if I set it at 1600x900 (exactly 1/4 native - 1/2 width and 1/2 height) then each pixel outputted should be represented by a 2x2 block of actual pixels on the display, crisp and clear as if it was natively a 1600x900 display.