But I'm not sure whether this is practically possible given the tree structure and the splitting strategy involves find the nodes which correspond to the most reduction in entropy in the model or highest gain in information gain.
Alternative (non-tree based) models will be able to make a differentiation by three classes without any problem (ISL, Ch. 4). One example is a logistic model (or "logit") of form risk = b0 + b1*savings. In this models you can also calculate marginal effects, telling you, in case someone moves from class A to class B, by how much will the probability of being a "bad risk" change (marginal effects).
Single decision trees often do not have a very good predictive capacity (see. Introduction to Statistical Learning, Ch. 8.2). If you are interested in accuracy of prediction, you should go a step further and grow a random forest with "bagging" or even better "boosting" on many trees (or "ensambles of trees"). In this case, many trees are grown, and they all together make a "vote" on how to predict some outcome.  
In summary: If you are interested in prediction, don't stick to simple decision trees, but move on to something else. 
Prominent boosting methods are e.g. catboost (https://catboost.ai/docs/concepts/about.html) or lightGBM (https://lightgbm.readthedocs.io/en/latest/).
From my understanding of the question, Technically, I think it is perfectly reasonable to have a decision tree/forest splitting into 3 or more nodes from the root node. Kindly check this example if you not. In this example, the predictor variable is whether to play tennis on a given day or not depending on how the climate is on that day.
I think that scikit-learn only implements binary trees. However, you can turn your example into a binary tree so you can use scikit-learn:
P.S: If this answer doesn't give you any new information or clarity, please say so. I'll remove it in order not to misguide or anyone.
You can find a bunch of good code directly covering topics mentionned here and discussed in the book "Introduction to Statistical Learning" online: https://github.com/JWarmenhoven/ISLR-python
IMO decision trees are - by definition - designed so that the single "best" split is chosen in each step (Introduction to Statistical Learning, Ch. 8.1). I think you need to split on values low, medium, high, in which case a first split would e.g. occur at (low) vs. (medium, high) and a later split would be between (medium, high), whatever gives the best fit. 