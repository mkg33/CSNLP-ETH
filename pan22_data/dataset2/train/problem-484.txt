The conventional wisdom that it needs to be 60 F (or whatever) in a server room so the hard drives don't fail is false . . . studies (by Google and others) have shown that increased temps in server rooms didn't make a significant difference in hard drive failure rates (up to a ceiling of maybe 80 F or something like that).
It's almost always cheaper to properly ventilate the enclosure(s) than it is to pay extra for the cooler drive (which, in general, has lower performance anyway).  Especially when you know that the drive may not last regardless, but the cooling solution will still be around for the next drive, so the overall cost is less.
I would get the "better" hard drives that run cooler assuming the cost difference isn't too crazy but even then it could be too hot so you have to get the HVAC under control too.
Have you ever had to alter your deployment plans because of thermal dissipation issues with your chosen hard drives?  Was your solution to increase the cooling solution, or to move to a cooler drive solution?
Probably not exactly what you're looking for, but I had a beefed up workstation that I used as a dev server about 7 years ago.  I put an Adaptec SCSI and 4 9GB ultrawides in it.  I tried adding about 3 different types of case fans that pulled in air from different places and that didn't even do it.  Eventually just took the cover off the case.  Made a great space heater under my desk.
Probably best to do a combination of both . . . you can beef up the HVAC of the room but just like with hard drives . . . it's going to fail eventually and you need the hard drives to work in hotter that normal conditions just for those times.  In data centers they are always tweaking the HVAC because as they add servers they increase the heat and at a certain threshold they'll add another AC unit.