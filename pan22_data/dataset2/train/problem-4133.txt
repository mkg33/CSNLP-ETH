backup.20091118.gz.gpg , as you can see 20091118 part is the one identifies the file date which is 2009/11/18 (year,month,day) . 
You might want to look into smarter and more efficient backup technology than just archiving complete snapshots.  OS X has Time Machine which can make hourly backups.  It only stores changed files, so despite being uncompressed its very space efficient.  Due to some clever indexing, after the initial backup its far faster than a full snapshot.  And because it just stores the files, no fancy archive format, recovering a file from backup is as easy as copying a file.
I have a windows server machine that takes daily backup daily and each daily backup takes 1.5gb thus every night, I want to remove backup files that are older than 1 week.
I am planning to write a quick batch script for this and schedule it via task manager, is this a good idea? If so I would be greatful for assistance at the batch script part.
Instead of worrying about the age of the files, first delete old backups until there are only 7 daily backups remaining and then worry about deleting the oldest file in the directory before we do each new backup.
There's likely something similar for Windows.  Seagate Replica and Genie Timeline are two possibilities.
The only real trick is the command DIR /B /O-D which lists plain file names sorted by date, oldest last.  We use the FOR loop to capture each file name in the OLDEST variable so when the loop is done %OLDEST% will expand to the name of the oldest file.