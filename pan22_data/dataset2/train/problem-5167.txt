The x64 chips can run 16-bit code (in some operating modes), and can interpret 16-bit instructions. When doing so, the bits that make up the AX register are half of the bits that make up the EAX register, which are half of the bits that make up the RAX register. So anytime you change the value of AX, you are also changing EAX and RAX, because those bits used by AX are part of the bits used by RAX.  (If you change EAX by a value that is a multiple of 65,536, then the low 16 bits are unchanged so AX would not change.  If you change EAX by a value that is not a multiple of 65,536, then that would affect AX as well.)
Now, if you're on an 8-bit CPU, when you write to memory, you may find some restrictions about being able to refer to an address of 8-bits, not an address of 4 bits or 16-bits. The details will vary based on the CPU, but if you have such restrictions, then the CPU may be dealing with 8-bit words, which is why the CPU is most commonly referred to as an "8-bit CPU".
Now, why did the bits get split into the A and D registers instead of the A and B registers, or the C and D registers? Well, once again, this is a sample scenario that I'm using, meant to be rather similar in concept to a real Assembly language (Intel x86 16-bit, as used by the Intel 8080 and 8088 and many newer CPUs). There might be some common rules, such as the "C" register typically being used as an index for counting operations (typical for loops), and the "B" register being used for keeping track of offsets that help to specify memory locations. So, "A" and "D" may be more common for some of the common arithmetic functions.
So humans always deal with the problem of not having enough unique symbols. Unless the computer has a system to deal with this, it would simply write 0, forgetting that there was a number extra. Luckily, computers have an "overflow flag" that is raised in this case.
There are no more numbers to add, so we simply create a slot and inser 1 because the overflow flag was raised.
When you add two numbers that are 8 bit, the biggest number you can get (0xFF + 0xFF = 1FE). In fact, if you multiply two numbers that are 8-bit, the biggest number you can get (0xFF * 0xFF = 0xFE01) is still 16 bits, twice of 8-bits.
xx and xx are part of two other registers, named the "B" register and the "C" register.  The reason that I didn't fill those bits with zeros or ones is that an "ADD" instruction (sent to the CPU) may result in those bits being unchanged by the instruction (whereas most of the other bits I use in this example may get altered, except for some of the flag bits).
A computer uses bits to encode numbers, but that is not important. That's just the way engineers chose to encode stuff, but you should ignore that. You can think of it as a 32 bit computer has a unique representation of more than 4 billion different values, while us humans have a unique representation for 10 different values.
Now, you may be assuming that an x-bit processor can only keep track of x-bits.  (For example, an 8-bit processor can only keep track of 8 bits.)  That's not accurate. The 8-bit processor receives data in 8-bit chunks. (These "chunks" typically have a formal term: a "word". On an 8-bit processor, 8-bit words are used. On a 64-bit processor, 64 bit words can be used.)
32bit and 64bit refer to memory addresses. Your computer memory is like post office boxes, each one has a different address. The CPU (Central Processing Unit) uses those addresses to address memory locations on your RAM (Random Access Memory). When the CPU could only handle 16bit addresses, you could only use 32mb of RAM (which seemed huge at the time). With 32bit it went to 4+gb (which seemed huge at the time). Now that we have 64bit addresses the RAM goes into terabytes (which seems huge). 
Now, getting back to the "40" in the above example: that is a series of bits, often called the "flags register". Each bit in the flags register has a name. For example, there is an "overflow" bit that the CPU may set if the resulting is bigger than the space that can store one byte of the results. (The "overflow" bit may often be referred to by the abbreviated name of "OF". That's a capital o, not a zero.) Software can check for the value of this flag and notice the "problem". Working with this bit is often handled invisibly by higher-level languages, so beginning programmers often don't learn about how to interact with the CPU flags. However, Assembly programmers may commonly access some of these flags in a way very similar to other variables.
On a hardware level, the computer works on single bits using exactly the same method. Luckily, that is abstracted away for programmers. Bits is only two digits, because that is easy to represent in a power line. Either the light is on, or it is off.
There are more flags and registers than just the ones that I've mentioned.  I simply chose some commonly used ones to provide a simple conceptual example.
Whenever a value becomes greater than the number of unique symbols, 9 in a humans mind, you add one to the number to the left.
Each CPU instruction should have some documentation, used by people who program in Assembly. That documentation should specify what registers get used by each instruction. (So the choice about which registers to use is often specified by the designers of the CPU, not the Assembly language programmers. Although, there can be some flexibility.)
A computer does it exactly the same way, except it has 2^32 or even better 2^64 different symbols, instead of only 10 like humans.
In your mind you only know 10 different digits. 0 to 9. Internally in your brain, this is certainly encoded differently than in a computer.
(There is also commonly an "underflow" flag, in case you subtract too much to fit in the desired result.)
Most of the content of this answer originally came from this answer (written before that other question was marked as a duplicate).  So I discuss using 8-bit values (even though this question asked about 32-bit values), but that's okay because 8-bit values are simpler to understand conceptually, and the same concepts apply to larger values like 32-bit arithmetic.
However the program is able to allocate multiple blocks of memory for things like storing numbers and text, that is up to the program and not related to the size of each address. So a program can tell the CPU, I'm going to use 10 address blocks of storage and then store a very large number, or a 10 letter string or whatever. 
Side note: Memory addresses are pointed to by "pointers", so the 32- and 64-bit value means the size of the pointer used to access the memory.
For instance, you might have multiple ADD instructions. One ADD instruction might store 16 bits of results in the A register and the D register, while another instruction might just store the 8 low bits in the A register, ignore the D register, and specify the overflow bit. Then, later (after storing the results of the A register into main RAM), you could use another ADD instruction that stores just the 8 high bits in a register (possibly the A register.) Whether you would need to use an overflow flag may depend on just what multiplication instruction you use.
A register is just a piece of memory. Registers are built into the CPUs, so the CPU can access registers without needing to interact with the memory on a RAM stick.
Whenever we must comprehend a larger number, we use a system. The leftmost number is the most important. It is 10 times more important than the next.
Because you are not displaying a number (as far as the computer is concerned), but a string, or a sequence of digits. Sure, some apps (like the calculator, I guess), which deal with numbers, can handle such a number, I guess. I don't know what tricks they use... I'm sure some of the other, more elaborate answers cover that.
Finally, a computer could display any number as a simple sequence of characters. That's what computers are best at. The algorithm for converting between a sequence of characters, and an internal representation is quite complex.
You may have learned a method in school. An algorithm. The algorithm is quite simple. Start by adding the two leftmost symbols.
A computer able to differentiate between four billion different values, will similarly have to make the leftmost value, in a set of values, be four billion times as important as the next value in that set. Actually a computer does not care at all. It does not assign "importance" to numbers. Programmers must make special code to take care of that.
The computer can generate a result that is more than 8 bits. The CPU may generate results like this: