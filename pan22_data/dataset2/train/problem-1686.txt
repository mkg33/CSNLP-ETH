We have multiple deployments of FC-based mini-clouds and iSCSI-based mini-clouds, and they both work pretty well. We're finding that the bottleneck is at the storage array level, not iSCSI traffic over 1Gb Ethernet.
We had large FC-served VMWare farms, there was pressure to reduce storage cost so we started building new extensions using NetApp 10Gbps iSCSI, we ran into performance problems so moved them all over to FC and it was the best thing we could have done - we're doubling the number of VMs per host and getting the same performance as we were seeing under iSCSI.
Your VMware rep should have plenty of documentation on Fiber vs. iscsi including some overall benchmarks, or at least real world implementation stories/comparisons.  Our rep sure did.
I know the issue has been resolved, but I suggest you have a look at this article about FC vs iSCSI.
If your vm guests are going to be running storage-intensive operations, then you have to consider that the speed of transferring a read/write operation from the VM Host to the storage array may become your bottleneck.
Since your typical iSCSI transfer rate is 1Gbps (over Ethernet), and FC is usually around 2-4Gbps (depending on how much cash you're willing to spend), then you could state that the transfer speed of FC is roughly twice as fast.
However, that doesn't really mean that the machines will work faster, as if they are running applications with low I/O, they might actually perform at the same speeds.
I could go into the detail (in fact I happily will if prompted) but ultimately iSCSI is almost a 'something for nothing' product and we all know about free lunches :)
One tweak you might consider is setting up Jumbo Frames. Scott Lowe has a recent blog post here that shows some of what he did to achieve this.
Of course our, very-mixed, VM load profiles may have exacerbated this but if you can afford FC then I'd strongly urge you that way.
It's a very noticeable change.  Though to be truthful, we were going from Linux Server Based iSCSI (fake iscsi) to fiber, aka a testing environment to production when my last company was rolling out VMware based shared hosting.  Our VMware rep stated that fiber is much less overhead when it comes to multiple VM's on a single ESX host needing access to shared storage.  I noticed the general usage of a Win2k3 VM instance doubled in performance, but disk IO, which I tested using hdtune on the VM was faster then our Dell 2850's standard IO (3 x 73GB in RAID 5 on a perc 4 if memory serves me).  Granted we were running maybe 5 or so VM's on each ESX host with low usage, as we were being trained up on it. 
You mention that the guests will be running low CPU load - those are always great candidates for virtualization - but the difference between FiberChannel and iSCSI doesn't really come into play yet.
While this was not  asked I am just curiois if RAID 5 is a good choice for virtual environment, have you considered RAID 10? 
The debate as to which is better is never ending, and it basically will depend on your own evaluation and results.