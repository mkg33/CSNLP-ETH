However, you may also want to look at the open_file_cache parameter inside nginx. See http://wiki.nginx.org/NginxHttpCoreModule#open_file_cache
For Squid, the default is 16 level one directories and 256 level two. These are reasonable defaults for my file systems.
P.S. I sort of got carried away in the theory but kquinn is right, you really do need to run some experiments to see what works best for you, and it's very possible that the difference will be insignficant.
You could put a squid cache in front on your nginx server.  Squid can either keep the popular images in memory, or use it's own file layout for fast look ups.  
Splitting them into directories sounds like a good idea.  Basically (as you may know) the reason for this approach is that having too many files in one directory makes the directory index huge and causes the OS to take a long time to search through it; conversely, having too many levels of (in)direction (sorry, bad pun) means doing a lot of disk lookups for every file.
Also think about how people are going to be downloading the images. Is any individual client going to be requesting only a few images, or the whole set? Because in the latter case, it makes sense to create a TAR or ZIP archive file (or possibly several archive files) with the images in them, since transferring a few large files is more efficient than a lot of smaller ones.
I would suggest splitting the files into one or two levels of directories - run some trials to see what works best. If there are several images among the 70,000 that are significantly more popular than the others, try putting all those into one directory so that the OS can use a cached directory index for them. Or in fact, you could even put the popular images into the root directory, like this:
...hopefully you see the pattern. On Linux, you could use hard links for the popular images (but not symlinks, that decreases efficiency AFAIK).
You will really want to look into caching.  Whether it be the built in web server caching or a third party cache like varnish.
As others have mentioned, you need to test to see what layout works best for you for your setup and usage pattern.
If you don't use a product like Squid, and create your own file structure, then you'll need to come up with a reasonable hashing algorithm for your files.  If the file names are randomly generated this is easy, and you can use the file name itself to divide up into buckets.  If all your files look like IMG_xxxx, then you'll either need to use the least significant digits, or hash the file name and divide up based on that hash number.  
The organization of the files has more to do with file system performance and stability than delivery performance. I'd avoid ext2/ext3 and go with xfs or reiser.