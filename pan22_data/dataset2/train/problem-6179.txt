I'm not aware of any training routine that involves freezing and unfreezing different parts of the model at different times during training.  People may have done this, but I'm not sure what the benefits would be.
It is a good practice to freeze the early layers when you finetune the model. There are two reasons why you want to do this :
I have recently read about Fine Tuning, and what I want to know is, when we are fine-tuning our model is it necessary to Freeze the model and train only the top part of the model and then unfreeze some layers and again train the model or one can directly begin by unfreezing some layers? Till now, I have read that one does not unfreeze the layers directly because then we risk losing the important features captured by the earlier layers 
It can work either way.  If you want to keep the exact feature extractors, then you should freeze everything except the "top" of the model.  You can also unfreeze the whole model; the "top" of the model will be trained from scratch, and the feature extractors near the "bottom" of the model will be tweaked to work better with your dataset.  The potential drawback of unfreezing the whole model is a higher potential for overfitting (and a longer, more expensive training time)
Unfreezing the earlier layer is up to you on the later stage of training (Some people do some people don't) but if you feel you might need to do this to push for better performance, then just try it.