I heard of software that basically tracks files' checksums and detects when they change, but I am wondering if there is a more low-level (e.g. filesystem-level) solution? For example, a filesystem that calculates file checksums every time files/blocks are written/updated and supports checksum verification to detect corrupted ones so that action can be taken before the error gets spread onto backup media?
Data backups/mirroring solve the problem of complete disk failure, but not the problem of silent data corruption. Moreover, if the source/master disk gets some files corrupted, the correct versions on the backup disks will be overwritten on the next backup essentially spreading the error and making the originals unrecoverable.
In 2017, what techniques/solutions are there to address the problem on premises (as opposed to uploading data archives into the cloud with 99.999...% durability)?
We know that hard drives have limited lifespan and data corruption happens. A hard drive may still appear to be OK and pass tests but some files may at some point start read differently to what they originally were just due to the medium physics/deterioration.
Finally, maybe there is some sort of completely different approach to the problem that I am not aware about?