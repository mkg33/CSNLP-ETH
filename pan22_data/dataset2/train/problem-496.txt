With only a little bit if data it can easily overfit. The big difference between training and test performance shows that your network is overfitting badly. This is likely also because your network model has too much capacity (variables, nodes) compared to the amount of training data. A smaller network (fewer nodes) may overfit less.
Whatever regularization technique you're using, if you keep training long enough, you will eventually overfit the training data, you need to keep track of the validation loss each epoch. at least use early stopping to stop the training process when the validation loss stops decreasing.
For increasng your accuracy the simplest thing to do in tensorflow is using Dropout technique. Try to use tf.nn.dropout. between your hidden layers. Do not use it for your first and last layers. For applying that, you can take a look at How to apply Drop Out in Tensorflow to improve the accuracy of neural network.
For this specific case, we see that the optimal epochs is occurring at 12, So we need to again train with 12 epochs and test on test data.
Since your 'x' variable are sentences, you can try to use Sequential model with one Embedding Layer and one LSTM layer: