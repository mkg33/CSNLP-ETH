I am currently dealing with a similar task. It is to build ETL process and design dimensional model. I have researched a lot for the best way to deal with it and found an amazing helpful source of techniques we should definitely apply when working with MPP.
Since Redshift is a columnar database, storage and query performance will be different than RDBMS models.  Optimizing for a columnar database is also different.  Because there is usually less disk I/O and less data loaded from disk then queries are faster. 
There is, however, this recent blog post from the AWS Big Data blog about how to optimize Redshift for a star schema: https://blogs.aws.amazon.com/bigdata/post/Tx1WZP38ERPGK5K/Optimizing-for-Star-Schemas-and-Interleaved-Sorting-on-Amazon-Redshift
I'm leaning toward importing my files from S3 into staging tables and then using SQL to do the transformations such as lookups and generating surrogate keys before inserting into the destination tables.
As far as ETL/ELT, there is AWS Glue as other posters have mentioned. And yes, there are a number of tools, some of which are free.  Amazon does have a DB Best Practices Guide, that might help you, too. One tip I've seen in other forums is to load your data as raw as possible and do the transformations in Redshift.  That would lead you to an ELT process.  With so many options, perhaps looking at a comparison of the 2 methods would help.  Here's a blog article from Panopoly explaining the differences, it might help you decide on a path. 
I have been researching Amazon's Redshift database as a possible future replacement for our data warehouse. My experience has always been in using dimensional modeling and Ralph Kimball's methods, so it was a little weird to see that Redshift doesn't support features such as the serial data type for auto incrementing columns.
In terms of the AWS blog post you reference, I take it you have looked at those recommendations and considered which options work best for your data for distribution, keys, cursors, workload management, etc. and have at least a good idea of the approach you would use.  I find it easier to work with a visual representation, you might consider a quick and dirty DB diagram showing how your existing tables would migrate to Redshift.  Covering the major ones to get a feel for how much data is going where. And I would certainly use the ODBC/JDBC drivers from Amazon, loading large amounts of data can be troublesome in any case, much less moving to a different DB type.
Note that Redshift sometimes works BETTER if you have a wide table with repeated values rather than a fact and  dimensions. The reason for this is that the columnar approach lets Redshift compress the different values down to a level that is pretty efficient. I do not have a formula for when to use many Dimensions vs a flat wide table, the only way is to try it and see!
be sure to take a look into this resource. I bet you will find it incredibly helpful. It is a ~35 page document with powerful techniques to leverage the use of MPP columnar stores. It supports the comments you see like
The question I have is about what is the best practice for loading a star schema in Redshift? I cannot find this answered in any of Redshift's documentation.
We needed to enforce uniqueness constraints so we chose to write to Postgres and then replicate new data to redshift every 10 minutes.