The best algorithm is to start blocking separate addresses. Then when multiple addresses are blocked in the same /64 you block the whole /64. Repeat that for bigger aggregates.
Depending on which region you are in you also might use the regional internet registry's (RIR) public database. In the RIPE database internet providers document the aggregation size they give to customers there. In that case you know exactly what size to use.
I use the example of blocking, but really it's a more general question, which extends to rate limiting, analytics etc.
Conversely, if I block /56, and it turns out the ISP allocates blocks of /64, I might also have excluded 255 other users. Now the probability of you actually affecting your traffic by randomly excluding 255 sites is pretty low if you consider a random distribution. However, it is not, by definition random. It is feasible that I could in this sense block a small geographical area where my website is popular.
That said, I have read in online discussions that /64 and /56 are both common allocations for domestic users.
A "site" in this context is something like a home or small office user. RFC 6177 discusses the allocation of address blocks to small sites:
Given that blocking a single IPv6 address would seem to be somewhat pointless as an end user typically has at least 2**64 of them, we need to block a range, which identifies a "site".
Prefixes are usually given out on nibble boundaries (multiples of 4, or one hexadecimal digit). Do you might want to scale from /64 to /60, /56, /52 and /48. A /48 is usually the largest prefix given to a single site.