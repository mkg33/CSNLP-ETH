From my experience, 250ms latency starts to feel like a noticeably slow connection.  10ms or better feels like a blazing connection.  It really depends on what you're doing.
Try using "mtr" to show information about the latency and packet loss to the different remote ends.  Unless you fully understand "slow path" routing, ignore anything but the last hop listed on that output.  Van Jacobson says that humans notice latency starting at 400ms, but realize that many connections require multiple back-and-forth exchanges, so a 100ms latency can quickly add up to a second...
Just to give you some experience about one case where this sort of issue was seen and the numbers related to it.
In our case, we're on our own Network (one big intranet), and are able to let our routers make decisions based on OSPF throughout the state :)  Unfortunately, anything off our network relies primarily on our ISPs layout.
I think geography will have a lot to do with packet transmitting time since the further you go, the the more hops you will most-likely add affecting overall latency.  If your customers are going to be based mostly on the west-coast, then I'd go for the west-coast hosting... Same thing on the east-coast.  If your customers will be coming from all over the US, or the world... then you'll just have to make the hard decision as to which side gets the less latency.  
We had a client that we spent a fair bit of time going around and around with related to this.  They originally were hosted in New York, and their staff is mostly located in the Boston area.  They were moving their servers to our facility located in Denver, about two-thirds of the way across the country.
I've ignored extra latency caused by additional routers/repeaters, which could be much, much higher. I've assumed the distance 4400 km and speed of light in fiber 200000 km/s.
We went back and forth a few times.  After around 6 months, we switched to a different primary upstream ISP, for reasons unrelated to this client (better pricing, more bandwidth, unhappy with the number of maintenance windows on the other provider), and with the new provider we were getting around 45ms average latency for this client.  At this point their performance concerns seem to have gone away.
Once they moved, they started bringing up performance problems from their Comcast links in home offices.  They used to have <10ms latency, and it went up to 80-ish ms.  They noticed slower performance reaching their sites, but said "maybe we are just going to have to put up with going from blazingly fast to mere mortal speeds."  They seemed to realize that there were limitations because of the geography, and that their users on the west coast would be potentially getting better performance.
All other things equal, you will have additional 44 milliseconds of latency just because of the speed of light. Give or take 1/20 of a second for each packet roundtrip. Not much for typical web usage. Passable for ssh sessions. Substantial if you access your DB directly with a lot of small consecutive transactions.