Now that I've read @smac89's answer, I see there's a next_permutation() function built-in to C++. Oh well.
As you've noted, your program becomes unusable when the input words get large. The challenge states that the words can be up to 100 letters long. There are 100! (~10^200) permutations in a 100 letter string. No computer on Earth has that kind of storage capacity. What you need to do is directly construct the next permutation.
You can get rid of every new call in this code and it will run with less memory and will be easier to read.
This is the strangest mix of C and C++ code styles I've seen. Why are you using both std::string and char*? Why both std::vector and dynamically allocated arrays (new[])? None of your news are deleted, so you're leaking memory everywhere, meaning you couldn't handle large words even without the space and time issues. If you consistently use std::vector and std::string, the memory will be freed automatically when they go out of scope and are no longer needed.
Instead of string *words = new string[test_cases];, use vector<string> words. Then, you can do words.push_back(w); instead of keeping track of an index.
I'm assuming that permutations(string word) returns a vector<string>* pointer so it doesn't have to be copied when passing to find_lexicographically_bigger(). You can also pass by constant reference: const&: find_lexicographically_bigger(const string& words, const vector<string>& perms). The argument is const because it will not be modified, and the reference & means no copy will be made.