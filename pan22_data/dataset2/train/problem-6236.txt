Anyway, the problem with rsync is that it seems to have no decent error handling support. Obviously, it wasn't meant for use in recovering data from failing hard drives, but all the so-called "data recovery" utilities out there that are meant for such use usually focus on recovery of deleted files or messed up partitions, rather than copying files off dying hard drives. Deleted file recovery is not what I need, obviously, so perhaps you can understand my disappointment in not being able to find what I'm after yet.
Great question, love the level of detail.  For a utility that would work well in this scenario I would recommend trying Filescavenger from Quetek.  It will allow for excessive re-reads in the 100+ count but more importantly it has a disk imaging feature that will allow you to dump the entire contents to an image without wasting time trying to assess the files then load the image and scan through it for files.  The utility runs $50 for a home users but you can download it and test it to the point of seeing what can be recovered from the image before spending money.
You want Progressive Copier.  It's targeted at doing whole drives, but since it takes a file path as a parameter, it should work for individual files as well.  I don't think it'll work for directories -- you'd have to specify each file.
Naturally, this is where you'd probably say "You should use ddrescue!" Well, that's all fine and dandy, but I've already got most of the data backed up, so I just want to recover certain files. I'm not concerned with trying to recover a full partition block-by-block as ddrescue does. I am only interested in rescuing just specific files and directories. 
I have a 3TB Hitachi that reads painfully slow, ddrescue will operate at the file level. But not on directories. However I could not get ddrescue's --timeout or --min-read-rate options to work.
So am I daydreaming here, or does something out there exist that can do this? Or, maybe even a way to make rsync or ddrescue work in such a way? I'm really open to whatever solutions might work, so long as they let me choose which files I want to "rescue", and can skip files with errors in the initial run, and try/retry those errors again later. 
It would quickly move on if it encountered errors, and when rerun would go back to try those files again.  
Ideally, what I'd like is some sort of cross between rsync and ddrescue: something that lets me specify source and destination as directories of normal files like rsync (rather than two full partitions as ddrescue requires), with a way to skip files with errors in an initial run, and then allows me to attempt recovery of those files with errors in a later run (with a slightly altered command, of course), perhaps even offering an option to specify the number of retry attempts ...just like how ddrescue works with blocks, only I want a utility that works with specific files/directories like rsync does.
If it's professional use it'll cost you a pretty penny but gains significant features including support for logical and hardware raid arrays.
Speed seems to vary wildly, and it gets stuck on files a lot, and when it finally gets "unstuck" it only seems to last for a short burst before it gets stuck again. The drive is also very quiet with only an occasional sound of files copying (usually when it gets stuck/unstuck for a brief time, before getting stuck again). Thus, there are none of those evil sounds that are normally associated with HDD death. Someone suggested that the problems sounded like they might be caused by a misaligned disk head, which requires a lot of re-reads before it finally reads data with success. Sounds plausible, but I digress...
The 2011 answers seem to be promoting tools that are just commercial equivalents to ddrescue - no indication that they are file-aware.  Fishy.  The 2013 answer is interesting.
I have a hard drive with either bad blocks or sectors that are failing to read due to potential mechanical issues, such as a bad disk head, bad motor, or some other issue that is causing the hard drive to read data excruciatingly slowly and with lots of read errors. I'm seeing an average of 50 KB/sec, with some reads dropping below 10 KB/sec, and frequently it gets stuck on a file or sector altogether, usually for quite a long timeâ€”from 2-10 minutes or more (when using rsync, before it times out). 
So far I've tried rsync with the following options, but it often gets stuck on a file for longer than the timeout, and ideally I'd just like it to move on to the next file and come back later to the files it gets stuck on. I don't think that's possible though. Anyway, here's what I've been using up till now:
If this fails as well, you may be past the point of software recovery tools.  If you are and you can justify the cost of in lab recovery let me know what country, state and city you're in.  I can recommend one of our or or competitors labs near you.