One widespread technique is to replace missing values with a marginal central estimator for each variable. This might work for a classifier like decision tree, if there are no missing patterns in your data. However using this imputation would alter your confidence intervals if you study linear relations between variables using regression. This is simply because it will alter sample variance. 
Another much sophisticated imputation method is to use EM algorithm to fit the maximum likelihood estimator of the variance-covariance matrix. This estimator is unbiased and using this variance covariance matrix you can recover the linear model in an unbiased way. You then can go to analyze linear relations. But this works only for linear and log linear models and requires a lot of data and also requires to have missing data at random, which is not always the case.
I think there is no answer to your question since there is no absolute universal "good". Everything depends on the question you ask and the tools you use. This is why there are a lot of imputation techniques. There is no replacement for a missing value. However, in the constrains given by your question and used tools, you can think of imputation which does not alter your answer or at least measure some effects of the missing values. I will give some simple examples. 
Another one is multiple imputation. What you actually do is to draw data at random for missing values according with a supposed distribution. You do that multiple times, let's say at least 30 times. You analyze each data set with traditional tools and later you aggregate those results into a single set of results. This works well mostly when data is missing at random, but is tedious, sometimes inconsistent and duo to randomness it can produce different results. Ultimately, sometimes is very hard to find proper ways to aggregate the results, it depends on the analysis you use.
If you just want to do descriptive analysis it might be a good idea to do this without imputation at all.