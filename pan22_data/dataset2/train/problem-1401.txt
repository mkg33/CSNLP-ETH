As an alternative, if these web services can co-exist on the same machine you might also consider making 2 VMs with the same 
Capacity requirements are best determined through load testing and empirical data.  Don't rely on predictions or inspired guesswork.
However, I would recommend having a few physical machines due to redundancy. If you have one physical server with a million VMs in it, when that one physical server dies (and it will) it will take a million VMs down with it.
Next you want to right size the virtuals. Never give a virtual more than 1 core, always scale out. If you add additional cores, cpu polling causes and artifical load and locks. For a good apache virtual we run 1 core, 1.8Gb mem and ~4gb of network. Under load the virtual sticks at about 2ghz, and the server reports a load of 1.28.Our database virtuals run 1 core, 6gb of mem and ~4gb of network.
In your particular case 1 system might be OK if your service will be dead should any one of these systems be down.  You haven't added any risk in that case. It also sounds like those systems will not need 6Ghz of processing power, in which case consolidation will certainly save money since you wouldn't buy 6ghz just for those 3 machines.  The other thing you need to watch for is (and certainly not least in consideration) are I/O requirements and potential I/O contention.
Always separate applications; you can not properly tune a virtual if you keep adding variables. A single virtual should have a single role. It should perform one thing very well. 
Availability is another matter.  Sure, multiple servers are better than one.  There may be other factors that influence the architecture.  Primarily cost, especially if this is not open source and there are per-server license fees or you are using third-party components that require royalty payments.
Blade servers do not make good virtual servers. You are severely limited on bandwidth and memory, the two biggest demands in a virtual environment. We tend to run around 4 vituals per core. Even with the best blade chassis you are limited to 40Gb of network connectivity per blade. If you are running 32 high-performance web virtuals, even with over subscription you will still be maxing out your connections per virtual. If you do get 40Gb of connectivity to a blade you will need to sacrifice a storage networking and go with a converged fabric, which takes away from the needed web bandwidth. A dell 910 or an HP 585 make good virtualization platform. 1 server with 24 cores, 140GB of network running 96 virtuals. 
No It's not the same.  depending on the virtualization platform there will be overhead (anywhere from 5-30%) from the hypervisor layer. 
I think that the sane way would be to identify where the crucial bottlenecks are or may occur.  VMs are great for isolation and depending on the kind of hypervisor used, it may have little impact on the actual performance. Virtual networking would probably work better than physical networking as well.
Given their requirements sound a bit 'wooly' and are actually quite low I'd be strongly tempted to virtualise this. I'd start with just two blades and some shared storage, then you can create, modify and delete their VMs as required, you'll lose very performance and gain a huge degree of flexibility, plus you can scale-out linearly and with no user impact.
DMZ. The DMZ should be on separate physical virtual platforms. For regulatory requirements and protection. 