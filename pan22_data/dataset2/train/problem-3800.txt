(I have no relation to the tool or its creator other than it's the only thing I found to actually work)
This being said, and if you don't need to read the file constantly, use some of the formats recommended in the above link. 7z and rar are quite efficient (i.e. they compress with high ratios at a decent time). If you care about space and not about time, then choose a PAQ-type algorithm (although you will spend a very long time compressing and decompressing the files). There are also speedy algorithms available.
Paragon allows you to either boot it as an OS in itself (ISO image), or install into another Windows OS with access to the target filesystem. Then you can defragment both the MFT and the files. This is to my knowledge the only way to fix this deficiency in NTFS, short of reformatting the volume.
A good analogy is of a cake being split into several boxes, some of which aren't empty. This is the initial file. The compression part squeezes the pieces of cake, leaving a space in the boxes. As the pieces of cake aren't together, because of the created space, the pieces that make up the cake become fragmented.
While not exactly what OP asked, I have had good experience with a 3rd party software named Paragon. NTFS by definition trashes your filesystem something horribly when you compress (or sometimes even write) files. This extends to consuming multiple MFT entries, and... It's bad. Microsoft's NTFS driver doesn't even clean this up when a file gets defragmented. Hence, 3rd party tools are required.
First things first. WBAdmin is in essence a backup utility that cam restore a full system. So, it's expected that it's output file is large (> 4 Gb). As shown by the quote, large files become rapidly fragmented. This is due to the way NTFS compresses: not by files, but by sectors.
I am still skeptical about NTFS giving out that kind of compression ratio. According to a test made by MaximumCompression on multiple files, NTFS gets the lowest score in compression ratio, a measly 40%. From personal experience I can tell you it's much lower than that, in fact so low that I never bothered to used it nor have I seen it's effects.
The best way to avoid fragmentation is to stop relying on NTFS. Most defraggers will fail to expand or move the compressed files. If somehow they did, NTFS could not be able to expand the files, or if he could, as the defragmentation process would fill the leftover space from the compression (the 4kB), the expansion would fragment the files, as the file wouldn't be written in the before-contiguous clusters.
Today, 2 years after the question was seemingly asked, I'd rather recommend deduplication - this can give you upwards of 90% disk savings if the images are just "a little" different. A W2016 Nano Server inside a VM works really well, but I suspect even FreeNAS or anything else using ZFS could handle it.