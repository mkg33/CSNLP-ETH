The command runs recursive copying that doesn't overwrite existing files. But the copying process is rather slow. I notice that it does not copy files in order. So my question is: can I speed the copying process by opening multiple terminals and run the same command above? Is a copying process smart enough to not overwriting the files copied by other processes?
No, the copying process is not smart to not overwrite the files copied by other processes. Executing multiple commands to copy the same files/folders is not a good idea.
Sometimes, you can't do much when the source and target machines are too far and network is slow. Here is a post to discuss why SSHFS is slow.
I mount a remote server over ssh (using sshfs). I want to copy a large number of files from the remote server to local:
The crucial bit here is that SSH is not merely about "logging in remotely", which many people assume it is, — it's rather about running any command remotely while connection its standard I/O streams to the local SSH client instance.
This approach has the advantage of consuming "full pipe" with a single flow of data (you can also stick | pv in between to see how it goes if you'd like some interactivity) compared to SSHFS (and SFTP) which does many round-trips between the server and the client.
Note that if this happens on a secured LAN or other controlled environment, it's best to ditch SSH and use a pair of nc or socat instances — the listening one on the server and the sending one on a client. This approach does not spend CPU cycles on encrypting the data so you'll likely to be bounded by I/O on either of the three components: the source FS, the network and the destination FS.