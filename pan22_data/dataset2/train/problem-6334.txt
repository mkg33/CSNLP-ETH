If you have a spare USB 3.0 port, I would suggest using a flash drive for your swap space. There are plenty of high-speed flash drives that are as fast as your SSD, but much cheaper - cheap enough to replace if it did begin to fail. A quick search on Amazon shows that there are many decent 16 GB USB 3.0 flash drives for under $20, and even some 64 GB drives under $60.
SSD may be fast, but it is not nearly as fast as organizing your computation to do the same set of operations without ever stalling to swap.
In this case please consider using a swap file instead of a partition. You don't need to worry about sizing much, getting rid of or adding it later doesn't require any repartitioning. There is no (noticable) performance penalty using a file over a partition. If you ever happen to need it, have a look at the size and this will then also give you good hints.
Consider what difference a swap of, say, 16GiB will make (or think of 64GiB). If you never use these 16GiB, you could as well not have them set aside in the first place. But if you do use them, what happens? Disk, compared to main memory, is exceedingly slow. Even with a SATA-600 SSD, transferring 16GiB takes between 30 and 40 seconds, and 2-4 times as long on some other configurations.
There are more considerations. If you need/want suspend to work then you need at least the size of your RAM and then some. However it sounds unlikely you need it given that you seem to mainly build a computational work horse. 
With that said, advice such as you need a swap X times the amount of RAM installed comes from people who repeat something they heard (and didn't understand!) from someone who repeated something they heard (and didn't understand!) decades ago.
Typically operating system installers ask you to make an initial swap area (as this is the simplest and allow even tiny computers to install), and it has been found that a good rule of thumb for typical Unix operations is to have virtual memory sized to be three times the physical memory, so this is typically suggested.  You, however, know more about the usage pattern so you can change this as appropriate.
The "use 2X your RAM" rule was an easy to remember rule of thumb in the 1980s and 1990s, it was never the "golden truth" (just something that worked OK for most users), and it doesn't apply at all nowadays.
Partition the entire flash drive as swap space, and you'll have swap space if you need it, and peace of mind knowing that the memory being repeatedly written is easily (and cheaply) replaceable.
You should have a reasonable amount of swap which you can easily afford (say, a gigabyte), so the OS can page out some stale stuff, and so the world doesn't immediately end when you once ask for a little more memory. But that's it.
As a random example that is immediately relevant to me this summer...  In implementing the quadratic sieve, one needs a large (apparently) contiguous array to mark up (with some complicated algorithm whose details actually don't matter for this example).  The array needs to be ~100 Giga-entries, so easily in the 1 TB range.  I could pretend to allocate that and let the OS do an amazing amount of inefficient swapping to get pages in and out of RAM to support all the sequential writes through the array.  Instead of doing something that boneheaded, I have arranged to allocate a much smaller array that exactly fits in memory and then reuse that little array to iteratively cover the rest of the big array in slices.  I've also stripped the OS, stripped the running set of services, replaced the shell, and customized two layers of memory allocators to do their darnedest to keep as much of the address space available to my process as close to contiguous as possible.
There is nothing wrong with working without swapspace if your memory pressure always is less.  Linux will transparently use any unused memory as a disk cache. 
Unless you do compute tasks that require datasets in the hundreds of gigabytes and (this one is important!) data is accessed in a more or less access-once fashion, you will never want to have a swap much larger than that. But then again, simply memory mapping a datafile works equally well for that application.
Now someone will inevitably object that you are rather paging in and out a dozen or so 4kiB pages, not 16GiB in one go. While that is true, the point nevertheless stands. If you only need to swap in and out a couple of pages, you don't need 16GiB of swap, but if you do need 16GiB of swap, then you are going to transfer them, too (one way or another).
First, it is sub-optimal because the operating system has fewer choices in what it can discard when it runs out of physical memory. There are two things it can do: Swap out something that isn't used, or throw away pages from the buffer cache. If you have no swap, there is only one thing it can do. Throwing away pages from the buffer cache is harmless, but it may noticeably impact performance.
As the others mentioned, a swap partition is a good idea even if you have plenty of RAM. It's not a good idea to put it on an SSD; the frequent writes of a swap partition will eventually wear out your drive.
The workload you want to apply to the machine needs a certain amount of memory to run (remember to add enough to the equation to handle peak loads), and you need to configure your computer to have at least that.   
A much better idea than having "a lot of swap" is (re-)organizing your work so that the working sets fit in memory, then using the file-system to store and retrieve the work you do.  I.e., instead of forcing the OS to guess what your memory usage patterns will be, use what you know about your problems to control your memory usage patterns.
In theory, 99.9% of all users could even use a 64GiB machine (or any 8+GiB machine) without any swap, and most likely never notice something missing. However, this is not adviseable.
You will be fine even with 1GiB (and likely less) of swap. My work computer typically uses no more than 140-150 MiB. A gigabyte is plenty of over-provisioning for that.
Modern operating systems provide virtual memory as a combination of physical memory and swap space, so if you need more memory than the machine has available you must add enough swap space to fill the gap.   I.e. if you need 80 GB max, and the machine has 64 GB you need 16 GB swap.
Second, private anonymous mappings might simply fail if there is no swap. That usually won't happen, but eventually when there is not enough physical memory available to satisfy them all, and there is no swap, the operating system has only either this choice, except...
Third, the dreaded OOM killer may kick in. Which means a more or less random process gets to get killed. No thank you. This is not something you want to have happening.