But if you are running only one worker process then you should set multi_accept on; in events block so the worker process accept multiple connections at a time.
I thought about a routing problem, but using a second server to call the script indirectly does not work either, while the two independent tasks work. It's driving me nuts.
I have reposted this as a yes-no-question since my first question one was closed for being too broad.
Using curl like suggested everywhere didn't solve it either. It all boils down to Nginx blocking itself.
I have thought about using Apache behind the Nginx for PHP, but if Nginx is the blocking part, I would have the same problem.
It's also not a coding problem, it works perfectly fine in every other scenario, except when Nginx is calling itself again. have tested it with two files, one does echo "X"; the second one echo file_get_contents($url1);.
Also, the second script is only calling the first script on the same machine sometimes. Often it is using other servers, so I would have to include complicated business logic to distinguish and act different in both cases, which I want to avoid.
Somewhere I read that Nginx is doing all requests one after another, so one long running script could block all.
If yes and you can supply a solution, that would be great, but for now it would be relevant if that is a possible cause of the problem.