the dd command will write to cache and no disk, this you can get crazy numbers like 1.6G/s because you are writing to RAM and not disk
On the NFS server(s), at least for Linux no_wdelay supposedly helps if you have a disk controller with BBWC. Also, if you use the noatime flag on the clients, it probably makes sense to mount the filesystems on the servers with noatime as well.
And the host server is running CentOS with VMware Server installed, which is in turn running the 7 VMs? Is there a particular reason you've gone with CentOS and VMware Server combined, rather than VMware ESXi which is a higher performance solution?
Also, at least for Linux server, you need to make sure you have enough NFS server threads running. The default 8 is just way too low.
And, as was already mentioned, don't bother with UDP. With higher speed networks (1GbE+) there is a small, but non-zero, chance of a sequence number wraparound causing data corruption. Also, if there is a possibility of packet loss, TCP will perform better than UDP.
To get above that you're going to need to look at teaming the network cards into pairs, which should increase your throughput by about 90%. You might need a switch that supports 802.3ad to get the best performance with link aggregation.
Switching to UDP transport is of course faster then TCP, because it saves the overhead of transmission control. But it's only applicable on reliable networks and where NFSv4 isn't in use.
NFS performance on ZFS is greatly improved by using an SSD for the ZFS intent log (ZIL) as this reduces the latency of operations. This thread about VMWare NFS on ZFS performance on the OpenSolaris NFS and ZFS mailing lists has further information, including a benchmark tool to see if ZIL performance is the bottleneck.
Newer Linux kernel version support even larger rsize/wsize parameters, but 32k is the maximum for the 2.6.18 kernel in EL5.
One thing I'd suggest though is your IO throughput on the OpenSolaris box sounds suspiciously high,  12 disks aren't likely to support 1.6GB/sec of throughput, and that may be heavily cached by Solaris + ZFS.
If you're not worrying about data integrity that much, the "async" export option can be a major performance improvement (the problem with async is that you might lose data if the server crashes).
The 50MB/sec isn't great, but it's not much below what you'd expect over a single Gb network cable - once you've put in the NFS tweaks people have mentioned above you're going to be looking at maybe 70-80MB/sec. Options along the line of: