DDR is a newer version of SDR. It has a better architecture which allows reading double the amount of words per clock cycle.
The DDR interface accomplishes this by reading and writing data on both the rising and falling edges of the clock signal.
When DDR was standardized, the fastest frequency it expected to support for the memory clock was 200MHz, so considering these limitations in hardware support, DDR was clearly a superior methodology to SDR. That we never went back to SDR techniques as memory clock speed started to rise is not particularly suprising. 
Your assertion is not wrong per se; DDR and SDR both tie their functional performance to frequency and the frequencies they both run at are directly proportionate (one is exactly twice the other) but it would be silly to attempt to revive SDR now.
Recently I've discovered that I don't see the actual difference between DDR RAM and SDR RAM with doubled frequency. How do these actually differ?
Can anyone tell me if there is a difference? Or the only reason of DDR is increasing the data rate without increasing frequency?
Related to that, at same CPU power, DDR1 @ 400MHz will work just as fast as DDR2 @ 800 MHz which will work just as fast as DDR3 @ 1600Mhz which will work just as fast as DDR4 @ 3200 MHz.
I think the real answer to this question is a matter of history, rather than ongoing technical decision making. In particular, I don't believe that any form of SDR is still an option for modern PCs (though other more specialized hardware may implement differant specs).
It seems to me that memory bandwidth is the same, energy consumption is the only thing that probably differs here, but I don't actually see why, because every transistor should work the same way.