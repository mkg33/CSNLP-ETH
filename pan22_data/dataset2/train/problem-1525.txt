If you don't want a DIY tiered storage option (if I had to I'd probably use the File System Management task in windows 2008 r2) I'd highly reccomend you take a look at a solution from Compellent.  You would not need any additional nodes (per se) for lower cost storage as you would simple have some fast disks and some inexpensive slow disks mounted from the san via the OS of your choice.  Compellent's OOB featureset is access based HSM.  Thsi solution also provides scalability.  Right now this approach might be expensive, (and you provided no future outlook) but long term it might be more cost effective than trying to manage and maintain a roll your own solution.
I worked on a social networking site that had a "lock box" for files.  As the site grew we were burning through about 200GB a day in storage.  
We do this exact thing with our VoD servers, where we use many unclustered servers with lots of memory to act as cache for the local disks which are in turn multiple SAS-connected 25 x 2.5" 15krpm disks, this is then streamed over either multiple 1Gb NICs or dual 10Gb ones. We spent a LONG time getting the PCIe slot/SAS-HBA positions correct as well as RAID cluster and disk block size etc. settings.
We kept track of busy files using web stats which ran every night.  If a file was listed in the top files list, then the script would update the database and set the file to "high priority". This told the web app to use the high priority URL and copy make sure the file was on the fast storage system.
Haven't really heard enough detail, but knowing what I know I'd look into a basic 1U server (or two for HA) with a lot of RAM running your choice of OS/storage-software, connected to a Xiotech Emprise 5000.  Assuming you can fit a good working set in memory the IOPS that make it through to the spindles will be pretty broad random i/o, and thats what the box is best at.  You could probably do a one-server(64GB)/one-array(2.4TB) combo for a touch under 20K.