Like this, you don't have to wait for anything, nor do you have dangerously hanging ssh sessions which, if disconnected accidentally, would kill your jobs. 
and this will seamlessly put you back into the shell you abandoned, showing you all standard output which you missed in the meantime, if any. 
The files you want to process are listed and piped into parallel which is instructed to run 8 parallel jobs and work in the current directory.
Use screen on each machine. screen starts a command (generally a shell) whch can be detached from your ssh session, so that it continues executing the job just started inside it; in other words, the job, though still running, will not be interrupted by any interrupt (not just kill -9) as you log out.
I have a series of server machines which I want to run the same command on.  Each command takes hours and (even though I am running the commands using nohup and setting them to run in the background) I have to wait for each to finish before the next starts.  Here is roughly how I have set it up:
Secondly, you may want to look into GNU Parallel for parallel jobs. You may not want to have more processes running than you have CPU cores, because of diminishing returns and disk overload. A suitable command for you might be as follows, which, again, you would have to run inside a screen in order to make it survive a disconnect.
Firstly, you may want to look at using screen instead of nohup for making the session survive a disconnection. screen gives you a virtual terminal you can return to later. The basic usage is screen yourcommand to execute yourcommand and screen -DR to automagicalliy connect to an existing session, or create a new one if none exists. Just running screen without an argument also gives you a prompt inside a "screen" that you can use.
Since screen doesn't understand pipes and other things in the commands given as an argument - that's the shell's job - you would need to either put the command in shell script or give a sh -c command to execute the command.
what that does is build a list of machine names and pipes the whole list to xargs.  The arguments to xargs mean:
Does anyone know of a way such that I dont have to wait for each job to finish before the next starts?  Or alternatively a better way of doing this, I have a feeling what I am do is fairly sub-optimal.