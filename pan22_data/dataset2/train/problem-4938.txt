Using compute on an already rendered buffer for screenspace effects is a good idea, since you save the fillrate of rasterizing a full screen quad every time you apply a new effect compared to the fragment shader approach. These days though, OpenGL and D3D have compute shaders built-in to handle this kind of use case. Why do you want to use OpenCL exactly? Seems to me you'd be better off using one of the rendering APIs.
OpenCL isn't really a drawing API, it's designed for doing compute workloads across different devices, not necessarily GPUs. You can certainly write a kernel that renders to a texture in OpenCL, and have this texture be available to Open GL or D3D. AFAIK, though, you wouldn't have access to the fixed-function hardware like rasterization, attribute interpolation, and fragment blending, so you'd have to implement it yourself (unless you're writing a ray tracer). You'd also likely have to use one of the drawing APIs to present your texture to the screen, by rasterizing a full screen quad and sampling your texture at each pixel. This carries a synchronization penalty - although OpenGL and D3D can interoperate with OpenCL, they can't access the same memory at the same time, so you need to wait for control to transfer from one API to the other.