The disk subsystem is complicated enough to be affected when a problem occurs, because of you'll hardly get anything in your log files.
You should also check root-mail for any interesting messages that might be related to your system crash.
First, you want to check /var/log/syslog. If you are not sure what to look for, you can start by looking for the words error, panic and warning.
Of course if your node has a built-in management system similar to Oracle's ALOM/ILOM, you can also check for possible problems and log files there.
I know that I look at /var/log but not sure which logs should I investigate and what should I look for. So Appreciate your hints. 
There are 2 ways of checking what triggered shutdown, first check the Out-Of-Band Management console for any issue in the hardware, i would suggest to configure SNMP and receive emails or add the traps in a monitoring software for any alert.
Try logging over the serial console. This needs some cabling, and an other system to pick up the lines, but you have better chance actually catching the problem. 
Then through the Operating System, you can either check /var/log/messages(RedHat based distros) or /var/log/syslog(Debian Based distros).
In a new Xeon 55XX server with 4xSSD at raid 10 with Debian 6, I have experienced 2 random shut downs within two weeks after the server being built. Looking at bandwidth logs before shut down does not indicate anything unusual. The server load is usually very low (about 1) and it is collocated far away.There seem to be no power outage while the server went down. 
If you have system graphs available (e.g. Munin). Check them and look for abnormal patterns. If you do not have munin installed, it might be an idea to install it (apt-get install munin munin-node)
Other logfiles you should check is application error-logs. E.g /var/log/apache2/error.log or similiar. They might contain information leading you to the problem.