If you'd like to accomplish it as an exercise - this would be good approach. But if you just need some working code - use an existing JSON streaming parser.
To give you any advice, first of all we need to know, what is your goal? What is the mission of this code?
In addition to being quite effective, this approach would give you clean, readable and testable code which is aligned with JSON semantics. Though state-machines can be quite verbose.
As an output from this parser you then can produce a stream of more high-level values than just JSON tokens. Say, if you need to process large array, you can push to that stream every element of this array. I think, you could pass JSON-Path expression as an input to flush to stream every time when matching JSON structure ends. 
In one of the comments you mention something about cost. If it is valid to assume that you need to process many such JSONs concurrently, I'd optimise for throughput and memory and use a streaming parser. This type of parser is effectively a state-machine, which takes symbol by symbol and makes a transition to the next state according to its previous state and processed symbol. This is basically the same method which is used in processing regular expressions.
Just trim it and run the one condition - if it is empty or white space it will quickly fail that test.
I would not do that. Sure you can replace strings if not valid but you're going down a rabbit hole, better force valid json or you will keep getting more and more errors if you're not fixing them upstream. Also what's the json standard, there are a few that are valid 