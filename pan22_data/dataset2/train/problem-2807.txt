All of the above provide the processing multiple applied to incoming storage requests. Doing 6.2 MB/s of pure write I/O will put a certain amount of load, but the range of loading experienced by a system can go from negligible to crushing depending on how all of the above factors shake out. 
The processing power required to handle storage depends on many factors. These days storage processing is a distributed process. But the various places where processing is encountered:
The relation would be not as much a pure space:cpu issue. Depending on the solution you are aiming at, you might need CPU power for features like deduplication, compression, encryption or hash / checksum calculations. 
For instance, Windows Server 2008R2 on actual server hardware with a couple of disks on a hardware RAID controller can take that 6.2 MB/s all day and night and barely break a sweat (baring bad drivers), even on relatively old 64-bit Pentium 4 CPUs. A FreeNAS based on a Core2 processor doing the same write rate over AFP may not be able to keep up.
is there a relationship between storage space and number of cpu's required to handle it? if i have 100GB of data a night coming in through a fat pipe remotely from 50 different sites so that the total data footprint over a 18hr period is 100GB, and i wanted to have a 20TB NAS or storage system receiving it, does the NAS server need to have, for example, 1 xeon or should it be a dual cpu with multi core etc
You also should concentrate on the I/O backend you are using. While writing 1,58 MB/second does not sound terribly challenging, using 50-100 simultaneous processes to do so will incur a lot of random write load. Hard disks do not cope well with random (write) loads as these will incur a lot of time-intensive head seeks, so you will need to have something that cushions randomness - like a DRAM or SSD write cache.
the purpose of the NAS is redundancy for data in the field. Assume that there is no bandwidth issue and we will be using fat pipes to move the data, my concern is the simultaneous piping of seperate geographic instances of 2GB of data coming in from 50 - 100 seperate sites. i don't want to have read/write issues or missing/crashing of a sync.
A simple data copy process would not incur much CPU overhead unless you have a really badly designed system. Your requirement of 100 GB per 18 hours would average to 1,58 MB/second - even a netbook would be able to cope with that nowdays.
i need a direction to start testing with this idea, so if i have to move that much data onto a NAS server nightly.