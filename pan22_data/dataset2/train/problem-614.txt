DRBD is, fundamentally, network RAID mirroring. Blocks in -> blocks out. You can run synchronously or asynchronously, depending on your latency requirements. Which of these you choose tremendously affects whether your replica is crash-consistent or not.
Reduced to that level, your question becomes: "what happens when MySQL starts up and it reads data files?" Either your data is well-formed and quiesced, and it starts without a hitch, or it's crash-consistent, and you might have consistency issues. (There's also the possibility that you have on-disk corruption, of course, and this can also be a problem with DRBD, especially if you somehow end up with a split-brain scenario.) Usually, it can recover itself by replaying logs if you're using a transactional engine, but sometimes you will have more serious issues. This is as true with DRBD as with other shared block storage, like a shared SAN volume or (heaven forbid) database files on NFS.
In your case, you may want to consider a standby slave in case you run into a consistency issue on the master's disk. It takes manual work to promote it, of course, but at least you won't be restoring hours- or days-old data.
Christian mentioned Galera. Check out Percona Cluster. It uses Galera, and is a very promising addition to bring MySQL up a notch in reliability. 
There is no way to ensure that any high-availability system will always continue working on a failover. The best you can do is make the right decisions when architecting your HA solution, and test the crap out of them to validate your assumptions about how it's going to behave when things go wrong.
It depends, naturally, on the nature of the failover. It also sounds like you already know the answer to your question.
Drive arrays run about $15-20K in this range. If you take into account that you need 2 of everything (not to mention that you need equivalent hardware per node), the costs of an array are well justified. Another benefit of an Drive Array is speed. In my case, I use the Multi-path drivers so the systems can use both controllers at the same time. The throughput compared to an internal raid is usually much higher. 
Then you can reasonably sure your database is highly available. In your example, if it fails in the middle of the transaction it will be aborted and your application should hopefully retry and should hopefully be able to connect to your second node which should hopefully have a consistent copy of the data (because all writes are synchronously written to both nodes before returning to the database that it has been written).
HAProxy. So on each web brick you run HAProxy which then connects and checks that the MySQL servers are alive. 
I removed DRBD from the picture by moving all the data & logs to a separate drive array connected via dual SAS controllers. We use an IBM DS-3525 for this. What is good about this setup is the secondary system is always connected, just does not have the partition mounted. I used Corosync to control the fail-over. When the primary comes back, Corosync shuts MySQL down, un-mounts the partitions, remounts them on the master, starts MySQL back up. Even if the master machine died in the middle of a transaction, InnoDB would recover.
Hypothetically, an ACID-compliant database should always recover gracefully from incomplete transactions. In practice, and especially with some MySQL versions, this isn't always the case (largely because MySQL doesn't have the greatest legacy of ACID compliance, though things have improved in recent years). Keeping frequent backups is always a sensible thing to do.
If you have control over the application code you can use MySQL Galera synchronous replication instead of DRBD. Galera has the requirement of odd number of cluster node members preferably at least three so majority vote wins who was correct data. You can augment MySQL Galera with 