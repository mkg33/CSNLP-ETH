This requires either to handle the insertion of the metric for each table at the time the metric is observed or more likely to insert into the minute table only and then aggregate the data into the other tables on a schedule. MapReduce tools can help with the aggregation of data.
The alternative solution if you feel that maybe this is geared towards a key/value persistence model is to utilize something like redis or dynamodb (in AWS) where you create a set of tables (min, hour, day, etc) for each monitored host and use the timestamp for the key (the value being the load). You can also simply prepend the host_id to the key if you don't want to create sets of tables for each unique host. Btw there are downsides of each. The first thing I can think of is that unless you use a hashing technique you're going to be pounding the same server for a specific host. In AWS land for example your key retrieval method is a hash (of your key) which spreads the load across your table instances (actually the disks the data is stored on): http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DataModel.html.
The problem in doing this in a relational persistence model (mysql) is that this data is very key/value oriented (http://en.wikipedia.org/wiki/NoSQL#Key-value_stores). You can of course do what you're talking about in mysql and I'm assuming you're going to have a lookup table or a host_id column to relate the metrics to the servers you're monitoring. One huge table with metrics (load) and then a table for the hosts you're monitoring with a foreign key (host_id) to constrain and join the two.
The data should be partitioned into separate tables the time frames that you mentioned, minute, day, month, year, etc. The goal is to reduce processing on either server (mysql or web) or client (in js for example). You want to grab the data quickly and display quickly, right?
There are plenty of other key/value storage systems as well and mongodb is something you might research as well: http://en.wikipedia.org/wiki/MongoDB