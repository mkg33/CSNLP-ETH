Word Moverâ€™s Distance (WMD) is an algorithm for finding the distance between sentences. WMD is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors.
I'm looking to solve the following problem: I have a set of sentences as my dataset, and I want to be able to type a new sentence, and find the sentence that the new one is the most similar to in the dataset. An example would look like:
Once you get word embedding of each word, you can apply any of the similarity metrics like cosine similarity, etc. on each sentence to measure similarity with others. 
the novel idea in this work, they interpolate between two sentences. and the results were quite amazing.
For 1. word2vec is the best choice but if you don't want to use word2vec, you can make some approximations to it. One ways is to make a co-occurrence matrix of words from your trained sentences followed by applying TSVD on it. Coccurance matrix of $nXn$ dimensionality when converted into $nXd$ dimensionality, makes for word vectors of $d$ dimensions. 
I've read that cosine similarity can be used to solve these kinds of issues paired with tf-idf (and RNNs should not bring significant improvements to the basic methods), or also word2vec is used for similar problems. Are those actually viable for use in this specific case, too? Are there any other techniques/algorithms to solve this (preferably with Python and SKLearn, but I'm open to learn about TensorFlow, too)?
There is some recent work based on Variational Auto-Encoder in RNN models.Generating Sentences from a Continuous Space, with pytorch implementations: github code.
they managed to compress the semantic, syntactic global feature of a sentence into some latent space expressed maybe with some finite 10 to 30 independent random variables (factorized distribution).
For your problem, you would compare the inputted sentence to all other sentences and return the sentence that has lowest WMD.