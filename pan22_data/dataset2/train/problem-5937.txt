XGBoost or ADABoost can improve for sure your accuracy or F1, especially when you have unbalanced class as they tend to give more weight to those observations that during the training go misclassified.
XgBoost often does better than Logistic Regression. I would use CatBoost when I have a lot of categorical features or if I do not have the time for tuning hyperparameters.
I need to improve the prediction result of an algorithm that is already programmed based on logistic regression ( for binary classification).
I tried to use XGBoost and CatBoost (with default parameters). but it takes a long time to train the model (LR takes about 1min and boost takes about 20 min).
You should invest time in a boosting model for sure (they will always take more time than Logistic Regression) because it is worth it. If you are impatient, try CatBoost instead of XgBoost to see the improvement in accuracy. That will definitely give you the motivation to spend more time tuning your boosted trees.
Another idea is to perform PCA (Principal component analysis) to extract non-correlated features explaining a certain percentage of the target variable variance. You could then run any classification algorithm.
You could try reducing number of features by assessing their importance. This can be achieved by training XGBoost on the data and then analyzing how each feature split improved gini score. More on this and the code you can find: here . You could get the sense of how many features are enough for the prediction for a small information loss. 