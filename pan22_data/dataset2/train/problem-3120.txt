Basic rules: Have enough electricity, a decent connection that will support your peak traffic, and enough cooling capacity to keep the servers from reaching thermal shutdown. After that, anything you can put together is just gravy. 
Back on the other hand, I've seen some gifted uses of limited resources. One that comes to mind is cooling. It's difficult to get sufficient cooling in a normal space on campus at a decent price without doing some serious improvements to the building's infrastructure. Since some of our buildings are historic, that's not likely to happen. One excellent use of limited cooling power was to vent all hot air to the outside using enclosed racks with APC-brand fan-back enclosures. The entire server "room" (four racks) was sufficiently cooled by a pair of portable units... but (most importantly) all heated air was immediately removed from the building and vented to outside. 
The only advice I'll add is that make sure you have a good meter of space in front of your rack, enough space to get access to the back of every rack and if possible access to at least one side. You can share the space at the front or the back, so you could have two racks facing each other with a meter gap in between. It depends if space is at a premium or not.
Why not consult with the experts? Positioning of racks can not only depend on cooling but fire suppressant and other things. Things which in the end determine if your insurance is valid or not. An interesting discussion but talk to the experts before applying our comments.
A bunch of fallacious stuff gets thrown around every time this comes up, like "you must have 24-inch raised floors", or "you must be on a 2nd or higher story of a building" ... anything that says 'must' is probably advice you can safely ignore. 
The 'ding ding' here is: Consult with your local experts, do what you can afford to do well, and know where your limits are before you hit them. 
Honest to god, I've seen some dumb stuff in my time. There's a department at my employer whose server admins (we are in a federated/distributed it environment at a large campus where many groups run their own small machine rooms) are known for letting their orange lights (dell kit) blink for months before addressing hardware issues. The main campus machine room once got hot enough that the flooring tiles warped -- without the ops center even knowing until a vendor-managed piece of equipment managed to call the vendor for help, and the vendor rang the ops center to ask what the hell was happening. (I think it peaked at about 125 degrees F.) There are so many single points of failure that I know of that I would probably win any sysadmin ePeen match with a new high score. Yes, it's terrible ... but, like most things related to the internet, it works despite breaking every rule known to man. 
I could tell you what we do here, but there's way too many little details to go in to. What I can do instead is recommend a great book: Build The Best Data Center Facility for Your Business from Cisco Press. Despite the publisher there's really nothing Cisco specific in it. It offers all sorts of great advice as to how to lay out server rooms big and small (although with an emphasis on big) and a lot of the issues involved. It doesn't go in to great depth on every single topic, but still has enough detail to be useful.