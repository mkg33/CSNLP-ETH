In our shop at my previous job, we had a secondary set of servers where we tested our restores. For our busiest customers we would restore tonight's backup, mark it as read_only, and their reporting tomorrow would connect to that copy of the database for all reports from yesterday back. This offloaded about 90% of the reporting workload and doubled as a backup/restore validation method. So if most reports don't need today's data, you could consider alleviating some of the production workload this way with some cheaper hardware - if you're not using Enterprise features you could even use Express for all the databases that are < 10GB. (Well, I see it's 2005, which had a lower DB size limitation, but you could always restore your copy forward into 2008/R2.) This would allow you to really distribute the databases to many low-end commodity servers (VMs or pizza boxes).
On 2005 I can think of three strategies to offload reporting: 1. Day-old data using log shipping (log shipping to standby kicks users out at each restore, so realistically restores can only happen when they don't expect to be on, like in the evening.) Advantage - incremental updates. 2. Day-old data using full restores. Disadvantage: full restores. 3. Replication. This gives the best user experience, but is harder/takes more manpower to set up and maintain.
As mentioned in Data Warehousing and Reporting, Transactional Replication is well suited for reporting scenarios by offering:
Aside - is there any advantage to just increasing the hardware available to the big customers, tuning and isolating them, instead? Enough disks and memory? Flash? Index optimization? Really they don't sound that big by today's standards (admittedly, I have not seen your systems)