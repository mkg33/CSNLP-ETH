Start off by warning them you're unhappy with performance, and they might simply move you to a lower utilized raid which might solve all your problems. Also ask them if they have some raid-10 storage available, and maybe ask for a volume there instead of the raid 5. If the problem comes back, then consider getting your own storage or finding some other host that can provide you better performance. 
From looking around, it seems like fio is the most configurable to measure the above. The config I've got so far is:
What I want to do is determine whether any particular Linux VM is getting that performance, and then run the same benchmark with other providers so I can compare.
SQL and data warehouses are more like 8:1 reads to writes, all small block, all random. In any case, anything but random reads is easy to cache, and is not likely causing you disk performance issues. Without knowing how they do their disks, it's hard to really help much, but consider asking them what they mean when they specify "RAID5-SAN environment". 
If I run this on an otherwise-idle 4x virtual-core machine with 8GB of RAM and 470GB of allocated storage, I'd expect to get 235 IOPS via the above (500 * 0.47). The results I get are:
Summing up read and write IOPS for each job (why doesn't fio include this in its summary?) I get 647, which seems like it's exceeding their specified service levels. Anything obvious I'm missing, or are their metrics skewed massively for some workloads (specifically I'm interested in PostgreSQL with data-warehouse workloads).
So a provider has given us 500 IOPS/TB as their SLA standards for disk performance in a VMWare & RAID5-SAN environment. This is apparently measured with:
Since they specify an SLA as IOPS per TB, I'd hazard a guess that each volume they provide to you is supposed to be on a separate RAID-5, allowing for more IOPS as they add volumes. Bad performance could easily be caused by bad raid neighbours: volumes on the same raid as you that take more than their fair share of storage resources. The problem with this is that sometimes your SLA will be exceeded, but sometimes you'll have to deal with high latency.