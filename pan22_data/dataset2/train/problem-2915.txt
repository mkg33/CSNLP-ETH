As you are using R you might want to look into the stringdist package and the Jaro-Winkler distance metric that can be used in the calculations. This was developed at the U.S. Census Bureau for linking .
There are lots of clever ways to extend the Levenshtein distance to give a fuller picture. A brief intro to a pretty useful module (for python) called 'Fuzzy Wuzzy' is here by the team at SeatGeek.
Another popular technique for detecting partial string matches (though typically at the document-level) is shingling. In essence it is a moving-window approach that extracts out a set of n-grams for the target word/doc and compares them to the sets of n-grams for other words/docs via the Jaccard coefficient. Manning and colleagues (2008) discuss near duplicates and shingling in the context of informational retrieval.
I've written a generic probabalistic fuzzy matcher in Python which will do a reasonable job of matching any type of data:
I've also written a similar project specific to UK addresses, but this assumes you have access to Addressbase Premium.  This one isn't in memory, so has been used against the 100m or so UK addresses. See here:
It's in memory, so you probably don't want to use it to match datasets which are above about 100k rows.
If you want to get this going quickly I'd reccommend using libpostal to normalise your addresses and then feed them into my generic fuzzymatcher (pip install fuzzymatcher).
A couple things you can do is partial string similarity (if you have different length strings, say m & n with m < n), then you only match for m characters. You can also separate the string into tokens (individual words) and look at how sets of tokens match or arrange them alphabetically and order them.