I understand boosting is a sequential learning technique and it use the prediction from previous model as a dataset for new model ,after adding weight to the misclassified data points. 
The point which was not clear how the weights are added for misclassified ones and diminished for the correctly classified ones. It would be great if veterans can help me to understand this
For Adaboost, the training subsets are selected by sampling from the whole training set with probabilities. 
At the beginning of running the algorithm, the probabilities are uniform. After a classifier is trained and tested, the probabilities of each successfully classified instances are decreased. Then, all the probabilities are normalized after adjustment. The next classifier is trained using dataset sampled with the adjusted probabilities. 
In the original boosting algorithm (Schapire 1990), three classifiers are used (say $C_1$, $C_2$ and $C_3$). The training dataset is randomly divided into three subsets, $D_1$, $D_2$ and $D_3$. The training process is as follow: 