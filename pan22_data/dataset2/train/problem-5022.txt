The problem is framed as predicting the likelihood of an example belonging to class one, e.g. the class that you assign the integer value 1, whereas the other class is assigned the value 0.
For a binary classifier, it is prominent to use sigmoid as the activation function. The sigmoid function's range is $[ 0 , 1 ]$. That makes sense since we need a probability which could determine two ( binary ) classes i.e 0 and 1. 
Another point to note is softmax is a generalization of sigmoid for producing probabilities for multi-class problems so that the probabilities strictly sum to 0,hence rather than using tanh go for sigmoid or either softmax(it is same as sigmoid for binary classification problems).
If you are using tanh ( hyperbolic tangent ) it will produce an output which ranges from -1 to 1. In this case, we cannot determine the binary classes. 
I was wondering, if the dataset's range is changed from [0,1] to [-1,1], shouldn't I be able to use tanh as the activation of the output layer? If so, what are the advantages of one activation over the other?