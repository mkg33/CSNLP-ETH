Thanks for all the advice. I have currently setup logstash on client to send their access logs to a central rabbitmq server and using another instance of logstash to parse the data into elasticsearch. with the RESTApi of elasticsearch i was able to do a few interesting dashboards (like the current location of users accessing the web server).
There's also tools like PastMon which can sniff and report on lots of low level network stats. Or mrtg.
Good ol' AWStats is a real-time log analyzer that has dashboards and widgets and wingdings and portals and panes o' glass and other such things. You can even customize it with plugins to your liking.
An interesting question, by the way -- I'm all ears for better solutions! +1 to your question because of that. :)
The performance side doesn't offer as much choice - but statsd + graphite provides a stonking backend for storing and presenting data from multiple sources (logs, javascript bugs). I'm currently planning an installation using this at the back end and Yahoo Boomerang to collect page load times. Have a look at Graphene for an example of what it can do. Writing, say an awk script, to parse the logs and feed the backend would be trivial.
There's really 2 branches of web analytics - marketing information and performance information (and user interface design which kinda spans both).
You can also feed your Apache logs to MySQL with syslog-ng and then use front-ends such as  logzilla (previously known as php-syslog-ng) for querying the data.
Visitors has a real-time mode and can show you basic information such as most visited pages, the hottest hours/days and even visual path analysis.
Google Analytics, Open Web Analytics, Piwik and to a lesser extent AWstats, Analog et al are primarily about gathering marketing information (what your customer base is, where they are, what browsers they use, what conversion ratio...).