The disk is connected with the CPU via SCSI, SAS or IDE bus. Which is a internal network running a specific protocol - SCSI or ATAPI. Ethernet is designed to work on longer distances and can be much slower than SAS/SCSI/IDE. So which one is faster, dependes on which technologies are you comparing. If you compare a 20 years old laptop HDD with a 10Gbps in RAM storage, the winner will always be the networking. And when you buy a storage you have to compare it versus price and manageability.
My experience tallies with that.  In fact, I have never deployed a data warehouse onto a consolidation environment where I could not run the same ETL process significantly quicker on my desktop PC.  I've also had sales reps from a major vendor of SAN equipment say off the record that a lot of their customers use direct attach storage for the DW system because SANs aren't fast enough.  
I work on data warehouse systems, and the canonical DW query is a table scan.  If your query hits more than a few percent of the rows in the fact table (or partition) then a table or partition scan using sequential I/O will be more efficient than a random access query plan using index lookups and seeks.
Well, there's Light Peak which is aiming for 100GBps networking speed, which is getting close to RAM speeds. Of course, the network can only deliver data as fast as the sender can generate the data, i.e. if the sender is reading the data from a hard disk then the receiver will only get the data at the same speed as the disk read, even with a superfast network.
However if you were randomly accessing small units of data and the network wasn't flawless or had many hops and more than just you accessing it, I'd bet that a local cache is faster even of its on a whirly mechanical disk drive almost 100% of the time.  But you bring up an interesting point and how long will will need local storage of anything if network speeds continue to grow?
Personally, I think there are several factors to consider.  For instance, how fast is the memory or disk you are accessing locally vs. the one you would be accessing via network?  If the remote data was on very fast SSD and faster than gigabit networking installed end to end, the remote might be faster for large streaming files.
Networked storage is at least an order of magnitude more expensive per IOPS than direct attach storage for random access workloads and closer to two orders of magnitude more expensive for streaming. 
It depends.  If your I/O is primarily random access then its flat throughput is probably not that great compared to the network bandwidth that could be available.  However, most network traffic is ultimately generated by processes that involve I/O.  If the working set of whatever process is generating the network traffic fits into cache then it won't be constrained by disk bandwidth.  If it thrashes the cache then disk will become a bottleneck.
Networked storage (i.e. SANs) tends not to perform well on streaming workloads unless it is tuned appropriately.  If the SAN is being used for a general purpose consolidation environment it will almost certainly be tuned quite sub-optimally for a streaming, spiky load like a data warehouse.  I have seen a vendor white paper suggest that you need about 3x the number of disks to get the same throughput on a SAN that is not tuned for streaming I/O as for one that is.