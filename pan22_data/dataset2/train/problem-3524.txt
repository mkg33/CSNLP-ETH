My question is: if I train a CNN from those 3000 greyscale images, would I get the same parameters from the first CNN? More precisely, how does CNN behave when we feed it with RGB images? Does it extract the combined features from the RGB images or does it merely learn the features in each R,G,B channel separately?
I have a basic question regarding convolutional neural network. Assume I have a set of 1000 RGB images and I train a CNN from this set. I can obviously split each of my RGB images into 3 different greyscale images, each representing red, green, and blue array - thus creating 3000 greyscale images.
This is only for the first layer, though, because afterwards layers have an arbitrary number of channels that has nothing to do with the colors in the original image.
It behaves like a fully connected network regarding the channels dimension. For each pixel location, the CNN takes the weighted sum of all pixels in that location on the channels dimension. The weights are of course trainable.
thanks for your answer. So in that case, if I feed 1000 RGB images vs 3000 greyscale images that are just split from those 1000 RGB images, I would get a different network parameters, correct? And also most porbably different network dimension, right?
I am asking this question because I am handling a slightly bit more complex data structure, instead of RGB images I will be handling a multispectral images that consists of 12 channels. RGB image typically only has 3 channels (or 4 if Alpha is activated and JPEG obviously can't handle the alpha channel).