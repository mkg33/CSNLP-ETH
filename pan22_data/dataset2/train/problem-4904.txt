As for the data itself, I agree with the replies that suggest not replicating the swap files. Swap files can take a lot of disk and are always changing, which would trigger a lot of replication traffic. I don't know whether VMWare supports replication of an environment without them, though... I assume that the VMs in a a VM datastore replicated without swap files would be crash-consistent, however I can't confirm that myself. 
3Mb/s is probably not going to cut the mustard unless you use WAN acceleration (such as Riverbed, mentioned by one of the other answerers). WAN acceleration works by keeping a cache on disk at both sides of the link where they store all the most recent data you've sent, and if you ever send a duplicate block, it sends a reference instead of the data. 
My first experiment with this was to create a schedule of 15 minutes between snapshots with a snapshot reserve of 500GB. I let this run overnight and until COB the following day. I don't recall the number of snapshots that could be held in 500GB but I ended up with an average of ~15GB per snapshot.
The solution for us to meet our RTO/RPO SLA was to install a Riverbed WAN Optimization Appliance between the 2 sites.
I then changed the snapshot interval to 60 minutes which after a full 24 hours passing means a total of 13 snapshots in 500GB. This leaves me with ~37GB per hour (or ~9GB per 15 mins).
The block size is 16 Mo for snapshot and replication on Equallogic SANs. This is why you got those astronomical numbers. No way to change that.
I was told by a Dell rep that a way (perhaps not the best?) to estimate deltas in a volume is to measure the snapshot reserve space used over a period of time of a regular snapshot schedule.
That said, assuming your storage is using the same engine to take snapshots as it uses to replicate snapshots, then the most accurate measure of change is indeed a snapshot reserve. You'd need to keep one snapshot and its reserve isolated for the duration of the the measurement period, though. Assuming EqualLogic uses copy on write snapshots, comparing data from the reserve of several snapshots taken throughout the day might actually make it seem like your data's changing more than it actually is.
I have a long-term goal of setting up a DR site in a colo somewhere and part of that plan includes replicating some volumes of my EqualLogic SAN. I'm having a bit of a difficult time doing this because I don't know if my method is sound.
I am currently in the process of something similar however with Solaris 11 and zfs as our san backend. Because of bandwidth I decided to separate out most of the components. We migrated to exchange 2010 so that we can set up our dr site with an identical copy. What I found was doing san level snapshots would be ridiculous for this data because of bandwidth issues like you are seeing. We decided it would be cheaper and more efficient to set up a dag and replicate within exchange itself. We also did the same thing with our mysql servers. What we clone now are systems with less deltas between snapshots. I was able to do the initial synchronization at the office and transport it to its final destination. 
During that time it'll be queueing up your net-change for when the initial sync finishes. And if you ever need to pull data from the remote array, it'll be 4.33 days until you're fully up and running (unless you have an out-of-band method of data-transfer, like a FedEx Overnight Shipping or a truck).
Your numbers boil down to 10.24 MB/s, which does seem a bit on the high side for pure write. But then, I don't know your workloads. 
This is where sync mode really comes into its own. A 3MB/s pipe is woefully underprovisioned for synchronous replication. A batched asynchronous replication will gain some of the benefits of write-combining, and therefore lower total transfer, at the cost of losing some data in a disaster. Unfortunately, I'm not well versed enough in Equilogic to know what it's capable of
For the sake of focusing on the most relevant information, I'd suggest defining an objective for your recovery point and recovery time. These are unimaginatively referred to as an "RPO" and "RTO". Disk replication is supposed to reduce them both by keeping a crash-consistent copy of the data that's never older than a few minutes on another site.Once you have these numbers, you can define things like how often you have to have a crash consistent replica.
These numbers are astronomical to me. With my bandwidth I can do a little over 1GB/hour with 100% utilization. Is block-level replication this expensive or am I doing something completely wrong?
However, you have a bigger problem. The initial replication will be replicating 1TB of data over a 3MB/s straw. 
As for the difference in net-change between your 15 minute snapshots and the 60 minute snapshots, I believe the 60 minute snapshot is getting the benefit of a lot of write-combining. Or put another way, all of those writes to the filesystem journals are being coalesced in the longer snapshot in the way they aren't as much in the 15 minute snaps. 