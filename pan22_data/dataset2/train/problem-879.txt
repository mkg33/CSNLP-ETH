For example, if you look at the The Computer Language Benchmarks Game, you can see that a gap often occurs between a compiled and an interpreted language, but it is not always true: in the regex-dna test, javascript performs better ... but only because it has a powerful and "native" support for regular-expressions. It is also clear that the gap between Java and C++ has greatly reduced thanks to its JIT compilation. 
Or is there some deeper ontological explanation for this? Is there some qualitative property of a low-level procedural language (or, at times, of a functional language) that makes it possible to express and compute a problem efficiently? Or is it simply a matter of brute resource and CPU cycle efficiency?
This is not because of a lack of investment of the language authors, but rather that, like with any level of abstraction, you choose a balance between the conciseness of your words and their preciseness. A word like "humans" is very abstract and describe a whole specie, but it doesn't account for the particularities and all the culture of each individual in this group. Just like scientific jargon, high-level language parsers are designed to concisely and efficiently describe operations of the targetted paradigm, but they cannot describe as concisely every other paradigms.
One aspect that has not been covered in the other answers is the fact that "higher-level" languages typically provide additional safety nets, which may involve some run-time overhead.
I've been wondering lately whether there's a 'general' explanation for the inefficiency of high-level languages in performing certain kinds of algorithms - from simple cases like the massive performance improvement in C over, say, Python, when looping over a set, up to complex mathematical algorithms.
I think that the gap between the performance of two programming languages highly depends on the type of the problem/algorithm and how you measure such "performance". 
Of course some of these checks can be optimised away, and some of them can be moved outside the inner loops. Modern CPUs also help tremendously, as these are branches that can be predicted well. Nevertheless, a number of these checks are there and they take a non-trivial amount of time.
... Using the programming language described in this paper we prove a series of hierarchy theorems ... The main theorem, for deterministic time, states that for time-constructible functions $T(n)$, there is a constant $b$ such that more decision problems can be solved in time $bT(n)$ than in time $T(n)$ ... 
Just an update on the issue: I think that the problem of efficiency of the constant factor is mainly qualitative, rather than quantitative. Indeed, a high constant factor is mainly caused by an inefficient interpretation of the programmer's intent by the language parser.
See for example: Computational Complexity via Programming Languages: Constant Factors Do Matter (A. M. Ben-Amram and Neil D. Jones)
... When the facts are about the performance of programs, the particular way each program does a task matters a lot. Obviously when programs implement different algorithms that difference may itself be enough to explain any difference in program performance. Less obviously, even when the same program is measured with different implementations of the same programming language, the particular way that program does the task may work better with one of the language implementations than the other - but slight changes to the program might reverse that performance difference. So there has to be some flexibility allowed in the way programs implement the same algorithm, and the tasks are kept simple enough for you to check the program source code. When the facts are about a wide range of different programming languages even more flexibility has to be allowed in the way programs implement the same algorithm - after all, the point of using a different language is for the different approach that language provides. ...
Is it purely a quantitative matter - C is more efficient because it's not doing weird magic behind the scenes and the programmer directly interacts with main memory? And because it's not doing things like interrupts for garbage collection and other general housekeeping like a high-level language might?
However, now it seems that a new class of languages are emerging: annotated languages, such as Julia or Cython or Numba. They are neither really interpreted nor compiled but in-between: you can write high-level code that the "interpreter" will have to guess how to run the most efficiently possible (and correctly of course), or you can almost compile your code by annotation or by using other specialized constructs such as dispatching or semi-automatic unrolling/vectorization of loops, so that your annotations avoid the need to make the language parser to guess what you were meaning to do at lower levels of abstraction: in short, annotations give you the ability to give precisions on your high-level abstraction. We will see in the future if this concept works out.
As a concrete example, I have occasionally compared the performance of C code and equivalent Java code (truly equivalent â€“ forget about classes and objects, your high-level data structure is int[]), and I have also had a look at the machine code that is generated by modern compilers in each case. While the quality of compilers varies a bit, and just-in-time things have their usual advantages and disadvantages, it seemed that a major difference was related to the safety nets. In Java, array references such as x[i] involve two sanity checks: pointer x is not null, and array index i is valid.
The theory says that different programming languages lead to execution times that differ only by a constant factor ... and the speedup theorem for Turing Machines says that constant factors are meaningless; but some efforts have been done to study computability and complexity from a programming-language point of view and the results are a little bit different.
I think now we are at a point where we can approach the question from a TCS perspective, especially from the perspective of the theory of programming languages: Would it be possible to, e.g., re-design Java so that none of these run-time checks are necessary, without losing any of the safety nets, and without putting any significant additional burden on the programmer?
In this sense, we can say that it's more a problem of semantic linguistics rather than just technical implementation: with a low level language such as C, the language parser don't have to make any guess, since you have direct control of the most basic instructions, and thus are responsible for the whole program flow. With a high level language, you get a more abstract control of the program flow, which makes it easier and faster to design complex program, but at the expense that you delegate some of the program flow design to the language parser: it now has to make some guesses about what you meant, and it can be very costly if it's some kind of program flow that the language parser wasn't primarily made for (such as linear algebra in Python).
First of all, I apologise off the bat for my mangling of CompSci language. I have no real training or background in computer science or mathematics.