If your database follows the normal pattern where data mostly gets added, less frequently updated and rarely deleted, then you can get a rough idea by determining the daily growth rate of your database.  Look at your backup history and determine how much growth there is per day.  That will be pretty close to your transaction log backup size.  
See here how to replay a profiler trace or distributed replay for info on how to run an XEvents replay.
You didn't specify which SQL version you are on but you could use a Server Side trace or an Extended events session to capture the traffic that is sent to the SQL server (if you can live with the performance impact it might have) and replay those captured traces on a test environment to see what the daily growth would be for the day where you ran your traces.
We have a moderately sized database (about 500GB) that we're going to switch from Simple to Full Recovery, with Trans Log backups every 15 minutes. Before we make the switch, I want to get an estimate of how much space those backups will take on average. (Actually, our storage folks want to know so they can carve out that space for me.) So how can we get that estimate?
That would allow you to rerun the captured workload on a test system and see for real what is happening with your log file.
Index maintenance will artificially inflate your transaction log backups so there will be spikes when maintenance is done, but that's how I would do it.  
Looking at how much log space is in use won't do it, since it's just using one area in the log while freeing others. So I'm guessing I would have to look instead at the rate at which the transaction log is being used. The plan is to monitor the VLF usage and track which VLFs are used and then freed over time, along with their size.
That's going to be a little challenging to set up, but it's doable. Before I go down that route, though, I'm hoping there's a better/easier way. Any ideas?