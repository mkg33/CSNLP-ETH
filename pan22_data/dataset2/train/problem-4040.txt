One problem you'll have is with SSMS. I have a client with a lot of objects in a single database, and if we expand the Tables node in Object Explorer, SSMS dies. 
As well as this, I suspect your code might need to be changed. Either you need to rely on the default schema for every connection, or else you need to change every query. Hopefully you've been using two-part naming so far, so every dbo.reference will need to change. Far from ideal.
Picking some random number of databases (such as 35 in your case) is a useless number to recommend.  As I can easily create a single database which has 1 TB of data change per hour, so unless the network link to DR can support 1 TB of data change per hour the number of databases on the instance is meaningless.
What NetApp should have told you was "You need to configure the server so that XXX Gigs of data or less are being changed per hour, and no more" where XXX is some number based on your bandwidth between the sites, amount of data that needs to be transferred, etc.
I'm betting that if you made this change (which I wouldn't recommend) that you'd end up with NetApp telling you that they still can't replicate all that data within an hour.  The reason being that basically the same number of blocks on disk will be changing so it'll still take the same amount of time to replicate the data.
You still have a lot of flexibility over filegroups, which is good, and having a single database might even help avoid disk head movement on the transaction log.
I doubt performance will be affected much either way. But the code changes required could be a huge frustration. And if you need to rely on the default schema, there are extra locks (LCK_M_X) that need to be taken out to work out which objects you're referring to.