Having said all that, however, Glacier just may not be the best approach for your needs.  Glacier is really meant for data archiving, which is different than just backing up servers.  If you just want to do incremental backups of a server then using S3 instead of Glacier might be a better approach.  Using a tool like Duplicity or rdiff-backup (in conjunction with something like s3fs) would give you the ability to take incremental backups to an S3 bucket and manage them very easily.  I've used rdiff-backup on a few linux systems over the years and found it worked quite nicely.
To avoid the surcharge for deleting data less than 3 months old this is likely the best approach. But it won't just be the data that doesn't exist any more that you need to track & delete.  As mentioned above, any time a file changes and you re-upload it to Glacier you'll get a new ID for the file.  You'll eventually want to delete the older versions of the file as well, assuming you don't want the ability to restore to those older versions.
So what this means is each file you upload is assigned a unique ID.  Upload the same file twice and each copy of the file gets its own ID.  This gives you the ability to restore to previous versions of the file if desired.
That's the tradeoff you really have to decide for yourself.  Do you tar/zip everything and then be forced to track those files and everything in them, or is it worth it to you to upload files individually so you can purge them individually as they're no longer needed. 