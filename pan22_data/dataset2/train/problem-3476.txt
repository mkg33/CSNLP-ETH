Modern setups use stripes of mirrored hard disks: this combines scalability/expandability with rebuild performance. Less 'efficient' than RAID5/6, but hard disks are REALLY cheap these days*.
ZFS can be raid-z{1,2,3}, stripe, mirror, and even RAID-10. Meanwhile RAID-5 is always just RAID-5. Hence you're comparing juice to apple.
One tip: RAID5 will take forever to rebuild an array on modern size hard disks (2TB or higher), and during that time the performance of the RAID array will be compromised.
Personally, unless you have a reason to pull the raid controller, I'd leave it in but map each physical drive through as an independent drive (i.e., set up one "raid group" per drive, each with one drive in it), then use ZFS on top of that.  This would let you take advantage of any battery backed RAM on the raid controller, but still let you use ZFS and get all of it's advantages.
You didn't mention if the hardware raid controller has a battery backup module on it or not.  If so, the controller will be able to commit writes as soon as they're in RAM on the controller... if not, it will have to wait until they are actually committed to disk.  Depending on your workload, this can make a major difference in performance of the raid controller.
Why not run benchmarks on your particular server hardware and figure out what is best for your combination of hardware and file usage?