The idea behind cross validation is to understand the performance of some measure of your model's performance on unseen data. This can be applied to loads of different statistics, not just ones relevant to classification like accuracy. Common measures for holdout performance of regression models include MSE and MAPE, either (or both) of which can be cross-validated over.
In classification type problem we know the class label so easy to compare, but how is it compare in regression type problem? 
p.s. In regression problems, the labels do not need to be "continuous" (as we define in the continuity of a function in calculus). They can be discrete but real-valued.
In both scenarios, we pick one or more performance measures and validate the model based on them. In classification, one may choose to use accuracy, precision, recall, or F-score. In regression, other metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R Squared Score (R^2) might be useful.
For regression type problem we know the result is a continuous value, so how is it be cross validated?
In regression/classification problems that the order of data points matters (eg time series), we cannot use conventional cross-validation. Instead, some special variations of cross-validation procedure must be used to make sure we do not train our model on the current samples and validate on past instances.