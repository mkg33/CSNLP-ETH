Usually people recommend /dev/random over /dev/zero; if you only use null bytes, it's easier to know someone tried hard to wipe files, and an external machine could recover the file even if a regular computer can't. Or you could just use the command shred - with the correct flags, it overwrites the file before deleting it.
modern OSs and professional servers have several systems to prevent loss of data. If you use disk journaling, you might not be writing random bytes somewhere else than your file - so you'd be doing just a slow and costly rm. (man shred explains this more in depth).
Were you trying to overwrite the file with null bytes? That should be a good way to erase a file and prevent any data recovery program from recovering it. If that's the case, you'd want /dev/zero, and even then, just doing a cat is too risky: it keeps appending null bytes until you interrupt the process or you run out of disk space. Using dd you can limit how much data you send to the file.
You caught my attention with cat /dev/null >. That's an odd command; /dev/null discards any data sent to it, so it's good for testing system commands, data disposal... and, apparently, disk reading tests.
That said, overwriting a file is much slower than simply deleting it. It's a good solution for sensitive data, however.