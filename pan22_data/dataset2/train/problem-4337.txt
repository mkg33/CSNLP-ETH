One thing that I noticed when I was trying to get your program to work is that all of your variables were declared up top.  This actually hindered things when I was fixing the parallel part because all of the variables became shared by default.  I had to move the variables to the correct scope to get things to work right (it could also be done using some #pragma options).
shouldn't be included in the parallelized region.  Since the multiple threads are all generating data at the same time, calling fwrite() could result in out of order writes to the file.  I suggest creating one master buffer for the whole file and then only writing the file out at the end of the parallel execution.  Theoretically, you could leave that fwrite() in the parallel part if you synchronized it such that the lines were generated and written out in order.  But that would slow down execution of your program.
Since the x and y directions are each being sped up by 4x, this results in a 16x speedup (close to the 13x speedup I observed).  But of course, only one out of every 4 lines was actually being generated.
Therefore, what is happening is that each of the 4 threads is computing the line at ypx=0.  In the xpx loop, the 4 threads all work together to complete that loop 4x as fast.  Then all 4 threads are writing the same rgb buffer for ypx=0 out to the file (4 times total), and then each advancing ypx by 1 (+4 total).  In other words, they wrote the first line out to the file 4 times in a row and didn't compute lines 1-3 at all.
What this part is doing is it is spawning N threads (4 in my case), and it is running the same code in all of the threads using the same shared variables.  Any variable declared before the #pragma is considered a shared variable, so in this case, xpx and ypx are being shared.
I ran your program with and without OpenMP to see how much of a difference it made (I commented out the #pragma to test the slow case).  I noticed the following: