I always use the SI (decimal) units for data throughput and IEC values (binary prefixes of 2^) for file sizes. From time to time however, I do see people calculating speed using a multiple of 1024 in their sums.
This Wiki page gives a detailed description of the problem and different solutions. I find that Wiki paged linked above a bit confusing.
With measurements of file size or memory I have always used multiples of 1024 (binary prefixes) such as 1KB file is 1024 bytes in size;
What are the correct usage guidelines for SI and IEC binary prefixes when using decimal (power of 10) units of measurement relating to computer networking and binary (power of 2) units of measurement (data transfer in particular, but also file size) ?
Purely from a networking and technical view point here (not legal regarding fair usage policies or T&Cs etc), what is a best practice, always used mega/giga/tera or mibi/gibi, IEC or SI notation for measurements and monitoring etc? How do you keep everything uniform?
It is possible for an ISP to measure 50GBs of data transfered as (1024^3) bytes * 50 == 50GBs (1GB == 1073741824 bytes). and the user may disagree when the 50GB limit is reached. A users who's machine displays Gibibytes for example would show 46.57 Gibibytes transfered.
A Gibibyte is this many bits: ((((50GiB*1024)*1024)*1024)*8)=429496729600 which is more. So a link that runs at 111.11Mbps for 1 hour hasn't transfered a Gibibyte.
There is some room for discrepancy to arise here between networks and customers. A standard billing issue example is that a customer of an ISP has a 50GB per month bandwidth allowance. Different operating systems use difference units of measurement for both speed and memory and display this using the different IEC and SI prefixes. This section of the same Wiki page on Operating Systems and Software lists the discrepancies between operating systems and software packages that mix and match different prefixes with units of measurement.