Sometimes correlated features -- and the duplication of information that provides -- does not hurt a predictive system. Consider an ensemble of decision trees, each of which considers a sample of rows and a sample of columns. If two columns are highly correlated, there's a chance that one of them won't be selected in a particular tree's column sample, and that tree will depend on the remaining column. Correlated features mean you can reduce overfitting (through column sampling) without giving up too much predictive quality.
I would tend to characterize this phenomena in something like a HDDT to mean the most efficient tree that makes no spurious decision based on available data, and avoiding all instances of decisions that may otherwise have been made on multiple data points without understanding that they were correlated.  
So moral of the story, removing these features might be necessary due to speed, but remember that you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection embedded in them.
Making a decision should be done on the minimum necessary variables to do so.  This is, as mentioned above, the formalization of Occam's razor with minimum description length above.  I like that one.  
In particular, methods like random forests and KNN treat all features equally, so thinning out correlated features directly reduces their signal-to-noise ratio.
Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other, and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a "good" feature, whereas if you remove B for instance, this chance drops to 1/2
More generally, this can be viewed as a special case of Occam's razor. A simpler model is preferable, and, in some sense, a model with fewer features is simpler. The concept of minimum description length makes this more precise.
If your supervised learning is for prediction, the answer - counter to conventional wisdom - is usually the opposite. The only reason to remove highly correlated features is storage and speed concerns. Other than that, what matters about features is whether they contribute to prediction, and whether their data quality is sufficient. 
Some algorithms like Naive Bayes actually directly benefit from "positive" correlated features. And others like random forest may indirectly benefit from them.
Methods that auto-select features like single trees, "pure" lasso, or neural networks, might be less affected. But even then, other than longer computing time, there is rarely anything to lose prediction-wise from keeping correlated features in the mix.
Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents:
Of course, if the features that are correlated are not super informative in the first place, the algorithm may not suffer much.
The answer to this question depends greatly upon the purpose of the model. In inference, highly correlated features are a well-known problem. For example, two features highly correlated with each other and with y, might both come out as insignificant in an inference model, potentially missing an important explanatory signal. Therefore, in inference it is generally recommended to thin them out.
Noise-dominated features will tend to be less correlated with other features, than features correlated with y. Hence, as mentioned above in the example by Valentin, thinning out the latter will increase the proportion of the former.
A good way to deal with this is to use a wrapper method for feature selection. It will remove redundant features only if they do not contribute directly to the performance. If they are useful like in naive bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting)
The keyword being harmful. If you have correlated features but they are also correlated to the target, you want to keep them. You can view features as hints to make a good guess, if you have two hints that are essentially the same, but they are good hints, it may be wise to keep them.
If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's razor. If your model is not "that much" worse with less features, then you should probably use less features.