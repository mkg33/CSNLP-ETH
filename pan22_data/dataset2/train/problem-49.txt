In your case, this comes to roughly 120+ GB of RAM, so not out of the question for current Xeon E5 server boards (128 - 512 GB usual RAM size per CPU). The article also contains a real world example with dollars that should serve you well.
The calculation is from actual pool size before the dedup, or more accurately, from the number of stored blocks in the pool (each block needs about 320 Bytes of space on the DDT, number of blocks needed varies depending on actual data stored). Therefore you would assume 6 * 5 = 30 as a rule of thumb.
If you're planning to use a Linux based ZFS implementation, before spending lots of $$$ on hardware, consider simulating your use case in a virtual machine. I would recommend testing the worst case for dedup (=100% random data). If you do not have the necessary virtualization resources at hand, be advised that you can always just spin up insanely huge instances on most cloud providers for a couple of hours for very little money.
First of all, the zfs_arc_meta_limit (the amount of caching memory that may be used for meta data, including the dedup table) has always been tunable (iirc). So even in very old ZFS versions where 25% might have been the default, you could use that setting to tune the amount of cache available for metadata. 
While user121391's answer is mostly correct, the 1/4 limit for meta data is no longer the case/has not been the case for a long time:
One last thing to consider: You can always tune the ZFS recordsize. Generally speaking, small record sizes will yield better dedup ratios (but obviously require more RAM for the dedup table). Larger record sizes will yield worse dedup ratios, but require less RAM for the dedup table. E.g.: While we're currently not using dedup on our ZFS backup storage, I have set the ZFS recordsize to 1M to match the block size our backup application is working with.
There's also a tunable if you would like to make sure a minimum amount of memory is always reserved for meta data:
In ZFS on Linux 0.7.0, it seems like there will be a way to tune the amount of memory with a percentage limit:
In case of a backup system where most of the user data is rarely accessed, >=75% for meta data + <=25% for user data might be more appropriate. Please keep in mind, that said tunable is the available amount of memory in bytes, not a percentage.