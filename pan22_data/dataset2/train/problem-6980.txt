The figure describing momentum is a bit misleading because it only considers a very simple case. Usually momentum will help you not to get stuck too early in a local minimum, so you end up in a better but still local minimum. 
Also, momentum is compatible with other forms of regularization. You can apply momentum to any regularized cost function. The statement in Elements of Statistical Learning is more of general nature, meaning that even if you could achieve the global minimum it wouldn't be desirable because it is most likely not the solution that will give you the best generalization (even though it has the best training(!) error).
Momentum can help you converge faster, but there are no guarantees to end up in a global minimum. In popular use cases of neural networks like image classification etc. the error function will be extremely complicated, so it will always be infeasible to reach a global minimum using gradient based methods. But practice has shown that driving down the error to a local minimum still leads to good results. 