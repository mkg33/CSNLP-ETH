Once this is done, you can use a high availability virtualisation system, which "only" protect you against hardware failure.
Running some of these checks inside a VM is hoorrendously difficult.  For e.g. In Windows 2003 Clusters, it requires a Quorum drive that it uses a SCSI lock on to ensure that it is the owner of the resources.  On failures it also sends out 'poison packets' to acquire that lock.  All of these feature are near impossible to implement without a RDM to a LUN.
I once attended a course for a NAS-system where they told us that the NASA goes that way - each piece exists in three different flavours. Only if at least two of them have the same result, the result is ok. Apart from that everything has to be redundant (in each of the three pieces).
Don't get me wrong, clustering, SAN snapshots, VM snapshots, off-site replication, HA lock-step virtualisation, Etc., have their place, but just make sure you choose what's required, not what looks nice and shiny.
You need to understand the required uptime for your application, and more importantly, the maximum amount of time your application can be unavailable when it does fail.  And it will.
So let's say your first level of high availability is done with 2 computers : you now don't need to bother about the second level, because it won't give you anything better.
This second point is critical; I've seen a "five nines" application being managed by a large systems integrator that was offline for nearly a day because the complexity of the technology being used to keep it highly available.  For day-to-day operational availability, the technology ticked the boxes, but when something went wrong with the config, the folks at the aforementioned company were properly stuck.
I think you've got the essence of the ideas about availability down.  Both Hyper-v and VMware HA functionality does not provide HA to the guests, just HA of the virtualization service.  Based on the availability requirements of the guest services you also require HA at the guest level (and depending on the technology involved may mean clustering).  You need to evaluate each service for the particulars on how to provide that required uptime.  SQL server for instance could use either transaction mirroring or server clustering.  Many times the additional overhead and challenges in clustering on the virtual services outweigh the benefits provided and it may mean that the service ends up being provided on dedicated hardware instead. (picking on sql server for a bit) SQL server is usually a potential candidate for remaining physical due to the potential for high network, IO, CPU and memory utilization as well as a need for redundancy.
So in the case of Microsoft Windows 2003 clusters (and I had to virtualise I'd use your 'child' approach).
All of these 'hardware detection' components will have a large overhead within a VM.(VM performance is always great for user apps, but anything kernel base will always incur varying degrees of overhead).
You first have to build a high availability system (for SQL, for the OS, etc). This mean you must have more than one physical or virtual computer, and you must use software able to support high availability.