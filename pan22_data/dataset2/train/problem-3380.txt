I think you are over-complicating the problem. All you need to is to have something like NFS. So, one server will access the folder locally and other one will access it remotely via NFS. I don't think NFS is that slow especially if the two servers are adjacent to each other within the same subnet.
Yeah, gluster i/o performance will not be as good as something more physical, but I'm not sure how you will be able to get around that given your setup.
We're using Gluster in a similar scenario (Ruby apps, not Drupal).  In your case, both machines would be both gluster servers and clients.  The Drupal installation would point at share as seen by the client configuration.  File operations on the share are propagated throughout the cluster, and you should be resilient to one node failing.
To address both of your problems, use unison. Is based on rsync and is addressing the problem with deleted files. 
Each node has its own local copy of the data (so reads are as fast as your local disk); you can configure it so that writes block until the data has been written to disk on the other nodes (so there's latency that depends upon the speed of your network, but all clients will see a consistent view of the files).
Have you consider using multiple caching frontends and a single drupal + database backend? Pressflow is an enhanced version of Drupal that has built-in integration with memcached and Varish (caching frontend).
Is it necessary for you to use 2 copies of Drupal? Drupal makes a lot of queries per page request, having multiple Drupal front-ends sharing a remote database backend can be a pretty big performance penalty.