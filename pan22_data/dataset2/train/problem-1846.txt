Depends on the type of index! I presume you were thinking of an "ordinary" binary tree index. These are typically used by "planners" when maybe 10% of the rows of the table are expected to be retrieved. Why such a low percentage? Don't forget that the DBMS has to retrieve index blocks, not rows, and much of the data retrieved won't be scanned for the row being looked for (ie overhead). Also, an index itself uses space, it's certainly nowhere near a "free" or even constant-time (and hence space) operation.
Binary columns do not generally make a good index used alone, but may be used in conjunction with another column. 
And in my answer so far I've assumed that we're talking about an ordinary index lookup for a value. There are many other index scans used in Oracle for example (see the link I provided).
An "id" column (for those who may not be familiar with Ruby/Rails, consider this an auto-incremented, unique, primary key integer column) most often MUST be indexed due to the nature of its use. If it is declared as the primary key, you don't need to declare an index ... it gets one anyway.
As for what that cutoff is, it depends.  Almost any modern enterprise relational database is going to have a cost-based optimizer that tries to work out the best query plan given a bunch of statistics about the distribution of data, the relative expense of different operations, etc.  Different databases and different versions of the same database are going to rely on a pretty wide range of settings, statistics, and other bits of information in order to figure out where that cutoff is.  My rough guess would be that when you're retrieving more than 10-15% of the rows that you're probably in the range where a table scan will be more efficient.  But it is easy enough to come up with cases where you're better off with a table scan if the index only gets you 5% of the data or that you're better off using an index to retrieve 25% of the data.
I can't tell you theoretically when it would be ideal to use a index, I'm sure a math model could be created to tell you that.  In practical terms within the SQL Server, In their demonstrations a index was used if it could avoid scanning 90% of the table.  The results varied though.  Sometimes it would use a table scan at a much lower percentage.  It had to do with carnality in your statistics as well.  
Varchar columns may or may not make a good index, depending on the contents. Many DB engines will allow you to index only the first columns of a varchar. Good example: first 8 of the last name plus first 4 of the first name.
Then there are also bitmap indexes: these store a bitmap (ie 0's and 1's, 1 for each row) for each unique value. These are ideal for booleans and other low cardinality columns (but can also be useful in other cases as some papers have shown).
The downside to bitmap indexes is that they are very expensive to update and hence most useful for mostly read-only databases (like decision support systems, data warehouses and the like).
In general, yes, you are correct.  A b-tree index is more useful when it allows you to identify a relatively small fraction of the rows in the table that need to be returned.
The speed of certain disk-based operations impacts Oracle's decision to use either an index scan or a full-table scan too.
See http://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#autoId25 for a good overview of the considerations of the "planner" (at least, the ones thought of in a reasonably current version of Oracle).
I recently took a SQL Skills immersion (IE1) event by Paul Randall and Kimberly Tripp, Kimberly being the authority on indexing and SQL Server internals.
Good answers, both. Most database engines will optimize queries to minimize disk I/O, so if an index doesn't narrow the choices to a rather small percentage of the table, the query optimizer will use a full table scan instead. This percentage varies between DB engines, but is usually on the order of 2-10%. 