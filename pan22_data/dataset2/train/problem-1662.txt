With that said, I'm still dealing with sluggish keyboard events on the problematic VM, so I think I've only removed the high load average without resolving the actual problem. I'll definitely update this answer if I find the root cause.
I've been dealing with a scenario very similar to yours. In my case, the load average dropped after changing the IO scheduler of the problematic VM's block device to the NOOP scheduler. This scheduler is just a FIFO queue, which works well when the hypervisor will apply its own IO scheduling algorithms anyway. No need to reorder twice.
When you have a performance problem in a VM first you need to approach the problem from both supervisor side and from VM. Another thing to keep in mind is that timekeeping in a VM is not precise. This also means that stats measured in a VM might not be correct.
Load average is based on the processes waiting in the run queue.  That means if you have processes that use fractional time slices often you can see a high load average without a high CPU utilization.  
That doesn't sound like a particularly high load average. If you want to track it down, iotop is probably the best tool for the job.
The best example of this is mail.  The amount of CPU time require to send a message is very limited, but when thousands of pieces of mail are moving around the system (especially if the mail daemon forks processes to handle each one) the run queue gets very long.  It is common to see well functioning, responsive mail servers with load averages of 25, 50 to over 100.
For a web server I would use page response time as the primary metric, do not worry about load average.  Under modern schedulers load average less than twice the number of cores will usually have no negative effects.  You may want to experiment with number of cores per VM versus total number of VMs.  Some applications will benefit from many cores on a few machines, others are better at a small number of cores and many instances.
The load average is the number of runnable processes waiting for the CPU. A process that is waiting for I/O doesn't count at all. The "usual explanation" is just dead wrong.
On VM you can profile everything from application to kernel with perf and visualize the output with flamegraphs
What are the CPU and I/O stats for this VM? Pay attention to CPU ready counter - it should be under 5%. Which version of ESX are you running on? What is your hardware architecture in test and prod?