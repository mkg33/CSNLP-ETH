One option if you are getting pounded by a digg/redit rush is to crawl your site 2 levels deep and create pure HTML copies of the first few pages.  This will allow it to serve HTML and not touch the DB for a majority of the users hitting the site.
remember to swapoff the swapfile and delete it when you no longer need it, to recover the disk space used.
Only 80meg free however most of it is uses in the cache setup for caching file access from disk. If your swap line has memory used, then yes you'll need more memory.
you can also create a swapfile to help the machine limp along until you get more memory installed.  this is not a cure, running from swap is SLOW - but it should stop the machine from crashing due to lack of memory, which is worse.
One option might be to install a caching proxy, yes, it will use resources, but will reduce the load on your db and such (depending on your site, of course).
Another solution that you might consider is to use Coral Distribution Network service. You could choose to use it during 'peak loads' or have your home servers do it all the time. You can check online on some example mod-rewrite rules to distribute your traffic.