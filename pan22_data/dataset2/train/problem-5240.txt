Even my cheap 12V LED bulbs (halogen retrofits) have a little buck (step-down) converter build in, making sure the 3 led-chips in each bulb gets the right voltage and amperage. I tested this on a variable power supply, driving an LED bulb from 0 to 17 volts (I stopped going higher). At about 9V it started lighting and increased up to 11,3V, from where it actually got a little dimmer as current control kicked in. The light kept the same level all the way up to 17V, however the amps dropped as voltage increased. This is because the power draw (wattage) stayed virtually the same.
Technicians and Engineers regularly replace power supplies on equipment, it is one of the most common mods that gets old equipment working again, especially where strange old proprietary battery packs were involved. Often performance can be improved by providing more consistant voltage over a broader range of current draw. These mods are well within the hobbyist's capability, provided they are able to spend the time, do the tests, and set up a test load (12V automotive light bulbs work well) and use a multimeter.
I have an Acer netbook that uses a 19 volt charger and I run it all the time on 24v dc.  I watched the current in as I increased the voltage in and noticed that the current went down as the voltage was increased.  This means that a switching power supply is changing that voltage to a different voltage inside.  Keep in mind that the battery the powers a laptop is considerably lower than 19 volts.  So voltage conversion is taking place within the device.  The 19 or 20 volts is not being used directly on the motherboard directly. The reason I do this is so I can use the same charger to charge an external battery that I carry with me.  This battery is made up of 6 x 2 18650 lithium ion cells.  So it needs 24 volts to come near full charge.  I have been doing this for over a year without any problems.  This may not be true for all units but it works for me with my Acer.  Ed
I used a lower voltage power supply for a laptop for a while (emergency solution) and the battery life seems to have drastically have shortened after that. In general you should stick to exactly the power supply needed, preferrably the one your laptop supplier provided.
It applies to power delivered directly to sensitive microelectronics: mainboard and CPU, memory, graphics card, drives.
"if you use a 20v charger on a 19v rated thing, you will be "forcing" an extra 0.235A (V=IZ, Z is the appliance's impedance) into the device in addition to the 4.47A."
It depends, not on the text "19V" or "20V" written on the power supply, but on the actual voltage and current profile as provided by that power supply... which can vary wildly from the writing on the outside.
20V is 5,26% more than 19V. I wouldn't worry about damaging laptop or batteries. I'd just measure if it really produces 20V (or at least it's within 10% from 19V).
Comparing the proposed replacement supply voltage and idle, medium current and full current versus the original (requires still having the original) is the only way to find out for sure. Another caveat is what happens in a short situation. If one power supply has OCP (over current protection) and the other happily provides more current, that can be an issue too.
But in case of laptops, I belive, microelectronics are not fed directly from AC/DC adapter, because still various components need different voltages - it's not a desktop, but it still has CPU, memory and drives.