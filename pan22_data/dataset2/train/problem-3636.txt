To make a game like an RTS networked, I've seen a number of answers here suggest to make the game completely deterministic; then you only have to transfer the users' actions to each other, and lag what's displayed a little bit in order to "lock in" everyone's input before the next frame is rendered.  Then things like unit's positions, health, etc. don't need to be constantly updated over the network, because every player's simulation will be exactly the same.  I've also heard the same thing suggested for making replays.
The answer to this question is from the link you posted. Specifically you should read the quote from Gas Powered Games:
Use fixed point arithmetics. Or choose an authoritative server and have it sync game state once in a while - that's what MMORTS do. (At least, Elements of War works like this. It's written in C# too.) This way, errors do not have a chance to accumulate. 
Remember, if you're interested in late join, you're going to have to bite the bullet and sync anyway. 
There's a nice article about Source engine that explains one way to go about sync: https://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking
A deterministic game will only be deterministic when using the identically compiled files and run on systems that adhere to the IEEE standards. Cross platform synchronized network simulations or replays will not possible.
Regardless of whether you go integer lock-step or sync, handle the core game mechanics separately from presentation. Make the game play accurate, but don't worry about accuracy in the presentation layer. Also remember you don't need to send all the networked world data at the same frame-rate. You can prioritize your messages. If your simulation is 99.999% matched you won't need to transmit as often to to stay paired. (Cheat prevention aside.) 
I've heard some people suggest avoiding floating-point numbers altogether and using int to represent the quotient of a fraction, but that doesn't sound practical to me - what if I need to, for example, take the cosine of an angle?  Do I seriously need to rewrite an entire math library?
Short answer: It's possible if you're insanely rigorous, but probably not worth it. Unless you're on a fixed architecture (read: console) it's finicky, brittle and comes with a host of secondary problems such as late join. 
If you read some of the articles mentioned, you'll note that while you can set the CPU mode there are bizarre cases such as when a print driver switches the CPU mode because it was in the same address space. I had a case where an application was frame-locked to an external device, but a faulty component was causing the CPU to throttle due to heat and report differently in the morning and afternoon, making it in effect a different machine after lunch.
However, since floating-point calculations are non-deterministic between machines, or even between different compilations of the same program on the same machine, is this really possible to do?  How do we prevent that fact from causing small differences between players (or replays) that ripple throughout the game?
Note that I am mainly interested in C#, which as far as I can tell, has exactly the same problems as C++ in this regard.
You make them deterministic.  For a great example, see Dungeon Siege GDC Presentation as to how they made the locations in the world networkable.
These platform differences are in the silicon, not the software, so to answer your question C# is affected too. Instructions such as fused multiply-add (FMA) vs ADD+MUL change the result because it rounds internally only once instead of twice. C gives you more control to force the CPU to do what you want basically by excluding operations like FMA to make things "standard"--but at the cost of speed. The intrinsics seem to be the most prone to differ. On one project I had to dig out a 150 year old book of acos tables to get values for comparison to determine which CPU was "right". Many CPUs use polynomial approximations for trig functions but not always with the same coefficients. 