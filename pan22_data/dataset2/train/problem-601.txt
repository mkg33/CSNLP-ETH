Squid handles crashes better because it also uses the local storage disk, and can reboot some cache from there instead of using the NFS.
If you're still thinking about separate machines (my suggestion, as above, is to use one machine with one nginx), then it might make sense to investigate varnish.  Varnish does all of its caching in virtual memory, so, you wouldn't have to worry about vnodes or cache inefficiencies with smaller files.  Since it's using virtual memory, its cache can be as large as physical memory + swap.
The cache server has 3 storage layers, you want the most common files to be available locally, and preferably in RAM.
The NFS file request is the most demanding operation, therefore you need a form of caching in your "cache" machines.
And you're not crazy not to use a CDN. You're crazy if you don't investigate whether it's worthwhile though :-)
Nginx with this system stores files in the local filesystem disk and serves them from there. You can use PURGE to remove a modified file from cache. It is as simple as making a request with the word "purge" in the request string.
Nginx with Slow FS uses the ram the OS provides, increasing the nginx ram available by the OS will improve the request average speed. However if your storage exceeds the server ram size you still need to cache the files in the local filesystem.
However when varnish crashes the ram cache is lost and the whole server can take a lot of time recovering. Therefore a crash is a big problem for varnish.
nginx does almost all of its caching through the filesystem of the machine, so, if you're going to use nginx for this, you must ensure to have an operating system that has an excellent filesystem performance and caching.  Make sure your kernel has enough vnodes etc.
Nginx is a multipurpose server it is not extremely fast. At least not as fast as static caching servers such as squid or varnish. However if your problem is the NFS, then Nginx solves 90% of the problem.
One obvious problem with this is that the 'master' server is a single point of failure (any remedy for this?).  Are there other potential issues which I have overlooked?  Are there elements here which will not scale well in this way? Would anyone suggest an alternative approach?
I would recommend getting a single (potentially dedicated) server for this, instead of using several individual VPS servers and separate nginx instances connected through nfs.  If you're thinking about using VPS and NFS, I don't think your concerns about scalability are justified.
Regarding memory requirements, I'm assuming I should give each Nginx server as much as possible so that hot files can be cached (by OS? Nginx?) an not have to be reqested form the NFS share constantly.
I would highly recommend against squid.  If you want to know why, just look at a varnish presentation, which describes why virtual memory is the best way to go for an acceleration proxy.  But varnish only does acceleration, so if you're using a single host with static files and good filesystem caching (e.g. FreeBSD), then nginx would probably be the best choice (otherwise, with varnish, you'll end up with the same content double-cached in multiple places).
NFS does not scale. It adds latency to every request and will eventually become too big a bottleneck. We have a similar issue at work, but with photos (so, much larger files) and wrote our own software to shard and distribute them. For a few GB of files like you have, you might be able to get away with the upload process doing an HTTP PUT to all servers and doing resyncs when servers have been offline.
I need to set up some VPSs for serving static content (many small files).  I plan on using Nginx for this, and would like to set it up so that we are able to scale out relatively easily.  The requirements are:
Or tackle it another way: have a (set of) central server(s) with all files and caching reverse proxies (squid, pound, varnish) that actually serve the files to customers.
Therefore you need at least two machines as load balancers, you can use a load balancer like HAProxy. It has all the features you may need, check this HAproxy arquitecture example.
The number of caches is dependent on your resources, and client requests. HAProxy can be configured to add or remove cache servers.