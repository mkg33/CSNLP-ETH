I don't think so. The bottleneck here would be drive bus speed, and file access overheads. That's why copying many files takes so much longer than one file of the same size. 
Ideally you'd want to configure your backup scenario with a synchronization tool (like rsync) which will only backup new and changed files, dramatically reducing the time to finish.
It's not that bad and this time I'm willing to wait (it's a one-time operation), but I'm curious to know if there are faster ways to do this in case I have the same need in future (with a less powerful machine and less time).
If you plan to just keep the files archived on your local machine, then I think your .rar archive is a good call, but I can imaging having to work with those files on your project, out of the archive, could be a nightmare.
For such a large volume of files, you'd need a dedicated storage drive, preferably a fast (10K+rpm) one at that, and preferably inside a dedicated file server.
I have a directory with a huge number of jpeg images (more than 300,000 files) that are part of a project. This is somewhat of a nightmare to backup and move -not because of the size (total ~3 GB) which is rather large but still manageable, but because of the number of files.
You might also be interested in the windows port of tar. Tar is a fast archiver that is designed to dump all files into a single .tar archive, without any compression.
Since I must keep those files I thought I'd combine them into some kind of archive. I'm trying WinRar with "store" compression, but it's quite slow.
The "store" option in Winrar will definitely speed it up, but creating the archive on the same drive where the files are stored, will slow it down too. Creating an archive on an external device will also hinder performance.