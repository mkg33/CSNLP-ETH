As far as scale - clearly, a number with a precision and scale of (8,0) can represent a number three orders of magnitude larger than one with a scale of (8,3) (12345678 > 12345.678). To my way of thinking, that is a valid definition of scale.
As noted in ypercube's Wikipedia link, precision is related to the concept of significant figures (or significant digits).
Just to be clear: This is not a rant disguised as a question, and more than mere curiosity. I am convinced that there is a good reason for this terminology, and I think I can learn about databases by understanding the underlying thinking here. (If this question is considered too basic for here, I understand.)
Total number of digits has direct implication on storage, and thus on GB and IOPS needed. From that then you allocate a portion to precision.
You miss one critical thing. Critical brutally in the past, critical for anyone not doing trivial work these days.
Total number of digits has no meaning at all as everyday concept. But those of us who deal with large databases often live with it. Saving a handfull of bytes on a hundred billion row dable is not exactly bad for performance.
Example: If you have a scale that measures mass to the milligram, and you have a precision of eight digits (three to the right of the decimal point, just for argument's sake), you can say something has a mass of 12345.000 grams, 12345.333 grams, or 12344.555 grams. If, on the other hand, you only have a precision of five digits, all of these would be listed as 12345 grams - therefore, your representation is less precise.
To the naive non-expert (=me) this is always a bit confusing. I would have expected it the other way round.