Consider monitoring the system with a tool like sar or Munin.  This will give you information on which component are busiest.  I like Munin for its graphical reporting, also the ability to set alerts.  sar gives good details on disk and swap activity.
Well my first instinct, if I believed that the slowdown was due to excessive traffic on a URL or file, would be to check the web server logs or any analytics you might have. This will tell you what traffic you are experiencing.
I suspect that some particular large file or URL is being hit a lot, but I am failing to sort out which one it is. Are there some standard troubleshooting tools or steps recommended? Thanks!
Note that is is normal to have a large number of connections in the WAIT state (at least from what I've observed). A large number of SYN connections may be to do a SYN flood DOS attack. 
The following script counts the number of established connections by IP address and is useful to see if there is one particular misbehaving IP address.
There is no general way of debugging poor webserver performance, but there are some tools which should be helpful for you. I assume you're using some http stack under linux.
Every sysadmin who have fighted such problems as your, would probably enumerate many more quick hints, but the fact is, that all this rules depends very much on the environment (hardware, operatoring system, webserver, application, database) and can be only learned by experience. Often quite painfull.
Including the service time in the Apache log is useful.  I usually change the remote logname (ident) field (%l) with the time taken to serve the request (%T) in the extended log format.
The following a two scripts I use to get a basic idea of the status of connections on a server using netstat. The first one just displays the number of connections based on the connection type:
Note that depending on your application multiple entries per IP is not necessarily bad. For example, in my case 10-20 entries/IP is normal but beyond that amount is suspicious. I've seen 'bad' IP addresses that either request the same document/file 100s of times or walk through every file on the site and download it. The last script makes it relatively easy to spot these IPs which you can then use to look in your logs for more details if needed. 
If your server is under heavy load and you have 1000s of connections this script can take a little bit to run (~1 minute in worst case I've seen).