Theoretically speaking, if your unit tests have 100% code coverage (i.e. every single method in your code is tested) your software should be correct, provided the tests themselves are accurate and realistic.
The answers so far covered already most of what should be said about the foundations of how relate specification and code to each other. I just want to add a more practical point that approaches the question in the header of this thread: 
At this point of the quest after trying some tools (Z, B, VHDL, and Estelle), I'm using TLA+. This is a variant of temporal logic with software tools for model checking and mechanic proofs. I think that I choose this approach because L. Lamport was behind it, the syntax was simple, there were lots of examples, there was a community taking care of it, and the language and tools were fairly well documented.
Regarding my initial question, I think that there is not a complete answer. However, it is worth learning that it pays off to specify formally some portions of a system. It is also pretty useful to reverse engineer some complex ones. That is, It is effective to create a blueprint for the difficult and critical parts. However, I don't think there is an effective method to translate automatically the specification to a programming language or framework (unless you restrict the project to a very specific environment). I also do not think that having a formal specification should prevent you from testing the software.
One approach to reducing the gap between a program and its specification is to use a language with a formal semantics.  An interesting example here would be Esterel.  Have a look at GÃ©rard Berry's web page for some interesting talks about his work bringing formal methods into the real world.
There exists tools that automatically analyze your code for errors (violations of the specification, or "typical bugs"). To my knowledge, these methods mostly build on static analysis and are not immediately related to the theories you mentioned (LTL/CTL/...), but they do find errors in real code and it is already feasible, from a practical point of view, to use such tools in industrial projects. I personally have not used many of them, but it seems that these tools start being accepted by practitioners. For further reading, I can recommend the following blog article:
See for example: "Challenges in Building Fault - Tolerant Flight Control System for a Civil Aircraft"
Two or three years ago I started taking a look to formal methods applied to software. This was a quest driven by curiosity, and by the feeling that I had to learn programming tools and methods with longer time-spans. Although I dreamed wishfully about a Silver Bullet, I really thought that there was not an answer to the question: "How can I write a correct program?".
Unit testing? Write a test for every single requirement in the spec, and then test every single method in your implementation to see that its output/input conforms to said spec. This can be automated so that these tests are run continuously to ensure no change ever breaks previously working features.
N-version programming (NVP), also known as multiversion programming, is a method or process in software engineering where multiple functionally equivalent programs are independently generated from the same initial specifications. The concept of N-version programming was introduced in 1977 by Liming Chen and Algirdas Avizienis with the central conjecture that the "independence of programming efforts will greatly reduce the probability of identical software faults occurring in two or more versions of the program".The aim of NVP is to improve the reliability of software operation by building in fault tolerance or redundancy.....
In a nutshell, I think that the following metaphor (from Lamport) is really powerful: "Do you expect a house to be automatically build from a blueprint? Will you buy a house that is not yet build and it does not have blueprint?".