Recently, we've invested in a number of vmware clusters.  We've already migrated over 100 physical servers onto them and all new servers Intel will be vm (unless there is a HUGE reason why not).  
This allows us to upgrade/replace the server nodes without downtime, at our convenience one at a time.
Like Techboy our servers are assests that are devalued over 3 years at which point they are replaced.
Extended warranties become more expensive throughout the lifecycle of hardware. Things like a SAN is worth the cost because it's a big hassle to replace a large SAN, but a server isn't as much. We've done costings on this it's always worth it (for us) to extend the warranty by 1 year at least before we deocommision a server (or virtualize it).
Never if we can avoid it.  I work for the government.  This means that we still have Pentium IIIs lying around.  :-(
I have only worked in large companies where the servers are treated as assets that get devalued (and therefore replaced) every 3 years. Plus support costs increase after this time so it can actually be more economical to replace large servers after this period.
It might be riskier to replace some business critical servers than it is to keep them running - it's a valued judgement you need to make on a case-by-case basis.
Typically we replace Intel servers after 4 years, when the manufacturers warranty has expired.  We want to limit the number of servers that are "out of warranty" and have to pay additional maintenance for.