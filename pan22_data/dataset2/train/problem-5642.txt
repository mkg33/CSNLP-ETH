The chances of #2 increase as you have more disks in your array, and follows the standard "bathtub curve" of hard drive mortality.  This is part of why you should have a backup, and one of the many reasons you hear the mantra "RAID is not a backup" repeated so often on ServerFault.
Also be aware that when you replace a disk in a RAID 5 there is a lot of resulting disk activity as the new drive is rebuilt (lots of reads on the old disks, lots of writes on the new one).  This has two major implications:
Of course, that means you will need to mdadm --fail and mdadm --remove partition sda1 from array md0, even though it appears to be fine right now. And when you install the replacement drive, you will need to ensure that its partitions are at least as large as those on the old drive, so that its partitions can be properly added to the md0 and md1 arrays.
However, md0 seems to be ok. So, what does all this tell me? Can the disk be faulty even though  md0 is working? If not, can I just re-add /dev/sda2 to the md1 array to solve the problem?
Even though /dev/sda1 appears to be working fine in md0 now, the fact that the other partition on the same disk (sda2) is faulty bodes ill for the health of the drive. I must concur with the other opinions already expressed here: replace the sda drive immediately.
I would recommend to replace the disk as soon as possible because if you lose another disk, all your data will be gone. 
Like SvenW said, replace the disk AS SOON AS POSSIBLE (Follow your distribution's instructions for replacing disks in md RAID arrays, and for God's sake make sure you replace the correct disk! Pulling out one of the active disks will really screw up your day.)
RAID 5 is N+1 redundant: If you lose one disk you're at N -- The system will keep operating fine as long as you don't lose another one.  If you lose a second disk you are now at N-1 and your universe collapses (or at the very least you lose lots of data).
I've got a server running Debian Squeeze and a 3x 500 GB-drive RAID5 system which I haven't set up myself. When booting, the status of one partition in the RAID-array seems to be bad.
Keeping the array working with a broken disk is the exact purpose of a RAID5. It keeps redundancy informations so you can lose one disk and still don't have data loss. 