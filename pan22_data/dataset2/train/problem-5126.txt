The most direct predecessor to the operating system would a combination of standard libraries for interfacing with devices, and the bootstrap loader system for getting a program brought into memory and started running.
Or the pre-UEFI PC boot process: processor starts executing in the BIOS. This loads the first sector off the disk and jumps to it. That looks for an active partition and loads a bootloader from there, which loads the operating system. Originally that would have been COMMAND.COM for MSDOS; now it's usually NTLDR.EXE for Windows.
In the beginning, the programs were hardwired into the computer, which would start running the program immediately from a particular location on bootup.
Then various forms of offline storage were invented: punched cards, tape, drums, even disks. Much more flexible. But not directly accessible from the CPU. The program needs to be loaded into memory before it can be run. So you write a program to load your program. This is known as a loader, or bootstrap (from the expression "to pull yourself up by your boot straps").
It should be noted that in early hardware (before 1960), I/O was much simplier.  You could read a card, or punch a card or print a line on the printer, each with a single instruction: the buffer size was fixed, and often the buffer address was fixed too.
Even in the early 60s, with more sophisticated processors (eg. the 7090), you could still read or punch cards with a small routine (about 20 instructions), that was easily copied into each program.
Since the computer was entirely dedicated to a single job, it didn't matter if the processor was idle while waiting for the card reader to be ready to read the next card, or for the line printer to feed up the next line.
Well, it did matter, because computing time was expensive actually.  This is why people invented multi-processing, time-sharing, added asynchronous I/O, and interrupts and device drivers and operating systems.  The hardware device interfaces became more complex, for the programmer, giving access to lower level I/O registers, which required more complexity from the device drivers.  This complexity cost (memory, programming time) was amortized over the several programs using the devices "simultaneously", multiplexed by the operating system.
This era saw the advent of most of the features we think of when we talk about a true operating system. Debugging, programming languages, multi users, multi tasking, terminals, disk type drives, networking, standardization of components, etc were all introduced in this era. This time saw a giant leap towards standardization of much of this which meant that we had more standardized devices but still each OS was hand crafted for each machine which meant that OS functionality was severely limited by whatever the engineers who designed that particular system decided they needed. During this time, there was a substantial grey area in what an operating system was because the different architectures handle thing much differently and a more general purpose machine will need a lot more OS than a machine which includes hardware to handle the same jobs. The fact is that hardware is always going to be faster than software and practically anything done in software can theoretically be done in hardware (it is cost\flexibility\size\time\etc which limits us from making almost pure hardware versions of everything to this day). An OS was made for a particular computer or type of computer; it would not work elsewhere. Each new computer design needed all low level OS software to be rewritten from scratch to work with a particular machine model. Near the end of this time a new OS emerged which would soon change this paradigm, UNIX written at Bell Labs by Ken Thompson and Dennis Ritchie.
The very earliest computers had the equivalent of what an OS does now built into them. You (the operator) were also part of the operating system as well. You flipped the register switches (or used a punch card) and physically swapped bus wires (think of the old fashion telephone operator station) and memory was linked (via physical wires) directly with light bulb (the monitor of the day) and printers (the long term storage) in such a way that the program output would light up and print directly to the device as it was being placed into the output memory buffer. There was no driver needed for these things because (due to the way those physical wires were ran) they 'just worked' (there was also no such thing as a monitor in these days. In fact it was still going to be a few decades into this era until a digital numeric display would be invented so that you could actually see the numbers you'd already entered into the register and the output as decimal numbers; printers ruled this entire era until monitors . They were wired exactly as they needed to be to work correctly. None of this part really changed much with the switch from mechanical (1890s) to electric analogue (1910s) to digital (1930s). This 'Plug N play' architecture was replaced with the interrupt system during this time and would not resurface again until the late nineties; of course by then there'd be a lot less plugging. With interrupts, devices were allowed to take CPU time which allowed architectures which weren't directly tied to hardware but it took several generations for this to really be the streamlined process we see in x86 arch (and newer); early systems often ran into horrible race conditions, hardware compatibility\delay problems, and other odd behaviours where interrupts are concerned. Because each machine used radically different (an experimental) architectures in this period; nearly all devices were custom made for the machine they worked on.
Still in the 80's I was using a micro-processor based computer, emulating one of those older systems.  The machine instructions had a uniform format over 80 bits (10 octets), and the instruction to read the first sector of the first hard disk and store it into the memory at address 0 was very conveniently: 0000000000.  So the boot procedure consisted every morning to type this instruction on the terminal, which stored it at address 0 and executed it, which then loaded the boot sector and continued execution at the next instruction (at address 10).  The file system consisted in a static table mapping file "names" to ranges of sectors, which were manually allocated!  I/O was done in assembler to those files by directly reading or writing the sectors, offset by the position of the file on the hard disk which was recovered by the "open" routine.
A single program changed all of this but it wasn't UNIX. It was the C compiler (which was famously made in a garage by Ken Thompson and Dennis Ritchie after Bell Labs cut it). Until this point, any time you wrote code it was either machine code (code that the machine directly understands but is not portable) or it was written in a language which compiled your code to byte code (code which is interpreted by another program as it runs). The huge difference for OSes that C brought with it was the ability to do what is known as cross compiling into machine code. This meant that code could be written once and compiled to run across many different machine types natively as long as a compiler had been written for that machine. Operating systems must be written in machine code because machine code is literally the only code that the machine knows.
I would say that it wasn't until Ken and Dennis first compiled the UNIX kernel using a C compiler that a true OS in the modern sense was born. Before that, an OS was either a physical object or simply a pre initialized chunk of memory space designed specifically for a particular machine. Adding new devices to the system literally required that 'kernel' code to be rewritten. Now, the UNIX OS that they had designed for a particular machine could be recompiled and ran on other machines without rewriting EVERYTHING (as long as that machine was able to compile a C compiler from a bootstrap environment the rest of the OS could be written in the relatively high level C code).
As the system gets more complicated, you may have a simple loader load a more complex loader. This started with microcomputers: the normal tape loader was slow, so load a decompressor and fastload the rest of the tape. Or disk speedloaders which doubled as copy protection systems by doing nonstandard things with the disk.