An ultimate speedup is achieved by preprocessing the dictionary one step further, into the binary format. Group the words of the same hashcode together, and prepare a TOC as an array of file offsets of each such group at the beginning of the file. This way the dictionary loads in a single fread() (you may also want to account for endianness with bit more sophisticated format).
You can probably increase the I/O performance of your code by reading from disk in chunks. The macro BUFSIZ, declared in stdio.h is the size of the internal I/O buffers and will probably be the most efficient read size. Use fread to fill the whole buffer in one call. Handling the seams can be a bit tricky if a word spans two buffers, but it can be done with a bit of care.
Now you can just read the hash in directly instead of computing it.  Without knowing your hash function, it's not certain that this will be of any benefit, though.
Currently, when you get a hash collision, you append the new word to the tail of the hash bucket.  It would be faster and the code would be shorter to prepend to the head instead.
Most likely, your hash function is the slowest part of the process (although you should profile to make sure).  What you could do is to precompute the hash of each word and save the hash to the input file next to each word.  For example:
If there is a particular reason that you need to keep your buckets in alphabetical order, then you could just preprocess the input file to list the words in reverse and then you'd end up with the same ordering in the buckets.
I also found strange that you add nodes at the end of the list. Unless you have a very strong reason of doing so, crawling lists is a pure waste of time. Inserting node at the head unconditionally may - or may not - gain some performance, depending of the quality of the hash.