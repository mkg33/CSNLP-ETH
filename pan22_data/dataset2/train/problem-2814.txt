My department does data migrations when a client switches from another software vendor to us, often we need to get a copy of their old data (whatever that may have been) and send it to us.
After doing some more research after asking this question I think I found a solution that fits all the criteria, rdiff. 
The big challenge we face is some systems will have hundreds of thousands of files (mainly document/image repositories) that the whole collection can be in the 10's of gigabytes in size. We grab a copy of their data at the start of the conversion process, then we grab a 2nd set right before the install which could be months later.
Tools like rsync seem like the perfect solution but from what I have researched there is no easy way to do "write only" a account like we did with the FTP. Preventing un-authorized downloading of another client's data is a big concern from the higher ups.
All that needs to be done is write a wrapper application that acts as self extractor for rdiff.exe, cygwin1.dll, and cygpopt-0.dll and then provides a easy to use GUI interface for the relevant rdiff operations.
Currently the frontrunner is rsync and writing some sort of user manager that would create a separate user account on the rsync server per client, but I am sure there are other options I do not know about which could be better suited.
After doing that we just make a signature file before the initial zip is performed and transferred and keep a copy of it on file. Once the 2nd transfer is going to happen we use that original signature file to generate a diff and only upload the diff to the FTP server.
We are looking to find a better solution for uploading that 2nd set of data. Right now the main method is just creating a large zip of the whole directory and FTPing (via a write only account) it to our server, that of course has a large overhead due to a large portion of the files are that likely have not changed seance the initial data grab.