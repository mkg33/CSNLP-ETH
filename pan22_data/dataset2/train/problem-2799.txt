If you have a large enough test set, you can split it into parts and get a measure of the estimate error in your metric. This will help you see whether differences in the test metrics are significant.
Before you test, it is a good idea to pick a metric that you care about for your intended use. Accuracy is not the only metric, but it is probably fine for your purpose. You might choose other metrics if there are different costs associated with incorrect classifications - e.g. if assigning negative sentiment needs to be done cautiously.
If you are in a hurry, you could just search for and use someone else's dataset e.g I found these ones on a quick web search: a Kaggle competition and a free sample hosted by Niek Sanders (I have no idea of the quality of these - you should sample them and see if the data is useful for you). There is a risk though that these data sets were used to train one of your classifiers, so it would give you a false high rating.
You can classify a few of the tweets yourself, and compare afterwards which of the two algorithmic results is closer to your classification.
Ultimately what matters is the results of using your classifier in a project. Just labelling data has no inherent purpose. It is the consequences of those labels that matter. So you need to be driven by results. If you use one classifier and get feedback that the product is not performing well, you may be able to trial another one. If your product is used by many people, and enough are giving feedback of quality, you could even A/B test a couple of classifiers in production.
This can be the best approach, especially if you can source data that you know matches your project goals. However, it would take a long time and a lot of effort.
You might look for emoticons inside the tweet-text only , that might work if you have lots+lots of tweets.
Then run each classifier across your test set and calculate the metric. The classifier with the best metric is your best guess at the one you should use.
It is important to source this yourself to ensure it does not overlap with training data used by any of the models you want to compare (if it did overlap then it gives a big advantage to any model that trained using it). You will need to collect ground truth data on sentiment for all the texts.
Each of the sentiment analysers should explain the model class and data used to train it in documentation. Quite often you can find papers comparing the different approaches and quoting accuracy scores on a standard data set. If you are lucky, you will find enough comparisons that you get some sense of which algorithms are considered "cutting edge" and which are out-dated.
Without more information we cannot tell what these algorithms were doing. It may well be that they were just using different thresholds internally: Algo 1 decided that everything > 60% threshold is "positive", all < 30% "negative", all in between is "neutral". Algo 2 may have used 75%/25%.
A variation of (1), you can do this at any stage, and it might work well in the context of a live product. For all the data that is classified, log how different classifiers respond to it, and where there is discrepancy, save it for later assessment. You will need some people to read and help classify the text (ideally without seeing what the classifiers thought), and you can collect metric data over time and use the classifier that does best once you have collected enough examples to make a clear decision.
By the way, Twitter messages are not really well-suited for sentiment analysis. They are too short and grammatically too messy. 