Sidenote: I would suggest that your core premise that games are fundamentally different to applications due to the entertainment factor is incorrect. Sneak a peek at something like Danc's "Princess rescuing application" presentation ( http://blog.iainlobb.com/2008/10/princess-rescuing-application.html ) and see what you think.
The only real way we have to test such things is with, well, testing. Professional game developers will tell you all the time that the single most effective way to know if certain gameplay is working and fun is to give it to players and observe.
Indeed, one of the reasons that achievements are everywhere these days in games is because they give invaluable information to the developer. If you want to know where players stopped playing your game, make an achievement for completing each level. If you want to know how many players aren't playing a certain race in Civilization, look at how many people got the achievement for playing that race. And so on.
Right now, the only useful metrics we have are sales figures and ratings (eg. MetaCritic). We don't understand the domain well enough to do much more than that, which is why you see a lot of copycat games - taking a game that we know to have worked and tweaking the formula a bit gives a good chance of having a second game that works. 'The acorn doesn't fall far from the tree', and all that.
Some designers have attempted to come up with broad checklists or systems for judging the quality of a game or its mechanics. eg. In Jesse Schell's 'The Art of Game Design' he describes 100 'lenses' which are each a way of critically evaluating a game. This provides a system of measurement, but the measurements themselves are still fairly subjective. Similarly the MDA system (Mechanics/Dynamics/Aesthetics) broadly implies that your mechanics should combine to form dynamics, and your dynamics should support the aesthetic, but again this provides a tool for measurement but no units of measurement.
The usual "silent observer" method of user testing I suspect is sufficient for a lot of metrics with follow up questions to explore any areas of confusion about what the testers might be feeling.