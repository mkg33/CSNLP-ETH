Yes, we are. Developing web applications is simpler than desktop applications (for many tasks). But the terminals have become less dumb.
The biggest factor seems to be the network.  Many of the "Terminal style" systems available move everything, including drawing to the screen to the server, requiring the entire screen be sent across the network.
Dumb terminals have long returned in the incarnation of the thin-client. Bit different, but much the same. 
The problem is that most of the new "dumb" terminals like Wyse thin client terminals are actually quite expensive. It can be hard to persuade management to buy machines with reduced functionality compared to a desktop PC of almost equivalent price.
We've moved half-way around, from mainframe to pc to cloud now. It'll not happen tomorrow, but sooner or later the wheel will continue spinning and initiate the move back to more powerful workstations. Maybe it'll be a resource hungry OS, maybe it'll be cheap and green power, maybe it'll be the realization virtualization is not the egg of Columbus either. Who knows.
Another factor is energy costs.  Many computers capable of running as "terminals" use FAR less power than a typical desktop PC.  (10's of watts instead of 100's of watts) 
As networks become more powerful, and energy costs go up, consolidating processing becomes once again a viable alternative.
If you ask me, it's a circle. Sure, the cloud will have it's day. Just until Intel figures out a way to get people to move back to individual workstations (=profit). 
Current solutions include VDI (Virtual Desktop Infrastructure) from a host of different vendors (ranging from VMware to Red Hat and way beyond that), Citrix, Sun Ray... I could keep going for quite some time.