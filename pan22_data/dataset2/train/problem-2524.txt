Now o the load. You talk of a peak of possibly 100.000 to 250.000 queries per minute (depends how high your peaks are - if many people use that during work start, it will be quite massive). This is about 4166 queries per second.
If that is not enough to handle the load - but it may well be, if you do the db design right and get some higher end systems (dual six core opterons). You can posssibly fit the whole hardware for one unit into a 2 rack unit high cage - Supermicro has some out which have space for 24 2.5" hard discs. No need to go SAS - WD Velociraptors should be more efficient, and get a fast SSD and an Adapted RAID controller and you get the SSD as read buffer ;) Should be more than enough to handle your load.
I personally think youa re in the spce for SQL Server / Oracle clustering. Anyhow, on SQL Server you would possibly go with:
As you didn't provide a lot of specifics I'm going to keep this brief as well.  The language is really up to you, although C Sharp / ASP.NET would fit well here.  I'd go with a noSQL database such as cassandra: http://en.wikipedia.org/wiki/Cassandra_%28database%29
Other options in the large relational cluster arena, are not as scalable and the costs will be outrageous. 
Building a database system that can handle 50 million queries a day, is not a difficult task. With a large cassandra server we are able to get ~100 reads per second per core, and ~25 writes per second per core. Based on your number 50M I would suggest 2 8core systems. In order to get the performance numbers you will need to tweak the OS, disk setup and memory specs. 
Your mean rate of queries per second is 600. What do you know of the traffic pattern? Are all the queries going to come in at lunch time, only during office hours in a specific timezone, what?) Assuming all the queries are evenly spread over an 8h working day, you'll be planning for a peak of 2k queries per second.
Database? If you have to. A simple key/value store would have a higher performance. 1.5B records of (say) 4kB each is 6 TB. Try this architecture:
5 frontends talking to a duplicated set of data stores. Maybe use 40 servers for that, storing 300 GB each. This means you can lose any one host and still continue to serve. If you're going to serve a novel result most of the time, I'd double that to 80 servers: you'll be incurring at least one disk seek for every query and I wouldn't be so optimistic as to hope for a sustained 50 seeks a second.
MySQL can handle thousands of queries per second on decent hardware, and if the app is coded to be able to segregate read queries from update queries, it's really easy to set up read-only slaves for scaling reads. Whatever the language, make sure the app support persistent connections and/or connection pooling.
SImilar setups should be possible with - well... not sure. Oracle - yes. MySQL - someone can step in and answer.
Lastly, with that number of reads versus writes be sure to plan your hardware (specifically your drive speed) accordingly.