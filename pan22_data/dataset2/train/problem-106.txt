Jeff is right, the number of variables here is too large to pin this down.  If you run a web service that converts video to MP3 the CPU is going to be gone really fast.  If you run a site that streams video or music, the bandwidth will saturate.  Then you have what language did you write it in, how good were your coders.....
Also - for users, keep in mind that there are different types of activity - think of Amazon. Do some users just browse a lot, do some users spend a lot of time in reviews or in the message boards, do some load up shopping carts and then checkout? You have to measure the type of user activity from your production system, to make classes of users to use in your testing, in ratios that hopefully mimic what the production load looks like.
Such modeling systems exist, but they are expensive.  Take a look at  Hyperformix and Opnet as solution sets for your question.   One thing to keep in mind is that all models contain some degree to assumptions and at some point in the future you will need to run an actual performance test of your model/proposed model to see if those assumptions are correct and that the system scales as predicted.   More often than not the actual performance test results in new information which is used to update the model to make it a better predictor of actual application performance.
Continue doing this and you have your algorithm. However, at some point, you will stop being able to add web servers to improve response times. You have found a bottleneck that isn't your number of webservers. Determine what that is and fix it.
Aha - now you have changed another variable besides number of requests. Re-do all your work. Also re-do all your work every time you change the code, or add indexes to your DB, or improve your hardware, or even put more cache in your SAN (for example.)
So, for your code, you could ramp-up activity from one to lots against a single server with something like LoadRunner and get a graph of request rate (simultaneous users) and response times. Make an equation from that data that fits the line or curve.
If your site is very complex, large number of processing items and other or requires 100% up time you many need many servers. 
Small low size sites with no server side code can handle very large amounts of traffic and your likely to run out of bandwidth before the server is overwhelmed.
Now add a second server and do the exact same testing. See if the equation is the same. Now double the user count and see if the graph continues on the same equation, or if it's different.
The amount of servers you need is a wide range and depends on what your website does, how much traffic it gets, how important your site is and how much reliability you need.
Sure - and it will be different for every website and all its components : the hardware of the servers (assuming they're all identical of course), the speed of all the network links (external and internal, if it's DB-heavy.) 