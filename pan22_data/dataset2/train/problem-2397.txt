Based on this and the responses this guy received from Synology etc, it would seem that it may not be such an issue at all.  It seems that I was correct in assuming that home/small office NAS' use software raid internally and he comments that software raid is more tolerant  than hardware raid.
It depends on your needs, and whether or not the NAS is designed for consumer drives as well as raid drives.
From the drive's perspective, whether the RAID implementation is in Software or Hardware is largely irrelevant - except for that a hardware implementation may complete read / write operations a little faster allowing the drive to spend more time spun-down.
You should also consider a small UPS to protect your NAS, to ensure data write operations can complete in the event of a power failure.
The key factor to consider is how much time your NAS will spend running, and how much time it will spend powered down.  Most consumer grade hard disk drives are designed to spend less than a third of their lifetime spun-up, whereas server / RAID drives are designed to cope with 24x7 run time.
I have read comments around the web that say that unlike hardware raid, linux software raid (mdadm) will not drop drives that stop responding which would happen if the drive went into error recovery and which TLER is meant to prevent.  Therefore a desktop drive will work in a DS211 NAS.
If you mean SAS drives, no, you do not have to stump up the cash for them for home single-user use.  Normal SATA drives should do, and using RAID will give you some protection against HDD failure.
I asked a friend who has a QNAP 4 bay NAS which type of drives he used and he went for standard desktop drives from Seagate.
If the drives will be constantly spun-up, you would be better spending a little extra and getting server grade drives.
The NAS might also have options that allow you to tune these kinds of timeout values so that this issue doesn't arise.
If you intend powering your NAS down when you don't need access to the data, or if the NAS device has very good power saving capabilities which actually spin-down the drives when not in use, you can probably get away with consumer grade disks - but still expect to have to replace them sooner than you would if they were installed in a desktop PC.
I imagine the most common issue you might find is that the drives will not spin up in the amount of time the NAS expects, at which point it will kick the drive out of the array, causing a costly rebuild of the array once you add the drive back into it.
Drive manufacturers will always, and with some reasoning behind it, recommend higher performance scsi/sas drives over sata i/ii drives.  I have four 1TB hitachi drives that were I believe about 100 bucks each in a raid array that I test vm's with esxi off of before putting them in production.  The machine has cranked away non-stop for, well, I can check, 202 days, no faults, no errors, great speed.  Four 73g 15k/rpm SAS drives are around 190 each.  That's 4x73GB (292GB) for ~800 versus 3TB for ~400.