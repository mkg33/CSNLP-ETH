I have not implemented an IB storage solution myself, but the main problem as I understand it is that the host drivers are not in wide use in your average environment. They are in wider use in the Windows world than in the Linux world. Where they are in use in the linux world, it's usually in "packaged" infiniband hardware appliances or supercomputing applications with tuned/customized drivers.
I approached the same problem by using 10-Gigabit iSCSI with a dedicated 6-port switch (HP 6400cl-6XG - $2200) and Intel dual-port CX4 NICs (Intel EXPX9502CX4 - $650). The cost per server came down to the NIC and a $100 CX4 cable. In this case, very little was needed to get drivers, etc. to work in a mixed Linux, Windows and OpenSolaris environment.
So, does anybody have some pointers about how to use IB for shared storage for Linux servers? Is there any initiator/target project out there?  Can I simply use iSCSI over IPoIB?
I'm contemplating the next restructuring of my medium-size storage.  It's currently about 30TB, shared via AoE.  My main options are:
Personally, I like the price/performance of InfiniBand host adapters, and most of the offerings at Supermicro (my preferred hardware brand) have IB as an option.
Do you need IB's latency benefits or are you just looking for some form of combination of networking and storage? if the former then you have no choice, IB is great but can be hard to manage, FC works great and is nice and fast but feels a bit 'old hat' sometimes, iSCSI can be a great solution if you consider all the implications. If I were you I'd go for FC storage over FCoE via Cisco Nexus LAN switches and a converged network adapter.
The difficulty with IB when building a SAN is to manage the srp target. There are very few pre-built solutions available and most are expensive. If products like Open-E introduced native IB support into their software (specifically srp) you would have an easy hardware solution. The client side is very simple to set up on RHEL and it works perfectly. We have a test system up and running now which is performing at 600MB/s consistently and under high load. The performance is amazing and the large amount of available bandwidth gives you great peace of mind and flexibility. Yes you are still limited to the speed of your array, but with IB you can connect multiple arrays without losing performance. Use one for backups, one for main storage etc etc and use them simultaneously without any loss of performance. In my experience, as a pure RDMA storage network, without IP, there is nothing that can beat IB and if you shop around, you can set something up for a very reasonable price. If someone was to introduce some storage appliance software similar to Open-E with full SCST SRP target support it would open up the mainstream market to IB and I for one would be very happy.
Linux has had IPoIB drivers for a while; but I don't know if there's a well-known usage for storage.  Most comments about iSCSI over IB talk about iSER, and how it's not supported by some iSCSI stacks.