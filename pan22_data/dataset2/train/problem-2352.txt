Although the definitions of the two concepts are fuzzy, there is a slight difference between online learning and incremental learning approaches. 
In online learning approach, model is updated to adapt to the new data. It is possible that the model can forget the previously learned inferences which is called as Catastrophic Interference.
Sequential and online learning is mostly associated with Bayesian updating. The sequential learning is used widely for an order in time of the data, meaning that $x_1$ is coming always first, then $x_2$, then $x_3$ and so on. The Dataset each has a certain order in that sense. 
These are quite fuzzy definitions, and in my opinion there is not clear definition though. I still hope that helps.
Concerning online learning, the people mostly referred to a data stream, hence a online learning is always incremental learning but incremental learning does not have to be online. 
In contrast to that incremental may be a whole block of data at time x and another block of data at time y. While the block internally may be randomly ordered. 
Whereas in the incremental approach, even as the model is updated, previous inferences are not forgotten.
So, the following answer is just based on different opinions of collegues and professors from the field. I want to try to summarize it briefly: