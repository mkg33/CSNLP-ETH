If this is not feasible in your environment your second best chances are rsync on the plain SQL dump (don't forget --compress) or on a gzip of the same, which has been compressed with --rsyncable. I don't know how well rsync fares on this, since inserted/deleted values in the dump file will cause a "shift" in the file which rsync needs to detect to prevent the retransfer of data which has not been changed.
Not sure if you're using Windows, but I've had good luck with MyWitch administering MySQL databases on remote servers, and they have a product (that I haven't used) called DumpTimer that will let you schedule mysqldumps and download them.  Shareware, so you'd have to pay for the full version that would let you run as a Windows service.
I am currently experimenting with rsync transferring the differences between the old and new dump files but the compression is much less efficient.
We currently use mysqldump and then bzip2 compress the result and scp it back to our backup server. This is a time consuming manual process and there are no snapshots created.
When you run rsync with --stats it should report how many bytes it actually sent over the network, to give you some figures.
One way to do it would be to set up database replication to a backup server and create the backups there.
You could dump the data in each table to a CSV file, ordered by a primary key (or something). Then use split to split it based on lines. Then use rsync to copy those files across. So if the first 1,000 lines (ie rows) haven't changed that file won't be rsynced. You could leave them unencrypted on the server and client and get rsync to do the compression on the network. That way it'd be quick for it it know "Oh I add 10 lines here", etc.
In fact this should perform better (higher compression ratios) than using a compressing-remote-shell or compressing-transport (see rsync(1)).