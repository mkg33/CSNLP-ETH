The authoritative data is in many cases used into materialized views with variable time-to-live, so when you restore automatically you risk breakage on undefined places in your data pipeline.
Now i want to find a shell script to check if all the databases backup is up to date or not(for an example: i have twenty database in the main server so i want to know all of them have been backed up completely or not) and compare the each database's current size with their previous size, in a sense i want to make sure with through this script that my backup is perfect and if not i will have a track of that occurrence.  
The way I would approach this is by setting realistic thresholds and alerts, for example start with: "the data will never be 40% smaller than yesterday" or "there will never be less than 1_000_000 products" and start from there. It is key to make sure the false positives are reduced as much as possible.
You should never perform automatic restore, always alert and escalate, otherwise you risk getting subtle corruption of your whole system.
This is quite difficult problem, you can get quite close by checking the backup size difference and use various anomaly detection algorithms to decide if there is something wrong or not.
For example, insert heartbeat like record, before you do the backup, then check if the backup contains it (which you can check by re-applying the backup on empty database and actually looking at the number of rows after the heartbeat)
I wrote a blogpost investigating backups as a graceful degradation strategy, that discusses similar subject, not sure if it will be helpful to you, but I think it might be.
The problem is that for example a simple drop column will create an anomaly, and then you will start getting a lot of false alerts, which will make you start ignoring the alerts, and when the broken backups actually appear they will be ignored :)
I have mysql production databases running in main server(Linux) and from this server every night the backup is taken into local server. 