Previous answers show how one gets more instructions executed by the processor's definition of "instruction" and one imagines that is actually the questioner's intent.
But another source of it may be that each "instruction" is actually a certain amount of data treated as an instruction input by the processor. If his source's counting just counts what the processor regards as instructions, the following adds nothing. But if his source counts all of what a human would call an "instruction," then:   Add in that not every instruction is as physically long as every other instruction (one might be 12 bytes, another might be 56 bytes, etc.). So if it loads 64 bytes of material each cycle as "an instruction" (or as many full instructions as it can before hitting 64 bytes)and one has six instructions in that 64 bytes, then six instructions (as you and I might regard them) will be finished in that cycle.
Since many very basic instructions (our "sensible" definition) are leftovers from early days with 8 byte instruction lengths, and very basic instructions are, by definition, perhaps used disproportionately, just this would go a long way to having more "instructions" performed than frequency would seem to allow.