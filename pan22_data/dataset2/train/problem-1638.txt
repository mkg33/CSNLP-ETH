Yes, it's a little hacky, but it may give you good results. You can re-normalise alt_y_proba to be a proper probability distribution again if you want, but it won't change the classification. Just make sure that the dataset you're optimising this subtraction value with is not used in the training of the neural net, as this may introduce overfitting. 
Is your input data standardised? What happens when you run it with only one hidden layer? What happens when you set learning rate to 1e-6 or 1e-5? What's your result when you run this through a logistic regression? What does the confusion matrix look like? 
I think it is probably doing something, just not enough to change the overall classification. Have you inspected the estimated probability distributions of the minority class test examples for different class_weights? I imagine that the probabilities of the true classes are somewhat higher for these examples, even though the predicted most likely class is still the majority class. 
1) A five-layer neural network is one heck of a complex model for a data set with less than 1 million points. (I’m trying to find a good link for this, but the intuition is that your choice of model should be driven by the complexity of the available data, and not by what you think the real target function is like.) If this is for a real-world project, a tool like XGBoost might work better on this data set — out of the box, you’ll spend less time dealing with problems related to imbalanced classes, poorly scaled data, or outliers. Of course if this is specifically for learning about neural networks, that advice isn’t much help!
The intuition here is that, for neural networks, as far as I know, re-weighting classes basically means re-scaling the gradient steps during training based on the class weights. If the classes are skewed 10:1, that makes sense: we take a step that’s 10 times as far for a minority sample. If the classes are skewed 1000:1, as in your case, it makes less sense — we’ll take 1,000 small steps as we optimize on the majority class, and then a single gigantic step in an essentially random direction when we happen to see a minority sample, followed by 1,000 tiny steps trying to un-do this work, etc. We don’t see enough minority samples to allow information about their class to average out.
If this is the case, then you can determine a value (through cross-validation or estimated on a held-out set) to subtract from the element of the estimated probability distribution corresponding to the top class before taking an argmax. i.e. test a range of values and pick the one that provides the best $F_1$ score or whatever else you want to optimise. Something like this:
2) For a class distribution that’s as skewed as your data, you might get more mileage from re-sampling the training data rather than re-weighting the classes during training. Down-sample the majority class first (just throw away majority samples at random); if that’s not satisfactory then try something more complicated like using SMOTE to up-sample the minority classes. Try taking this to the extreme; build a (collection of) new training sets by randomly sampling only 1,000 points from each class.