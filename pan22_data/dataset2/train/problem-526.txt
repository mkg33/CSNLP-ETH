Personally, if the metrics will just be used by you to evaluate the model, I would use the sensitivity and specificity within each class to evaluate the model, in which case, I care less about the balance of the classes in the test data as long as I have enough of both to be representative. I can account for the prior probabilities of the classes to evaluate the performance of the model.
And really, it depends on the audience for, and interpretability of the model metrics, which is the thrust of your second question:
In other words, if train my model with 1000 random samples of "0" class, and 1000 random samples of "1" class, should I test the model with the remaining 3519 samples of "1" class, and randomly select another 3519 samples of the majority "0" class, or I can go with the remaining 17921? 
On the other hand, if the metrics will be used to describe predictive power to a non-technical audience, say upper management, I would want to be able to discuss the overall accuracy, for which, I would want a reasonably balanced test set.
A fast solution: If you use SKLearn's Random Forest, I highly suggest "class_weight" => “balanced”. 
I have a dataset with 4519 samples labeled as "1", and 18921 samples labeled as "0" in a binary classification exercise. I am well aware that during the training phase of a classification algorithm (in this case, a Random Forest) the number of 0/1 samples should be balanced to prevent biasing the algorithm towards the majority class.
Recall becomes important when you are let's say identifying fraudulent transactions out of several millions. You will have very few classes labelled as 1 (fraudulent). So even you simply label all the classes in your test data as 0 you will be more than 99% accurate but you should ask yourself the question "Out of all the actual classes with label 1 how many did i get right from the trained model". That is what recall gives you.
What is the impact of an imbalanced test dataset on the precision, recall, and overall accuracy metrics?
That said, it sounds like your test set is drawn independently of the training data.  If you are going to balance the training data set, why not draw one balanced data set from the raw data and then split the training and test data?  This will give you very similar class populations in both data sets without necessarily having to do any extra work.
Once you have balanced your classes, for a start you take 80% of it as train and test it on remaining 20%. In your case 4519 samples are 1 and 18921 are 0 so lets say you upsample 4519 and now you have 15,000 samples of class 1. You will take 80% of (15000+18921) and train your model. Test it on remaining 20%.
There are several ways of balancing classes. Either you can increase the number of samples of minority class or decrease the number of samples of majority class.