My manual file mirror, versions, and deletions had bloated 350GB (300 noted above) in 3 months where ArecaBackup only bloated 20GB over the same time frame.
I have been using Areca Backup. and it seems to keep track of duplicate files well.  I recently moved 300GB one day and it only backup up 8GB actual data meaning it did not recopy the files and just referenced them
The biggest danger is if your configuration files get corrupted or go missing you cannot rebuild them; but you still would have your files.
If you use backup software that uses Single-instance storage such as Windows Home Server or Danz Retrocspect (now EMC Retrospect) it shouldn't matter f you move your files around or even duplicates them, since the backup software will identify identical files and only store one copy of them.
I appreciate it's ability to backup the files in such a way that you can access the actual file in a directory tree on the backend; that is files can be stored in a directory tree in their original format instead of a proprietary format to fight possibility of corruption of a backup.  Although typically you would browse through the GUI.
Was just researching the same solution, I ran across Acronis Backup which appears to have plugin to do this but haven't yet had a chance to test this out. 
How about using rsync for this (or a Windows equivalent)? If you use the --delete option, it will automagically remove files that are missing due to renames/deletes.
BackupPC pools identical files using hardlinks. By ``identical files'' we mean files with identical contents, not necessary the same permissions, ownership or modification time. Two files might have different permissions, ownership, or modification time but will still be pooled whenever the contents are identical. This is possible since BackupPC stores the file meta-data (permissions, ownership, and modification time) separately from the file contents.
I have not found a way to delete a file from the archive, so if there is a large file I want gone I delete it in the directory structure and if someone tried to recover it for some reason I'm sure it would throw an error when recovering it.  Although I only do this for known large temporary or duplicate files so this has not been an issue; namely from users using backed-up locations as a scratch space and the scratch work ends up getting backed up.
I know this is old, but extremely relevant as duplicating 100's of GB's to Tb's of uneeded files often is problematic for anyone budget conscious.