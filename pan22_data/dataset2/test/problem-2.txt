4> maybe load some of the "where in (" data into a temp table as a seperate query before the main query or create a string if there arnt may values and dynamically create the query (so rather than "... where in (select .. " use "... where in ( 1,2,3...)").
2> Choose indexes so that the data that the query has to search through is optimised. Maybe change the idex type (if the engine has different types)
Starting off with the read: Your fact table is partitioned, yet you are not using partition elimination in the not exists query. Work that PartitionID into the where clause of the not exists subquery and you are bound to see an improved plan on 13 billion records.
An INSERT..SELECT can easily be split up and tuned in 2 parts. The read (SELECT) and the write (INSERT).
As others have pointed out look at the query plan and see which parts take the longest time - they will be probably be joins and scans (DISTINCT clause) and they to optimise these by making sure there are indexes to assist and maybe restructuring the data.
Additionally, correct use of an ssis data flow task can allow you to obtain a bulk update lock, which is better than an exclusive (which you'd get from adding a table lock on your current query) because not only are the reads multithreaded but, so are the writes.
The performance drop is probably becase in the 1 day of data, the database engine may be doing a sort 1000* on 1000 pices of data, but the month is sorting 1000* for 1000 pices of data/day * 30 days (where 1000 * 1000 << 1000 * 1000 * 30 ). It may be faster to run the query 30 times for 30 days of data rather than once for 30 days of data beacuse the amount of data that the query has to loop through is less.
By using a plan guide you may be able to force the query to use the one plan for all occasions (after testing of course).
What are your indexes? Are they also partitioned? There are alot of unknowns here, but here are what I see as room for improvement:
3> change the order that the joins take place, may change the exection order that the planner chosses.
The write: Is there any concern for concurrency on the fact table when you are doing the insert? If so, can you move the insert to an off peak time? Additionally, setting lock escalation to auto can allow partition level (HOBT) locking, allowing a minimally logged bulk insert to 1 partition while the rest are free to be read. I've used this in the DW with much success. If no concurrency concerns, look into what you can do to minimize logging (simple or bulk recovery, trace flag 610 if not a heap, and throw a table lock on the insert). Logging is a common bottleneck. Data can be lazy written but the log can't. Logging is great for OLTP, but this is a fact table. Fully logging fact table inserts = slow.
You should have an index on  [FACT].[DataMine].PartitionID, but it may make a difference to drop that before the insert and then re-create it, as sometimes this is faster. Possibly try a bulk insert as well. You would need to load the results of this query into a temp table first though, otherwise the sub queies would have no index on this field when they are selecting, which would make the situation worse.