I work in python and R (as do most practitioners I know personally), but find that python is easier to deploy in a production environment. Much of the work relies on libraries these days, so it useful to know the language's library landscape (when it comes to project timelines, familiarity with your tools reduces the pressure on yourself and your team immensely). It is quite common that people experiment in jupyter/ipython notebooks and make their code available online (e.g. KDNuggets post). We often use notebooks at work and commit them to the code-base to provide the empirical backing for the solution. I would recommend you find some cool notebooks that interface with a database, and see if you can run their code (it is python typically). At least this way you will start to get a feel for more of the grunt work associated with the tools (considering that you already have the theoretical background).
If you find learning SQL and Hadoop unbearably boring, you should not be looking for a data scientist job. Anyhow, feel free to skip Hadoop. There are lots of deployments of Hadoop, but they are being phased out with more modern tech, for example Spark, which is also what most companies use for new deployments. I hope you find learning Spark a bit more bearable. The Edx online courses on ML with Spark are actually very nice.
A large component of data science work in industry is data wrangling. It is quite important to have some basic understanding of data storage systems as you will often have to extract the data you need yourself (unless you work for a large company). Hadoop may not be a necessary skill, but knowing something of relational data stores (SQL) and object storage (No-SQL) will be very useful. A person often needs to be able to process data quickly too, so you will need to know something of optimisations such as indexing.
I would focus on Python or on R, not on both. A lot of employers are Python and R and allow you to choose yourself as long as it's on one or both. I feel like Python is growing faster in the Data Science community than R and is in my opinion a much more sophisticated language and has a wider support for things that help with data science but are not directly related.
Getting and cleaning data is a central part of a data scientist's job. My team (data science at a mid-to-large internet company) regularly deals with data wrangling at scale -- combining terabytes of browsing history with a multitude of other data sources, structured and unstructured, to investigate problems. SQL and Spark/Hive/Hadoop are nearly daily parts of our workflow. We filter new candidates heavily on their ability to use SQL intelligently and favor those who have worked with big data technologies in production environments. 
I believe technology is cheap and science is expensive. You can learn R, Python, SQL and Hadoop pretty fast (considering that you know programming) but learning statistics, machine learning and the methodology of working with data is difficult and takes time. (which you know based on your background)
On a side note, distributed processing/storage is a fascinating research area and the mechanics behind databases are pretty cool. Perhaps you can make it a bit more interesting for yourself by not just focusing on the semantics of SQL, but also some under-the-hood bits, like multi-pass algorithms for joins and so forth. You could read the Amazon DynamoDB paper or Google's BigTable paper for an intro to big, distributed databases. 
In my eyes, go and apply for jobs with self-confidence. In the meanwhile, consider learning SQL and Python. They are necessary for jobs in industry.
The best thing to prepare in my opinion is just by doing some project(s) and figuring things out along the way. Join a Kaggle competition or find some personal project. I spend hours and hours working on those and I keep coming across problems that I haven't faced before, requiring me to read papers, implementing some new ideas, trying things out. This also allows you to build up a github portfolio to show off some cool projects you have been working on.
Our team is python heavy, though we are using Spark more and more. It's got a great ecosystem for working with big data and has great machine-learning support via scikit-learn and the like. We often prototype in jupyter notebooks and scale solutions by building python apis that run our jobs or handle incoming requests. Other data science teams here use R. Pick one, do a few projects in it, and do a few projects in the other. You'll figure out which syntax and style works best for you. 
Hadoop and SQL are things that you will pick up reasonably quickly and in my experience are far from necessary for a lot of jobs. 
As jab already mentioned, I think soft skills are very important, although difficult to learn without real working experience.