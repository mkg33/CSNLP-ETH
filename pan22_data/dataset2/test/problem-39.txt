The most popular quantile is the median, or the 50th percentile, and in this case the quantile loss is simply the sum of absolute errors.
In this case, I think I put parameters the number of linear functions and criterion to judge if a point is on a line or not.. 
Just as regressions minimize the squared-error loss function to predict a single point estimate, quantile regressions minimize the quantile loss in predicting a certain quantile.
Since running K-Means, or GMM might not give you cluster centers corresponding to the placement of the line it does not seem to be the right thing to do. 
and finds 3 lines(as the second figure (the lines in the figure are just put for idea without computation)), and then finally suggests points which may belong to neither of them. (In this case, for example, (71.6, 22))
Other quantiles could give endpoints of a prediction interval; for example a middle-80-percent range is defined by the 10th and 90th percentiles. The quantile loss differs depending on the evaluated quantile, such that more negative errors are penalized more for higher quantiles and more positive errors are penalized more for lower quantiles.
From here you simply have to calculate the distances to the lines, which will be a cluster center, or point in the 1D. 