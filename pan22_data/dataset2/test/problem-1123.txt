One of our customers hit a pretty uncommon XFS filesystem bug on december 24 2005... Well at the time I didn't know it was an linux kernel bug of course, I thought it to be just some  of the usual suspects ( 13TB RAID with 8KB free, spurious drive failure in the array, etc).
So far so bad... however, customers had some difficulty getting through to the facility to bring the downtime to the provider's attention.. the provider only had VoIP phones, which were connected through... well, you can guess.
This was all put in context one day, when I received a call from one of the world's largest card providers (think Amex, Mastercard, Visa, Diners... in fact it was one of those brands, I don't know if they would appreciate me mentioning it). I was front-line support, my only job was to assess the scenario, rate it, and put it through to the appropriate support division. This case was the only Priority One case I ever put through.
In re-installing a laptop's operating system for a manager, someone made a copy of all it's data over the network to a linux station in /tmp. There were some problems and it took more than one day.
Finally as the filesystem was unmountable, I asked the operator on the line to enter xfs_repair -n /dev/whatever. Hmm, it wants to clear the log (obviously, as the FS isn't mountable), but no too ominous message. So go for it : xfs_repair /dev/whatever.
When I worked for Cisco, I used to get customers who had bought $30 wireless cards and who were spitting chips when their driver wouldn't install, or people with the cheapest most basic router Cisco had who would rant and rave over support issues.
Fortunately, the second time that you do the same thing you do it better and faster, since that you already know how to do it. Now the website is live. And I have backups :=)
A man from the card company called up and stated that their link between their east-and-west-coast US mainframes was down. If an account was created on one mainframe, the transaction was always processed on that mainframe. Which was fine if your closest link was always near to that mainframe. But on this particular day, if you had an account on the east-coast server, but you were in the west coast, the transaction would be denied because the link was down.
They took down their primary network link to the internet to perform some software maintenance on the router, fair enough.
Really puts it into context next time you feel tempted to rant and rave to customer support over you $30 wireless card.
However, at the same time, the upstream provider of the secondary link switched it off to perform some testing (apparently they had been told, but it had been mislabelled in the datacentre)
Standard question when assessing damage was "How much is this costing your business?" The reply, calm and collected, was "About a million dollars every 30 seconds".
I'm not sure that this could be an interesting answer, but I'm also a coder. I coded my last website completely on a production evoirement, with no backups at all on my pc. A bad day after 16 hours of continuos work, I had to empthy a partition, and the fastest way to do it was to format it. I runned fdisk -l to check what was the name of the partition I had to format, and unfortunately I readed the wrong line, and formatted it.
Hu oh... Turns out that to add insult to injury, the xfsprogs were of some version that would do severe harm in this exact case... Ouch. 8TB of data were gone for real.