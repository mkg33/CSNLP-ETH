On the other hand if you have service on multiple systems and those systems are going to be backends behind some load balancer, given the assumption that files you need to have shared are related exclusively to visitors of given backend, you could try with stickiness so every returning visitor always hits same backend.
If Linux, I would suggest using NFS filesystems/mounts across the nodes that require access to the files. Have you considered this?
If your instances do concurrent write access to the files in question, consider using a distributed file system capable of propagating locks instead. It would not need scheduling and you would not need to worry about update conflicts (i.e. a situation where a file has been updated on both instances before synchronization).
Using single file system is bad singe you are introducing single point of failure in your system. Making good architecture design from start is key to scaling your application. Depending on local disk storage is never good option. If you are writing system that will be working across number of nodes you should consider using some TCP service which any node can access over network. 
If however none of these work for you, cross syncing local files to multiple servers will be hard problem to resolve so as ewwhite suggested, NFS file storage looks like only sane option then.