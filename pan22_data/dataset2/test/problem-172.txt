Not ideal, but I can pick up a lot about students' code by walking around the room and watching them code. 
Students could run the test code as many times as they wanted. Then, they'd submit their solutions and I'd run it against the other test code. 
Yikes, that doesn't sound like a fun way to grade. Sounds about like what I do with students that are competing in ACSL competitions. But that's only 6 or 7 students, 4 times a year. I can't imagine trying it with 100 students all year. 
Please, take a look at Stepik. Actually I myself work at Stepik, but if you're looking for a free cloud-based solution, it might be really useful for you. Code challenges are tested using stdin->stdout and you can specify any test cases that will be used to check student's submission. 
I do this a lot with projects to get a feel for where the kids are on the project. By the time it's turned in I've already got a pretty good idea on what they've done.
For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. 
I have seen universities use linters and test suites to automatically check the correct state of the assignment so it can be submitted. Here is an example in Python or the student assignment helper programs in C, used for the book CS:App as another example of possible small programs you could include with assignments to make this easier to review submissions.
Only saving grace on this one is that labs are worth very little and tests make up a majority of their averages. So even with gimmie grades on labs, they can still get hammered on the tests.
Sometimes I fall into this trap, especially with my second and third year students. I give them labs, they work on their labs, and I assume that they're done correctly if they've been working in class. 
Skim the code and see if you think it works. Same as the participation, this only really works with smaller assignments and when labs aren't worth all that much. 
A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3.
I can then go back and download the submissions if I want to look at them for style or tips. Generally I try and do this with everybody on at least one lab per lab set. 
Biggest downside is it was pretty time consuming to write unit tests for every lab. I've gotten pretty quick, but it's still a time suck.
What I've found is that students average about 11 submissions before they move on to the next lab. I only grade the last one, although I'll go back and look to see how they progressed. 
It seems to me that the mistake is using C++ for an introductory course. Complete beginners do not need help shooting themselves in the foot. The curriculum may be out of your control, but if you have the option then teaching the introductory course either in a strongly typed LISP (SML, Haskell, or something similar) or at least in a concise portable language (Python, Ruby, etc.) should make it easier for both the students and the graders.
Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly):
Downside to this is that it works well for smaller labs, but not as much with larger projects. I can break down large projects into pieces that they can check, but eventually there has to be a full project turned in. 
This is what I'm doing now. Kids login to Canvas and it launches an LTI tool embedded in an assignment. They do their coding in Chrome and click the Test button when they're ready. They can keep testing until their happy with the grade. Grades get sent back to Canvas. I still have to transfer them to our actual gradebook. 
Before I moved online I would write a JUnit test for every lab, and usually I'd write two. One that I would include with the starter code and one, more in depth, that I would use to test. 
Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly.