Any large queries (with or without an index) start to lock up the database after the red circle.  You can see the aggregate 5 minute CPU usage when I locked up the database for almost a minute performing a DELETE.
Both of these solutions work, and the bonus with (1) is that I don't have to spend any extra money.  I'm working on tuning the value of innodb_buffer_pool_size so that it is as large as possible without causing swap.  I ran the same DELETE query in 1.2s and the database continued responding.  The below screenshot wasn't even possible with the production database since the database would stop responding during these long queries, so the dashboard would never update and eventually lose connection.
It was the swap!  I ended up replicating the database on to the same hardware, and wrote some scripts to emulate live traffic on the database.  I also ran some big queries to help fill up the buffer pool and then ensured that my replica database approximately matched the metrics of my production database.  I then tried running large queries against it and it locked up, even with indexes applied.  I could reproduce the issue without taking down the production service, so now I can break things as much as I want.
1) I changed the value of innodb_buffer_pool_size to 375M (instead of its default AWS value of 3/4 the instance RAM size).  This reduces the maximum size of the buffer pool and ensures that the database memory footprint won't grow large enough to push the OS/etc into swap.  This worked!
I noticed that running the same large queries earlier in the life of the replica database hadn't caused an issue, and tracked down the point where the locks ups begin.  It happens almost immediately after the buffer pool gets large enough to push some data (OS, or otherwise) to swap on the t2.micro instance.  Here's an image from Cloudwatch of the swap growing after the freeable memory drops below ~50MB or so: