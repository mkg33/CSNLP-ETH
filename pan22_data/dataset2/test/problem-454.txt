Probably the most important thing you could do, however, is to start using pathlib. Don't bother with os.path for this, go directly to the good stuff! The pathlib module is part of the standard library, and does many things that "just work." Including, of course, overloading the operator / to do concatenation:
could be better handled by using command line input like in argparse. And for filename you could even use os.listdir to read all files from a directory. Both would give you more flexibility in using this script.
First, you are storing Nan and 'Y' values in your columns: floating point and string data? The Nan means false and the 'Y' means true. But there's already a True/False data type, and it is neither floating point nor string. Why not make your table a table of boolean values?
It seems easy to write something like filenames = [ 'Q1.txt', 'Q2.txt' , 'Q3.txt' , 'Q4.txt'] and it is, until your boss says, "Hey, could you add the last two calendar years to that report?" and you're suddenly stuck trying to move files around and change your code and change the field names in your dataframe and change your HTML and change your spreadsheets and ...
Third, you are creating one-row dataframes to append values, and then resetting the index of your dataframe. You can just store to a new index!  df[new_name] = new_info
Second, you are treating your names as a column. But they are really the index! So why not make them the index? If you do so, you can use name in df and not have to maintain a separate Python set object.
But you're really either updating an existing record, or accumulating new records. Why not capture all the new records into a python data structure and do one single dataframe update at the end? 
Writing code like this lets you make one change - renaming Q[1-4].txt to 2019Q[1-4].txt - and you still get the list of paths you need. 
You wrote a lot of code to handle the management of names in your dataframe. But you overlooked the fact that Python has data structures other than dataframes that are easier to work with.
Following is the program, that I've written, and wanted to hear your thoughts, on the style/efficiency and tips to improve the same
First of all, you claim that names only appear once in any given quarter's data, if they appear at all. So your logic is about checking to see if a name appears in the dataframe because of some previous quarter.
But the fact is that Python file operations will honor windows paths spelled using forward slashes (/) instead of backslashes (\). So you could just as easily define that like this:
There's an example right in the Python documentation for Reading and Writing Files that shows how to use with for file I/O. It's built-in RAII support, if that means anything to you, and it's absolutely the correct way to do file I/O in most cases. Certainly in this one. Plus if you don't use it, every suggestion you get from anyone that (1) reads your code; and (2) knows how to code in Python is going to include "use with for file I/O!". So let's make this quick change:
An even better question might be to ask if you need to use the dataframe for this at all? You are building what amounts to a boolean table. You could store the boolean values in a data structure built from collections.defaultdict and only convert it into a dataframe for the purpose of generating the HTML at the end.
I've written a python program to rank the names that appear in the file(s) based on their frequency. In other words, there are multiple files and want to rank the frequency of the names that appears the most first, then the least at the end. Each name can appear only once in each file. In other words, the same name cannot be repeated in the same file.
What's the difference? The difference is that after the with version, the infile is closed. Why does it matter? Two reasons: First, with handles success, failure, exceptions, and always makes sure to close the file. Second, file handles are a surprisingly limited resource, especially if you're debugging code in a REPL. Using with is a good way to never run out.
Use the features of pandas as they were intended, and your code will get simpler, shorter, and faster.
Also, the variable eachname in for eachname in allNames: should be renamed in something like name, to prevent lines like emptyDataFrame = emptyDataFrame.append([{'Name':eachname}]) from looking weird.
This is a good use for Python's set data type. You can test for containment and compute intersections and differences quite easily.
It's better if you have only one source of truth for things like this. In this case, let's use the filenames as the source of truth. Then we can use glob to match them:
I suggest using os.path.join instead of + to handle dangling slashes, when concatenating file names.
That is called a set comprehension, and it works similar to a list comprehension. (And yes, it rocks!) Sets are O(1) for lookup, so you can just use the in operator.
Finally, the df.to_html() function takes a large number of parameters. Including formatting parameters that you can use to translate boolean values into 'X' or 'Y'. 
Python has raw strings which make it much easier to use backslashes. The documentation is a bit scattered, but the upshot is that a raw string looks like r'foo' and backslashes don't need to be escaped. This makes raw strings the format of choice for writing Windows paths and regular expressions.
Pandas is great at dealing with tables of data of known, basic types. It supports indexing, lookup, boolean, and database type operations. Try to see how you could use that.