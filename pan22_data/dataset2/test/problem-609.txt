The way full scan is done is that for each folder there's one run of the function so JVM will exit the function, deallocate RAM and run it again on another folder.
I've been having the same issue. Trying to store millions of files in a Ubuntu server in ext4. Ended running my own benchmarks. Found out that flat directory performs way better while being way simpler to use:
The best way is to arrange the folder structure the way that each file is in dedicated folder e.g. year/month/day.
I applaud you on your short hash; previous systems I've worked on have taken the sha1sum of the given file and spliced directories based on that string, a much harder problem.
Both of your proposed options creates up to three inode entries for each created file. Also, 732 files will create an inode that is still less than the usual 16KB. To me, this means either option will perform the same.
Certainly either option will help reduce the number of files in a directory to something that seems reasonable, for xfs or ext4 or whatever file system. It is not obvious which is better, would have to test to tell.
Benchmark with your application simulating something like the real workload is ideal. Otherwise, come up with something that simulates many small files specifically. Speaking of that, here's an open source one called smallfile. Its documentation references some other tools.
In my experience, one of the scaling factors is the size of the inodes given a hash-name partitioning strategy. 
It will have to allocate large amount of memory and deallocate it in short period of time which is very heavy for the JVM.
hdparm doing sustained I/O isn't as useful. It won't show the many small I/Os or giant directory entries associated with very many files.