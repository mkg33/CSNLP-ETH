Your final snippet as was already pointed out has a problem that it reads the entirety of each file into memory (that's what f.read() does). This won't work for large files.
I suspect this will behave slightly better than your approach, but it's still not optimal. Your second script is entirely unnecessary because I did output.write(line) instead of output.write(line + '\n'). But, if you did need to do some processing on each line, why do you use an intermediate file? Why not just do it in your loop above? For example:
Windows doesn't support sendfile though, so you'd likely want to use this and fall back to copying the file in chunks when not supported. When you do this make sure to open in binary mode ('rb' and 'wb') as this matters on windows.
However, there is something better if you are on a *nix. Python provides os.sendfile() which uses sendfile. This has the advantage that you don't have to do any copying yourself. All of the copying is done by the OS (which is much faster for many reasons, but most of them boiling down to context switching). You can use it like so:
The solution that @rzzzwilson proposes is definitely the correct one. Buffered writing by using chunks (I'd choose a size that's in the kilobytes, perhaps the megabytes, but nothing too crazy) allows everything to fit into memory and composes nicely with some of the other buffering, cacheing, and prefetching already done by the userland IO library, operating system, and even harddisk itself.
@rzzzwilson has the right idea with what you should be doing, but let's break down your code to address some of the other performance and style issues: