I think the best way to get intuition for this issue is to think of what the complete problems for exponential time classes are.  For example, the complete problems for NE are the standard NP-complete problems on succinctly describable inputs, e.g., given a circuit that describes the adjacency matrix of a graph, is the graph 3-colorable?  Then the problem of
Now we know that $L' \in CLASS_1[g(n)]$ and hence  $L' \in CLASS_2[h(n)]$ (decided by some algorithm $B'$). We would like to get from here to $L \in CLASS_2[h(f(n))]$. But that is straightforward - algorithm $B$ deciding $L$ just pads the input accordingly and runs $B'$ on the padded input. 
succinctly describable inputs, e.g., those with small effective Kolmogorov complexity.   This is obviously no stronger than whether they are solvable on all inputs.  The larger the time bound, the smaller the Kolmogorov complexity of the relevant inputs, so collapses for larger time bounds are in effect algorithms that work on smaller subsets of inputs.
Of course there are some technical detailes involved here (f.e. we have to make sure that the padding can be implemented in the classes we consider) but I just ignore them to give the general intuition.
I see the padding arguments in terms of compactness of representation.  Think of two translator Turing machines: $B$ blows up instances, and $C$ compresses them again.
What we do is: we take a language from the bigger class and we pad it, so that it can be solved by a weaker algorithm giving us containment in the smaller class - the weaker algorithm can do it, because it has the same amount of 'real work' to do as before, but it has its restrictions (being a function of the input length) lifted by extending the input.
OK, so your goal is to show that $CLASS_1[g(f(n))] = CLASS_2[h(f(n))]$ basing on $CLASS_1[g(n)] = CLASS_2[h(n)]$ (we don't specify what exactly are this classes, we just know that they're somehow parametrized with the input size). We have a language $L \in CLASS_1[g(f(n))]$, decided by some algorithm $A$. Now we make a language $L'$ by padding each word in $x \in L$, so that it's length is now $f(n)$, and we see that it is contained in $CLASS_1[g(n)]$ (our new algorithm $A'$ basically just ignores the added zeroes and runs $A$ on the real, short input). 
It isn't possible to apply the idea the other way, using $C$, because only some of the languages in the easy class are generated by blowing up languages in the hard class.
The padding argument works with $B$, by composing $B$ with the deterministic version of the TM for the language in the lower nondeterministic class.  The outputs of $B$ collectively form a language which is not compactly represented, so this becomes "easier".
This step may be summarized as follows: we want to decide $L$ in the bigger, more resourceful class. Using our extra resources we pad the input and run the algorithm deciding the padded language.