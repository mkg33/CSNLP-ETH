Your method looks interesting. I will borrow it when I have the chance. Let me make a couple of suggestions ("suggestions", not "prescriptions"):  
In this very short example the third one is probably better, but with more data using more features will lead to better results.
Remember that there are no "best predictions". There are only "reasonable prediction methods", and "useful predictions." My suggestion of using some average as baline for the index is borne in from the fact that averages are more stable than single observations.    
I have a dataset which tracks the prices of 21 products, charged by 24 companies, in 150 different cities across the globe. However, the data set has missing values--that is, I might have Company X's price for Product A in London, but not New York. I am looking for guidance on how to impute these values given the data that I already have.
The method I have tried so far is to estimate coefficients for each category. For example, holding Company and Product constant, what is the average price multiple of Market B over Market A? Market C over Market A? Then, if I know I have a price for (Company X, product "Good") in markets B and C, I'll multiply them by those average multiples in order to obtain a "best guess based on market". Then I repeat that holding constant Company and Market, and Market and Product. At the end, I'm left with three "best guesses". Then the issue is converting my three best guesses into a single guess, because, based on some testing of this method, the true price may lie either between or outside these three best guesses. This is the histogram of errors based on a training sample:
I would train multiple machine learning models based on information that is available. First of all I would standardize the data per product, so that the average is 0. That way you can better compare different price categories. I would add the mean price known to your set however, standardized over your whole training data, because I can imagine policies being different for different price groups. Now you can train different models for different products. The more features you use, the less data you have so this is a trade-off that you have to test. Here is an example of what I'm talking about (non-standardized):
To compare imputation models you could per price remove a number of known entities, train on the rest and compare the output to the known price. You can use a crossvalidation scheme to get all the prices in there once if the process is fully automated. I would do this at the standardized prices so it's about the relative mistakes and not absolute.
I describe the method that I have tried so far below, though I feel it is overly simplistic. I am looking for suggestions as to how to better capture the complexity of my data and obtain a reasonable accurate imputation for the missing values. Any advice is appreciated.