Sounds like you got a poltergeist in your machine! :) Bad jokes aside - are you connected via VGA (not DVI, or HDMI, or DisplayPort)? If yes, it sometimes take a monitor to receive an analogue signal, wake-up the Analogue-to-Digital converter, and then translate the signal so it can be displayed on its digital LCD display (I am assuming you are using LCD).
If you are INDEED using the VGA cable, that might explain why the monitor doesn't show anything at the start.
With regards to your second problem of it not showing when Windows has loaded - it could be you have set a resolution or a screen refresh rate that is not supported by your monitor. Thus when you plug it out and plug it in, Windows redetect the monitor and automatically adjust the settings (thus making it appear on the monitor again).
Make sure you adjust the settings under Display Properties to correspond with what the monitor can achieve. In fact, set it to safe settings first (1024x768, 60Hz refresh rate), and test. Even better, reboot in Safe Mode (F8) to ensure it works, then change the settings in Safe Mode.