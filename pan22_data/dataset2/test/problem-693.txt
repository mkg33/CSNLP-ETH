Let me contact him to see what is the exact solution he got, maybe this answer can point to the correct direction in the meanwhile.
If you have a single process which has allocated a huge amount of memory on a NUMA system, it may be that it's only letting it have memory belonging to one NUMA node or something.
However, telling your database to use 32G of 32G seems optimistic, as there will be significant overhead. In particular, on x86_64 the page tables will take up a lot of space (maybe .5 G?) and other parts of the system will need some space. I'd think that about 75% was a better setting.
I have a Sybase server with CentOS 5 which causes no swap. The trick is to set swappiness to 0 and to left enough memory for OS. The server has 16GB (10GB for Sybase and 6GB for OS and other services). Start with a smaller Sybase cache and increase it progressively.
A friend had this exact problem.  I'm assuming you have a multicore system.  Being that the case, Linux divides the total physical RAM per processor (ie. if you have a 4core and 16 gigs of ram, it reserves 4gigs per core).  When all available RAM is used up for a particular processor, it proceeds to use swap space, instead of taking free RAM from the other processors.
You're probably running software that maintains big caches in process memory. Such pages will be swapped out when another memory-hungry task is run. The memory will not be swapped back until necessary, so when the other memory-hungry task is done, there will be a lot of free memory that won't be filled until necessary.