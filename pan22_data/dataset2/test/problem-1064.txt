Now, for the finessing.  If you have some common-sense insights into what the distribution looks like, e.g. you think somewhere around 25% of the latencies are less than 0.1 ms, you can add these intuitions to your data.  If you don't have any idea what the distribution should look like, then you can just roll with what you have.
From here, you have two choices: either (a) linearly interpolate between the points you have, or (b) fit a functional form, such as a beta distribution, to your data.  (a) is simpler because it requires no regression; however, it will not give you a picture that is more fine-grained than what you already have, and the calculation of frequencies requires a bit of Excel formula kung fu. (b) will provide you a fine-grained picture that more than likely resembles the underlying data more closely than the linear interpolation, and it only requires straightforward, simple Excel formulas to get frequencies for any bucket or percentile; however, it requires a regression, which requires the Solver add-in.  I prefer option (b) because it gives you the most bang for your buck (i.e., effort).
I think your best bet is to interpolate a cumulative distribution function from the data you have. This may take some serious finessing and hand-waving, especially if the sample data you provided is close to the distribution you're working with. However, it sure beats the hell out of creating (and storing!) a million+ fake data points from a distribution you're just guessing at in the first place.
To get the CDF, you'll need to calculate the cumulative probability for each bucket. I assume there's always some latency, so take 0 as your minimum value with a frequency of 0. To find the CDF value at each bucket upper bound, use the formula 
I'm about 1.5 years late to the party, but I thought I'd post just for anyone else who stumbles upon this.