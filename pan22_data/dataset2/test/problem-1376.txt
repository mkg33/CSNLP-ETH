Regarding an HTTP cache, I think you'd be better served with a decent array of spinning disks, and plenty of ram. Frequently accessed objects will stay in memory cache - either in squid's internal cache, or in your OS's fs cache. Simply giving a machine more ram can significantly reduce the disk loading due to this. If you're running a large squid cache you'll probably want lots of disk space, and the high-performing SSDs still only come in relatively low capacity.
From what I've seen, RAID controller bandwidth is saturated too soon for getting the most benefit out of a 7 disk RAID set, at least as far as sequential performance is concerned. You can't get more than about 800MB/s out of current models of SATA controllers (3ware, areca etc). Having more smaller arrays, across multiple controllers (eg, several RAID1s rather than a single RAID10) will improve this, although the individual performance of each array will suffer.
And, of course, SSDs are small. The Intel X25-E, which I would say are the best SATA SSD I've seen, only come in 32 and 64 GB variants. 
In any case good luck with your SSD build. I'm interested to hear how it turns out as I'll probably go that route in the future. 
The Squid documentation recommends not using RAID but to set up extra cache directories on additional disks.
Just use raid 0 or keep them separate.  I think separate would be better, since a drive failure wouldn't take down your entire cache.
For RAID levels, standard RAID performance notes still apply. A write to a RAID 5 baically involves reading the data block you're going to modify, reading the parity block, updating the parity, writing the data block, and writing the parity, so it is still going to give worse performance than other RAID levels, even with SSDs. However, with drives like the X25-E having such high random IO performance, this probably matters less - as it's going to still outperform random IO on spinning disks for a similarly sized array.
Why are you even considering raid 10 or 5.  You want performance here.  You don't care if the drives just go down, since its a cache only.
In my case I built four servers with 16GB of RAM each. I set 9GB as the the in memory cache for Squid to use. I configured them as a set of siblings so a query to one server would query the others before looking for the data. Altogether I had 36GB of in memory cache. I would not got over four siblings as the communication between them starts to bog down. 
Not all SSDs are equal, so you have to be very careful about picking decent ones. FusionIO make PCIe-backed SSDs which are really high-end performers (with relatively low capacity), but costly. Intel's X25-E SLC SSDs perform really well, and are more affordable, but still low capacity. Do your research! I can definitely recommend the X25-E SLC variants, as I'm using these in production systems.
There are other SSDS out there which may give you great sequantial read/write speed, but the important thing for something like a cache is random IO, and a lot of SSDs will give approximately the same random performance as spinning disks. Due to write amplification effects on SSDs, spinning disks will often perform better. Many SSDs have poor quality controllers (eg, older JMicron controllers), which can suffer from significantly degraded performance in some situations. Anandtech and other sites do good comparisons with tools like iometer, check there.
I set my web application to query a local Squid server running on 127.0.0.1. Then configured the parent of this Squid instance to be the VIP. This allows for very quick failover in the event of the entire VIP going down. If the parents don't respond, the child queries the services directly. It's also handy if you're using a single Squid server and don't have a VIP. Of course if the local Squid instance on your webserver goes down everything grinds to halt.
I'm not very familiar with SSD drives, but I can talk about the sort of architecture I've used which may help solve some of your problems. 
I haven't really looked at 3.0, but 2.x is still single threaded. At some point you're going to run out of CPU or TCP buffers. I'd spread the cache across 2-3 less boxes if possible. Also you may want to make plans to partition your Squid farms in the future if you see the system growing.
I configured a VIP for the four servers for client to talk to. This solved what happens when one server goes down. 