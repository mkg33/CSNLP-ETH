It was the first to have impact and thus has been established, especially in complexity theory. This is a weak reason, but people work that way. We work on old open problems first instead of declaring new ones.
One of the nice things about Turing machines is that they work on strings instead of natural numbers or lambda terms, because the input and the output of many problems can be naturally formulated as strings.  I do not know if this counts as a “historical” reason or not, though.
It's my understanding that Turing's model has come to be the "standard" when describing computation. I'm interested to know why this is the case -- that is, why has the TM model become more widely-used than other theoretically equivalent (to my knowledge) models, for instance Kleene's μ-Recursion or the Lambda Calculus (I understand that the former didn't appear until later on and the latter wasn't originally designed specifically as a model of computation, but it shows that alternatives have existed from the start).
All I can think of is that the TM model more closely represents the computers we actually have than its alternatives. Is this the only reason?