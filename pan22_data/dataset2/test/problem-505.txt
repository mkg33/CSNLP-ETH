3. Look at the hosting plan and server. Use ps aux in a linux console and see if there is something sucking up your memory and processor.
2. Look at the programming and sql statements, unit test with simple scripts that mimic the operations involved, it may not even be what you think is the issue. IF the simple scripts take up time using SQL Joins, split it up and do the same thing with a programmed loop to do the same. This is were memory can help
If you can get a server with enough RAM to hold, at least, the hot part of your dataset, you'll be fine. Also, RAID 1 and 5 are not the fastest way to arrange your data - RAID 0 is faster, but, then, you'll have to consider the higher odds of a filesystem failure that wipes out your database - not a nice thing to happen. You can RAID 1 or RAID 5 your RAID 0 array, provided you have enough drives and controllers.
It's unclear if this question was specifically about SQL Server.  The original poster should clarify this.
You can even play with replication here - do your writes to a disk-heavy server which replicates to one or more memory-heavy servers where you run complicated queries. 
Reducing the overheads from older assumptions in RDBMS implementations was exactly the design goal of VoltDB, but it does scale horizontally with no architectural limit on the data size, and it can persist to disk for full durability using snapshotting and command-logging.
Just as many smaller and more special-purpose languages have been widely adopted in recent years, we are entering an era more special-purpose databases will be needed.
For some further reading on this topic, I recommend the academic paper The End of an Architectural Era (Itâ€™s Time for a Complete Rewrite).  It's not a difficult read.  
In http://www.tbray.org/ongoing/When/200x/2006/05/24/On-Grids . Note that was six years ago. Yes, we have database systems that try (and try hard) to keep the entire dataset in RAM and rather shard to multiple machines than to use the disk because disk is magnitudes slower anyways. You need to write out the dataset to disk but as in the motto above, that's more akin to a background backup task than an online operation. Durability is achieved through append only logs with these databases (I am thinking MongoDB and Redis but there are tons more).
This question is similar to a basic one that has led to a lot of research and development in database architectures over the past 5-10 years.  Now that it is feasible to store an entire database in RAM for many use cases, the database needs to be designed around working in RAM, rather than simply applying older inherited architectures to RAM-based storage.
Overall, you must keep size and scalability in mind. While you may seem to begin with small storage needs, your data will grow very quickly and exponentially. DB's are best using atomic data, which are data broken down to the smallest possible size. Because of the small size, it travels faster within the data warehouse. Then, you also factor in the DB structure. In the future, you could be linking to outside DB's, which is why structure is also crucial. In this scenario, it would make little difference for your query if half of the data lives outside of your data mart. When data is queried, the point is not to keep stored data on the RAM; rather, the query should be quick in accessing and returning data. 