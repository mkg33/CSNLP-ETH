I think it is very common to think that there where no non-obvious steps when the data where collected, but this is not the case: Most of the time, some process or indivudal did somethink with the data, and these steps can and will influence the shape of the data.
I am a CS master student in data mining. My supervisor once told me that before I run any classifier or do anything with a dataset I must fully understand the data and make sure that the data is clean and correct.
I like @Kallestad answer very much, but I would like to add a meta-step: Make sure that you understand how the data where collected, and what types of constraints there are. 
tractors worldwide. I was not at the briefing, so that was opaque to me. Unfortunately, the measurements where off for some parts of france: People all liked ice cram, but we expected a random distribution. There was no obvious reason for this uniformity, so I began to hunt the errors. When I queried the contractors, one had misunderstood the briefing and selected only ice-cream lovers from his database.
The second error was more challenging: When doing some geographic analysis, I found that a lot of people had extremely large movement patterns, which suggested that a lot of them traveled from Munich to Hamburg in minutes. When I spoke with ppeople upstream, they found a subtle bug in their data aggregation software, which was unnoticed before.
Folks here stated great steps, but I think there are great information at the following link what I do when I get a new data set as told through tweets,
I'll add one thing- if possible, do a reasonableness check by comparing you data against some other source. It seems that whenever I fail to do this, I get burnt:(
And when you start building models, make sure to plot the residuals, looking for extreme errors that might be due to an outlier, or look at the confusion matrix and make sure it is balanced. Use k-fold cross validation to optimize your models and look at the variance of the training error for each fold, if one fold performs much worse than the others, it may contain outliers.
It sums up the steps the folks tweeted answering the great @hmason question "Data people: What is the very first thing you do when you get your hands on a new data set?"