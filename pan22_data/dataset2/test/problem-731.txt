I implemented Risky Martin's proposal of using reciprocal multiplication, and it seems to be my best performer so far. Who knew?
I was kind of surprised because I thought that performing 1.0 / input[i - 13] (a division) would be just as expensive as dividing, but maybe there's an optimization because one is a long and the other is a byte? Voodoo? Or maybe I'm just wrong?
One optimization is to skip sequences that contain a 0.  Another is if the previous sequence doesn't start with a 0, calculate the current sequence by dividing the previous product by first digit in the previous sequence and multiplying by the last digit in the current sequence.
This doesn't do any multiplying or dividing to find the correct index. Only at the very end, when it has found the best run, it calculates that runs product.
I hope you had code to convert the 1000 digits from the string into an array of digits.  I know you're going for speed, but a lot of times readability and maintainability is more important, and in those cases you'd want to calculate a product using loops or LINQ instead of coding an increment to the index 13 times.  You should also calculate the currently hardcoded value of 987 using something like int count = input.Count() - sequenceLength;.