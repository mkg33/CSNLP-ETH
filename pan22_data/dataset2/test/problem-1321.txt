This is a question that is very dependent upon the data in question. Assuming that you can train your models in a reasonable time, I would start by not cleaning the data at all, and seeing how well your model performs, and then cleaning it a bit and redoing the experiment and so on.
What I am thinking about, to what extend we should deal with the data quality issues? A robust algorithm should tolerate with the data quality issues but if we do better data cleansing work we get outcome confidently.
As far as I know, when we deal with 'big data'. It is common we deal with more than 10 years' customers data. Quality issue is always there. 
This all depends on the needs and the budget for the models. The first cleaning steps usually bring a decent increase in performance. The more steps you take the slower the improvements will increase in general. If you are doing something for yourself, cut it off at a certain point, if you are doing it for somebody else, ask them what they want and how much they are willing to pay for it. It's comparable to the 80/20 rule where the first 20% of the total work will help with 80% of the performance.
This is because it is possible to overclean your data, essentially removing variations in the data which are actually useful to your modelling.
On another note, your statement about robust algorithms should tolerate with data qualities, you have to be careful with this. If there are biases in your data (missing values are not random for example) they will not learn properly, no matter how robust your algorithm is. Spending time using domain knowledge to fix this properly will help a lot.