In 32-bit land, PAE is what allows you to access memory over the 4GB line. The theoretical max for 32-bit machines is 64GB of RAM, which reflects the extra 4 bits PAE gives memory addresses. 64GB is less than 80GB.
Recently, I had our server admin tell me that the new servers we'd ordered with 140GB of RAM on them had "too much" ram and that servers started to suffer with more than about 80GB, since that was "the optimal amount". Was he blowing smoke, or is there really a performance problem with more RAM than a certain level? I could see the argument - more for the OS to manage, etc - but is that legitimate, or will the extra breathing room more than make up for the management?
Unless he has some clear reasons for why SQL Server can't handle that much memory, the base OS and hardware can do so without breaking a sweat.
I found at least one scenario where you can have too much ram. Admittedly this is a software limitation, not a hardware limitation. 
From there we get processor-specific issues. 64-bit processors currently use between 40 and 48 bits internally for addressing memory which gives a maximum memory limit of between 1TB and 256TB. Both way more than 80GB.
I'm not asking "Will I use it all" (it's a SQL Server cluster with dozens of instances, so I suspect I will, but that's not relevant to my question), but just whether too much can cause problems. I'd always assumed that more is better, but maybe there's a limit to that.
He was blowing smoke - if he'd said 4GB and you were using 32-bit operating systems then he might have had half of an argument but no, 80GB is just a number he's pulled out of the air.
Java applications (like ElasticSearch) suffer when using more than 32GB of ram due to compressed object offsets. 
Obviously there are some problems if memory isn't 'bought wisely', for instance larger DIMMs usually cost more than twice the price of the half-size versions (i.e. 16GB DIMMS are more that twice 8GB DIMMS) and you can slow a machine down quite a way by not using the right number/size/layout of memory but it'll still be very fast. Also of course the more memory you have the more there is to break but I'm sure you'll be happy with that system for what you're asking of it.
Take it to an extreme and say you have Petabytes of memory:  The system (cpu) is not going to work harder to manage memory mappings.  The OS should be smart enough to consume this memory for disk caching, and still have plenty to manage application space (memory and code).  Mapping memory in RAM vs virtual space will consume the same amount of CPU cycles.  