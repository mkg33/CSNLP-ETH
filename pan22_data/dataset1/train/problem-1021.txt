There is no need to iterate over the range. Consider a pseudocode:
Each find is \$O(\log N)\$ at worst. Overall complexity is \$O(N \log N)\$ regardless of the target range.
See my .NET fiddle: https://dotnetfiddle.net/34jkmD
The fast method is sort of based off vnp's answer, although needed some major tweaks to make it correct.
Binary search for lower and upper bound indexes: O(log n)
--This binary search finds the range of numbers which will sum with numbersArray[i] to fit between start and finish
The next for loop loops over this range of valid numbers: This can be up to O(n) but typically shouldn't cover the entire array of numbers. It then adds numbersArray[i] to numbersArray[j] and checks if that value has already been counted in the dictionary: O(1).
So worst case is also O(n^2) (same as yours) but will perform far better on data sets with larger ranges of x for -x <= t <= x, but possibly worse if x is low and the number of values in the hashset is very high. Jump on the fiddle and have a play around with the initialization vars at the top to see what I mean.