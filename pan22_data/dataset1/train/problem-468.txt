One of the well known problems of machine learning is overfitting. The main reason for having a test data set is to 
Now think about the second point, quite often it might occur that you tune up the hyper parameters so that your validation set provides a good measure, but this means you might be overfitting your validation set and when you deploy your shiny model in the real world, boom! poor results are returned.
As per the amount of data to reserve for testing, it depends on how big is your data set, normally the rule of 60% for training, 20% for validation and 20% for test has been used, but if your data set has 5 millions of points, you probably just need 1% for testing (as that will provide you with 5000 examples for testing anyway).
So, yes, your last question makes sense, you will probably be fine taking a small portion of the original data set (again, say 1%) in order to get good results. 
I have a highly biased training dataset where AppId 6,7,8,9,10 are almost never purchased. I made this up just to see how good my comprehension of calculating the classification metrics is such as Accuracy, F1Score, and etc.
The calculation was proven alright and as I expected, the model is right on almost all cases except the app id 2. While I'm doing this, this idea comes into my mind.
Well, if I already know that the app id range is from 1 to 10, isn't it fairly enough to just prepare only 10 rows of test set as long as it has app id 1 to 10? even when the training data set is gigantic?
Why do I need to care about N% of test data set from the training data set?