The following code is to train a neural network model of a given dataset (50,000 samples, 64 dim).
In the beginning, you can see below that the val_loss gets reduced from one epoch to another very well.   
When the model takes many epochs, the loss change becomes so small, especially when the number of epochs increases.
Generally, the decrease in loss tends to be smaller, the longer you train your model. You can think about this in a way, that the model first makes good progress in learning, but later any further improvement becomes harder (thus slower). At some point the model stops to learn. This comes from the logic of gradient decent, a numerical optimization process which is behind most ML models. If the model has learned what it is able to learn, loss does not decrease any more.
What can you do about this? You can try to make your model „better“ in terms of learning capacity. You can increase the capacity of the model (more neurons) or add more layers. You can also adjust the learning rate during learning by „callbacks“ (ReduceLROnPlateau). In this case, you lower the LR automatically if learning progress becomes small. By doing so you can try to make the model to learn more detailed patterns. See callbacks for Keras: https://keras.io/callbacks/.
Here is a nice blogpost about how to train NN: http://karpathy.github.io/2019/04/25/recipe/