Should I use Self Organising Map to reduce the dimensionality?
Not the first choice. For dimensionality reduction you have more handy and basic methods like PCA and NMF (if you don't have negative values) or Archetypal Analysis which are recommended to be used first.
and then K-means to classify them into normal and abnormal?
Red alarm! Kmeans is not for classification but clustering. Be careful about the fundamental understanding of what you are about to do otherwise you get confused. If you have labels K-NN for instance is a supervised (thus proper) method.
Or I can use just K-NN to classify them into normal - abnormal without any dimensionality reduction?
Impossible. As you already know you have the problem called Curse of Dimensionality where the euclidean distances are highly distorted. You need to reduce the dimenstionality before K-NN. The only class of methods I know which may not need dimentionality reduction are Kernel Methods e.g. SVM.
Once I examined the Curse of Dimentionality with K-Means in MATLAB and I saw after 7 dimensions everything was distorted. It of course depends on the nature of the data (distribution,etc) but this is what I got from my experiment.
And if later I'd like to find why this person has included into the abnormal class how can I find that this happened because of these two features, his weight according to his height?
This question is so fuzzy and about the goal of Machine Learning. In Machine Learning you get a set of answers (called labels) and try to learn them. You are not supposed to say if they are right or not. So if you are said a specific guy is abnormal you should ask the one who provided labels (in the terminology we call him expert) about the reason. If a guy with a certain weight and hight is called abnormal you only know this weight and hight indicate abnormal people. Why? Well, who knows?
If I did not get the last question correctly please drop me a line in comments. Would be glad to help.
I recommend you to apply PCA to your data and take the first 5 PCs and feed them to K-NN or LDA. You can plot some of these PCs against each other and see how they discriminate your classes. It gives you a good impression of what is happening. You can also plot the variance or entropy of all features beforehand and drop those features whose variance or entropy is lower than a threshold.
Could you please help me to understand it because I'm not sure if I got it correctly.
Let's say I have a dataset, of persons, with 100 features, various characteristics like height, weight, age, etc. I want to classify if are normal or abnormal. By abnormal I mean if a 20 years old man is 170cm and 150kg to identify it as abnormal.
Should I use Self Organising Map to reduce the dimensionality (these 100 features) and then K-means to classify them into normal and abnormal? Is that a correct approach? Or I can use just K-NN to classify them into normal - abnormal without any dimensionality reduction?
How many features can I use with K-NN? All the examples I've found so far use just two.
And if later I'd like to find why this person has included into the abnormal class how can I find that this happened because of these two features, his weight according to his height?