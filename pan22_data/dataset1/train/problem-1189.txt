Suppose that you have a learning problem and it's just for fitting a function which depends on only one feature, and the function to be predicted is a quadratic shape. you can have a good performance by just having the input feature if you use linear SVM but it will have errors. adding extra features, polynomial features as the input may be useful but it adds more complexity to the classifier and it causes more computational overheads. By the way that's true. You can have better estimate of functions by adding the high order polynomials of the features in hand. I don't know if you are familiar with normal equation or not but what that does is to add high order polynomials to better fit the function which makes the current data. 
Intuitively, the linear classifier you built is only trying to find a local minimum meaning he is trying a lot of different operation but it is not looking at the whole space of possibilities, if an "and" operation is relevent to your problem and you add it as a feature it will improve the SVM performance.
The fallback of adding features like this is that it can help the model overfit : it is easier to find a linear bound that separates well the data if the input space has a lot of dimensions.