I've encountered this issue before and in previous cases for us the solution was to develop a system of ETL processes which bring the information over into the warehouse or reporting database. What we did was design the process to run near continuously and at the head of each iteration we would grab the current maximum datetime stamp of each table. We would then process each record that was generated or updated between the last timestamp and the current one we just retrieved then repeat this process constantly. There is a chance with this for partial data, but with the process running continuously any partial data would be resolved within a few minutes.
As already suggested, you'll need to build an ETL (or series of ETLs) to extract data at regular intervals. Have a look at Oracle's Change Data Capture (CDC) so you can pick up only those records which have been modified since the last load.
http://docs.oracle.com/cd/B28359_01/server.111/b28313/cdc.htm
As for the destination end, the Kimball methodology suggests you'll should use two separate partitions, a real time partition and a data warehouse partition. Push all real time information into the real time partition and then nightly empty that table into the data warehouse. 
http://www.kimballgroup.com/2001/11/design-tip-31-designing-a-real-time-partition9/