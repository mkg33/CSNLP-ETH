Sipser proved that no infinite parity can be computed by an (infinite) circuit of any constant depth, which you can view as a warm-up to the result that PARITY is not in $AC^0$. 
There are also some results and attempts at proofs of lower bounds in proof complexity using nonstandard models (some results of Ajtai and Krajicek. See esp. Krajiceks' "Forcing with Random Variables and Proof Complexity," available from Cambridge Press, but also a draft available online) . The basic idea is to build a nonstandard model of arithmetic in which a statement is false in the model (rather than "true, but without short proofs"), and then, from the properties of the model, infer that a corresponding sequence of finite statements does not have polynomial-size proofs in some proof system. 
I'm not sure, but my impression is that often these results sort of "hide the asymptotics under the hood" so that it's not so much a reduction from threshold to finiteness as it is a new mathematical language in which "false" in the new language corresponds to "without short proofs" in the old language. That's not to say that the new language doesn't provide a useful new viewpoint, but I'm not quite sure if it's what you're looking for.
It is usually simpler to reason about calculus where the limitation is finiteness of computation rather than a threshold like "computable in polynomial amount of time".
In formal languages theory for example, rather to use the $\exists n. x^{n+1} = x^n$ to characterize aperiodic mono√Ød, it is easier to use profinite words so that $x^{\omega+1} = x^{\omega}$.
In complexity theory, the only technic I know which is linked to that is the padding trick for example linking the problem of P vs NP to EXPTIME vs NEXPTIME. But the natural infinite equivalent of complexity questions would be computability ones'.