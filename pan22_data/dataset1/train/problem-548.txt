Answering your last question first: your mean and std should be computed over the entire image. So for an RGB image you will get a mean/std vector of 3 values for the entire dataset (calculated per channel). Normalizing per pixel is wrong and would result in loss of information (you need to think of an image as a single discreet function, you can't treat each part of it separately without changing its overall meaning).
Regarding normalizing the validation set, you can manually create the validation set and use the "validation_data" argument in the fit method:
I have a question that normally, when we are making a training set and a final test set, 
So, when we are making a training data, a cross-validation data and a final test data, shouldn't we compute mean and std for preprocessing using the training data alone and then standardize the validation set and test set using it? And, if I'm right how can we do it in keras, since in keras like in the below code, taken from kaggle( by Francois Chollet MNIST tutorial):
The validation set is created using the fit method and thus, it is also used in computing the preprocessing mean and std. So, how to create validation set before and use only the training data for feature scaling and normalization? And which method is the right method?
Also, in the above code scale and mean are being computed for each column respectively and is being used for normalization , should we compute mean and scale(standard deviation or range) for each pixel or for the entire image? How does that affect?