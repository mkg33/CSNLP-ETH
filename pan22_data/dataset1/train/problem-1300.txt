The only way to really know how many amps your server will use is to power it up, give it some work and measure it. You can work out the maximum load of your server using P=IV (where P is the power I is the Amps and V is the volts) so P/V=I. P will be the maximum power rating of the PSU and V will be 110/230 etc so for a 700w PSU using 230V then the amps will be ~3A maximum. In reality it will be lower. 
I've been looking into getting some servers with co-location, I've worked with co-lo'd servers before but this is the first time I've actually a rack up.
All co-lo' providers specify a limit on the AMPs a server can use (I.E. 2 AMPS, 8 AMPS etc.) but I'm having a hard time figuring out what this means. So I have a couple of questions relating to this;
Say I have a 1000W power-supply* with 4x12V output rails. Firstly is the total output voltage 12V split across the 4 outputs or 48V combined across the 4 outputs?
Using my slightly inaccurate brain-calculator (And providing I'm remembering the forumlar correctly) if it's 12V split across all 4 outputs (So 12V total) it would be ~85 AMPs (So a single power supply would use far more than what's avaliable to an entire rack with most providers.) If it's 48V combined across the 4 outputs that's still ~20 AMPs.
Am I missing something or just being stupid? A lot of the stuff I found online semed vague. I'm also guessing our actual usage will depend on many things including the efficiency of our other hardware and the usage of the server, but I'd like to calculate the maximum usage we might hit.
*I haven't got round to the bit where I actually figure out what power-supplies/components we need for our servers, I just grabbed the specifications from a random power-supply online. If there is some data I missed the power supply I based my example on is here