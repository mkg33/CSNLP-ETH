Some "monitors" that really are TVs have braindead logic of always applying analog TV era -overscanning to digital inputs, too. Such displays can never ever display sharp image from any source. (The whole analog overscanning was an artefact of less-than-perfect tube drivers, and later lazy TV stations not bothering to render the image correctly to the edge because "all viewers are overscanning the image".)
Try to look at user manual of the TV or search internet forums for tricks to get TV to accept native resolution. Technically the only correct solution is to set both graphics output of your computer and the TV to "overscan: 0%" setting. This may be tricky for some TVs; for example, one Samsung TV model required image source to be connected to HDMI2 connector (the TV had 4 HDMI inputs) and the connector had to be labelled "PC" (instead of, say "PS3") for the magical overscanning and image processing circuit to switch off.
Try going through monitor test pages at http://www.lagom.nl/lcd-test/ and see if you can achieve 1:1 native pixel mapping from your display adapter to monitor display.
I have a monitor (with TV tuner - Philips 221T1SB), native resolution of 1920x1080, but there is an Overscan in Catalyst. It says from left to right '10%' and '0%', if i set it (all the way to the left) on 10%, i get screen with black borders (you know what i mean), but if i set it to 0%, i get a filled monitor. It does make sense, but why the hell is there such an option?
I never had any AA issues before, but now, with this monitor, here they go. I can't find any solution, i have only one lead = overscan, or pretty much anything with the CCC settings.
Yes, i almost forgot. After a bit of trying to configure the monitor with remote, i found there is a PC channel option, few other channels, and then a HDMI option. I have monitor through HDMI, but why i get black screen when i select the PC channel?
I tried almost everything. Please guys, give me a hand. I don't want jaggies!