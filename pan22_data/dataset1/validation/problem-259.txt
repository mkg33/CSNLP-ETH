The relation would be not as much a pure space:cpu issue. Depending on the solution you are aiming at, you might need CPU power for features like deduplication, compression, encryption or hash / checksum calculations. 
A simple data copy process would not incur much CPU overhead unless you have a really badly designed system. Your requirement of 100 GB per 18 hours would average to 1,58 MB/second - even a netbook would be able to cope with that nowdays.
You also should concentrate on the I/O backend you are using. While writing 1,58 MB/second does not sound terribly challenging, using 50-100 simultaneous processes to do so will incur a lot of random write load. Hard disks do not cope well with random (write) loads as these will incur a lot of time-intensive head seeks, so you will need to have something that cushions randomness - like a DRAM or SSD write cache.
is there a relationship between storage space and number of cpu's required to handle it? if i have 100GB of data a night coming in through a fat pipe remotely from 50 different sites so that the total data footprint over a 18hr period is 100GB, and i wanted to have a 20TB NAS or storage system receiving it, does the NAS server need to have, for example, 1 xeon or should it be a dual cpu with multi core etc
the purpose of the NAS is redundancy for data in the field. Assume that there is no bandwidth issue and we will be using fat pipes to move the data, my concern is the simultaneous piping of seperate geographic instances of 2GB of data coming in from 50 - 100 seperate sites. i don't want to have read/write issues or missing/crashing of a sync.
i need a direction to start testing with this idea, so if i have to move that much data onto a NAS server nightly.
if it is too vague a question i can elaborate it more, thanks.