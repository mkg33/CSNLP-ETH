That's a perfectly valid <a> tag. Your code is not going find this tag.
Web pages can be big. If we just look at amazon.com
So this page is 200K bytes long. I don't think its a good idea to load the whole page into memory then parse the page.
Personally I would just work with the stream. You can get iterators that work on streams just like they work on a text string so your algorithm would not have to change.
Most standard C libraries have a C++ version. The main difference is that the code is guaranteed to be placed in the standard namespace rather than the global namespace.
If you are not mutating your parameters (check and text) then pass by const reference this will prevent you objects from being copied. Currently you will be making a copy of text which if it is 200K in size is a serious performance issue.
Yes. Iterators are the best thing. But these are not iterators (apart from the name).
An iterator has a couple of properties. It can be incremented/decremented (you have that covered). But an iterator can be dereferenced with operator* to get the value. It looks very much like a pointer. This is an index (nothing wrong with that. indexing into text is just fine just use the correct terminology please).
You declare second_iterator way up the to tof the function. But it is only used deep inside a nested loop. Just declare variables before you use them. Then it is easy to see the type information close to the point where they are being used.
My intention is to create a complete HTML parser, so far I made a basic algorithm that iterates trough text and extracts everything in an "a" tag.
It works on everything I tried, but I want a review of what I could do better or what I did right.