It does make sense, they are just two different things.
Dropout only makes your model learning harder, and by this it helps the parameters of the model act in different ways and detect different features, but even with dropout you can potentially overfit your traning set.
On the other hand, early stopping prevents your model from overfitting by taking the best model on your validation data so far.
However, for the sake of simplicity, I think it is easier to just use dropout (training a neural network is not easy and the training may not be successful due to many different reasons, it is a good practice to reduce the possible reasons why the training is failing as much as possible). Unless you have short time to train your network, with a sufficiently high amount of dropout you will ensure that your model is not overfitting.
My final recommendation is: just use dropout. If using a 0.5 dropout rate still overfits, set a higher dropout rate.
Avoid early stopping and stick with dropout. Andrew Ng does not recommend early stopping in one of his courses on orgothonalization [1] and the reason is as follows. For a typical machine learning project, we have the following chain of assumptions for our model:
And what we want are tools that can target one of these four objectives and not the others in order to keep improving our models more efficiently, and this concept is called "orthogonalization." An analogy would be the different options in photo editing apps such as brightness, contrast, and saturation adjustments, which are independent of, or "orthogonal" to one another. Examples of orthogonal optimization strategies for the four goals are listed below:
Fit the training set well on the cost function (e.g. bigger neural network; Adam optimization)
Fit the dev set well on the cost function (e.g. regularization; bigger training set)
Fit the test set well on the cost function (e.g. bigger dev set)
Performs well in the real world (e.g. change the test set; change the cost function)
Because early stopping both fits the training set less well and improves the dev set performance at the same time, it is not orthogonal and Ng advises us not to use it.
[1] Week 1 of Course 3: Structuring Machine Learning Projects of Coursera Deep Learning Specialization