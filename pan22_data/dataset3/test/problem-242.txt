How can you learn something when there is so little to learn from?
However, there are features more or less useful relative to other features.
Overfitting certainly is a real problem within machine learning.
Overfitting is a real issue for almost any algorithm, and noisy features make it more easy to learn things that are not actually there.
In general pruning features are seen as a good best practice.
But if I remove around 10 bad features (found using some technique) I am observing a logloss of .45.
I am hesitant to think of features as bad features.
till now I was under the impression that machine learning algorithms (gbm, random forest, xgboost etc) can handle bad features (variable) present in the data.
Proper partitioning of a dataset can help to adjust the model parameters where warranted helping in a more generalizable model.
Yes it can for sure, some algorithms are more robust to this than others but doing proper feature selection is adviced.
If you have 10,000,000 data points 150 features is not an issue, if you have 400 data points 150 features is way too much.
My question is, can bad features really make such big differences?
In one of my problems, there are around 150 features and with xgboost I am getting a logloss of around 1 if I use all features.
Usually a combination of creating too powerful of a model, not having enough data, and/or having too many features can create an undesirable outcome.
It's not only the algorithm but also has a lot to do with the amount of data points you have compared to the number of features.