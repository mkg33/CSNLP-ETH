Use this report to add to the ignore list and to the alert list.
Generate a daily report that excludes the "ignore" messages.
The process of investigating messages you don't immediately understand (which will be a lot of them at first!)
I don't believe any general purpose tips can be made to interpret error logs, except that you must research each error on a case-by-case basis, e.g.
Consult the documentation about the log files that the developers handed over along with the application.
You're learning what those cryptic log messages mean - and which are trivial, and which are a big deal.
For patterns identified as real errors, deliver an alert to admins in real-time.
Their job isn't done when the code is, it's done when the operations people can run the application and keep it running, and that means documentation, handover meetings, designing for manageability etc.
Be able to handle floods of alerts from a broken system that you can not fix right away.
By getting in there regularly and proactively reading the logs, you're gaining experience and familiarity.
I think this is the greater advantage to the sysadmin.
My habit with server logs is: review them regularly, and investigate/resolve the issues I find.
About a specific common situation when you have all of these at the same time: (1) a problem in a distributed environment (2) a huge pile of debug info scattered over co-operating servers and different logfiles (3) no documentation for interpreting the logs (4) nothing on google (5) no clue (6) ping-pong players instead of vendor's support.
Usually when I get a new system to manage, it will have quite a few errors in the log, many which recur fairly regularly.
My goal with such systems is to revisit the logs weekly until I have solved or understood every new error that comes up; then relax my log reviews to monthly.
Let developers troubleshoot production issues once in a while.
For handling something like syslog, especially when aggregating many machines, a general purpose suggestion can be made.
with Google or by reading source, to understand it.
The main reason this is effective, really boils down to a couple of old sayings:
More seriously, documenting log files and how to interpret them needs to be make one of the developers' tasks.
Obviously if you're solving issues while they're small, you're ahead of the curve, and users/management will have less reasons to yell at you; that's a good thing.
The prior admin often shrugs them off with something to the effect of "not real sure what that's about, but the users never complained, so I didn't consider it broken enough to fix!"
teaches you a lot about the internals of the OS and the apps running on it.
Failing to do this in a Unix environment is probable the single most significant (costly and damaging) commonly-made oversight.
Keep a list of patterns to ignore, and a list of patterns to alert on immediately.
It is worthwhile to keep two additional levels of patterns - those worth reviewing but that are not likely to be a problem, and those worth alerting on but not disrupting someone.
(Or even watch the log file in real-time excluding the ignorable messages).
I do this proactively - not waiting until the users are howling about a system outage.
Ideally your ignore list should be thorough enough that you can read through the messages that fall through, and your alert list should be simple enough that you can investigate every one that you are alerted on.