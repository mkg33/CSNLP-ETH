To emphasize that point you might have some examples of code (or pseudo code) to handle both a normalized and non-normalized version of the same dataset and the storage requirements for a sample dataset at each level of normalization.
As you have said, they intuitively know how separating things makes sense already.
It seems that the presentation of the material is done well.
The problem is that if you store the same information in two places, independently, they can get inconsistent when one is updated and the other not.
However, the benefit of static typing is that certain kinds of errors in programs can be caught earlier (in the compiler or intelligent editor) rather than later (runtime).
It is especially needed in Relational Databases since the database doesn't actually store the semantics of the data (other than weakly via field names).
The field is complex since some of the higher levels are inconsistent and are inconsistent with other "measures of goodness" of storage protocol.
One thing that hasn't been mentioned (enough) in the earlier answers is the reason that one wants to do normalization in the first place.
One suggestion I have for you is to make students "measure" the cost of a denormalized schema by executing update and delete statements on schemas that are not normalized.
That way, maybe, you can find out where the ones without traction are skidding out of control.
If not included in the lecture, you could explain the trade-offs between doing the normalization (the design work), and storage space for the extra tables vs. the potential savings in coding and possible duplication of data.
There are several levels of normalization, each reducing a certain kind of redundancy.
One example of how the cost can be measured is in measuring the number of SQL statements needed to execute one logical update (a full address repeated in every pending order of a customer).
After reading multiple perspectives on database normalization, I condensed my learning into the article at https://bkmjournal.wordpress.com/2010/07/29/database-normalization-as-i-remember-it/.
Therefore, I suggest that in teaching normalization you start with an example that exhibits redundancy problems and demonstrate how it leads to losing information (lossyness) through inconsistency.
So, start concrete in this case and work toward the abstract.
The semantics there is the semantics of relations (rows, columns, project, join...), not the semantics of, say, "Employee Payment and Taxation" (Name, id, address...).
Java gives explicit types to variables and then checks that the assigned values have those types.
Given that the problems assigned from the textbook are in a sequence that increases with complexity, there isn't much you can change.
That is actually redundant, as the Python language shows.
As a simple example, suppose you store someone's age somewhere, but also store their birthday somewhere else, and they aren't dependent in any way on each other.
However, in storing data, redundancy is generally bad.
Database Normalization is a way to reduce redundancy.
A possibility is to assign one problem at a time, going over the results of it before moving to the next one.
Take what they know "intuitively" and make it explicit.
Then show how the, say, First Normal Form normalization makes it better, but maybe not yet perfect.