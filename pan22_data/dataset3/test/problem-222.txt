A file that has been optimally compressed will have no patterns or anything that can be reduced.
https://en.wikipedia.org/wiki/Lossless_compression#Limitations
For example, if you forced the above example to be re-encoded, you might end up with something like this:
As you can see, by replacing repeated data with just the data and a count of how many times it occurs, you can reduce this specific example from 35 bytes, down to 20 bytes.
If we compress it we could say that it is 20 A's, newline, followed by 20 B's, newline, followed by 20 C's.
Also, lossy compression usually uses some sort of pattern-based scheme (it doesn’t only discard data), so you will still eventually reach a point where there are simply no patterns to find.
If that's true we almost don't need file storages at all.
So why can’t you compress a file that’s already compressed?
What you could try is to use a different compression algorithm because it is possible that the output of one compression algorithm could possibly be prime for a different algorithm, however that is usually pretty unlikely.
If all compressed files after compressing again reduce their sizes (or have sizes no larger than their parent) then at some point the size will become 0 which can't be true.
Moreover, this is a small, contrived example; larger, real-life examples could have even better compression.
Because when you did the initial compression, you removed the patterns.
For example, RLE (Run-length Encoding) is a simple compression method where data is examined and runs of similar data are compressed down as so:
(The OO was left alone because replacing it with 2O would not save anything.)
In fact, often when you try to compress a file that’s already compressed, you could end up with a larger file.
That’s not a huge reduction, but it’s still 42% smaller.
Text files often compress really well because they tend to have a lot of patterns that can be compressed.
For example, the word the is very common in English, so you could drop every single instance of the word with an identifier that is just single byte (or even less).
Once we have done the first compression there is no new patterns to compress.
Of course, this is all about lossless compression where the decompressed data must be exactly identical to the original data.
You can also compress more with parts of words that are similar like cAKE, bAKE, shAKE, undertAKE, and so on.
Because compression works on the basis of finding patterns and reducing data that is similar.
Now, the compression data (the run-counts) are themselves being treated like data, so you end up with a larger file than you started with.
With lossy compression, you can usually remove more data, but the quality goes down.