However, I would agree with jamesmf to check the discriminative power of your feature at first.
Imagine the case where your features cannot separate between positive and negative examples at all.
In this scenario you might but have strongly predictive features or you may have insufficient data.
I am trying to solve a classification problem on a highly imbalanced data set.
But, the prediction error for the minority class is extremely high even after using a balanced data set.
You can also check this paper which provides a comparison between different methods and draw some insights when over- or under-sampling are preferable.
Balancing your dataset does not guarantee an even prediction split.
You would therefore expect that your prediction error would mirror the distribution of the majority/minority classes.
After creating a balanced data set, I applied the random forest model.
SMOTE is not designed to work with severe data imbalance specially if you have wide variation within the minority class
In this case, even if you balance the dataset, you will learn a decision boundary that essentially randomly guesses on each example.
By experience, I would also consider to check the ROC and AUC.
In R, you have this toolbox that can provide you different options.
I am using SMOTE to over sample the minority samples and down sample the majority ones.
In my experience, giving weights to observations (if the algorithm in use supports it) generally works better for highly imbalanced  classification problems.
One might try to use under-sampling as well as other over-sampling methods.
Since, your are using RandomForests I would suggest you to try that.