There is a lot of information on this very topic (and potential ways to mitigate), just google regression and multicollinearity.
Then by checking the feature importance, I realize zip is a pretty good feature, so I decided to add some more features based on zip - for example, I go to census bureau and get the average income, population, number of schools, and number of hospitals of each zip.
If you still have other features in the model, which are not related to the ZIP, they could potentially become overwhelmed - that depends on the model you use.
These criteria can help tell you when to stop, as you can try models with more and more parameters, and simply take the model which has the best AIC or BIC value.
BIC simply uses $k$ slightly differently to punish models.
When you expand your features this way, it is something you might want to keep in mind.
In this case you might compare these to something like Principal Component Analysis, where a collection of features explain one dimention of the variance in data set, while other features explain another dimension.
$${\displaystyle \mathrm {AIC} =2k-2\ln({\hat {L}})}$$
There are metrics that will try to guide you with this, such as the Akaike Information Criterion (AIC) or the comparable Bayesian Information Criterion (BIC).
$\hat{L}$ is the maximum value of the Maximum Likelihood (equivalent to the optimal score).
A simple example will be using xgboost regression on your data and specifying the number of cycles.
However, they may also explain things about the dataset which simply cannot be contained in the ZIP information, such as a house's floor area (assuming this is relatively independent from ZIP code).
It will decrease to a limit after which you'll be able to deduce that the model has plateaued after a certain cycle.
I have a practical question about feature engineering... say I want to predict house prices by using logistic regression and used a bunch of features including zip code.
These essentially help to pick a model based on its performance, being punished for all additional parameters that are introduced and that must be estimated.
As far as stopping a cycle is concerned, well there are several measures and factors that you need to be aware of to check where your model has stopped performing better and those are measures like the RMSE.
So no matter how many ZIP-related features you have, you may never explain importance of floor area.
With these four new features, I find the model performances better now.
Also, unnecessary features only add to the burnout, so it's always good practise to clean up certain features.
It is always worth to have as many features as possible.
One thing to keep in mind, however, regressions, in general, do not work well with data that is highly correlated (multicollinearity).
where $k$ is the number of parameters to be estimated, i.e.
There is always a limit to this though since an information overload too might burn your processor, so be careful of how many features are being engineered.
Run the model and you will get the RMSE for each cycle.
number of features you apply, because each one will have one coefficient in your logistic regression.
The more the information, the better will it be able to perform and predict.
Eventually the model will be dominated by these zip-related features, right?
If you can keep adding new data (based on a main concept such as area i.e.
the ZIP code) and the performance of your model improves, then it is of course allowed... assuming you only care about the final result.
So I add even more zip-related features... And this cycle goes on and on.