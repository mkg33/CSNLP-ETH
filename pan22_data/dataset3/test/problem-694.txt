I have two Windows servers (2016) in two different places, both with about 8TB of data.
My theory is that this should be possible through some looping of Powershell's Get-FileHash, where I run it on one server, export the data to a file, then move that file to the second server and somehow compare it - but I'm not enough of a Powershell guru to do it myself, unfortunately.
They both should have the identical files and file structure, but it's possible that things got changed between them.
Absolute paths would make it difficult to compare the two output files.)
It is small and straightforward to use and should be perfect for your scenario.
There may also be a utility that can do this, if anyone knows of one - the main sticking point is that I can't use a program that simply compares both directories, because the two servers are not connected to the same network (or the internet) at all.
That would make sure each server serve the same file set, and that would allow you to setup sync schedule in case it's a low bandwitdh site.
It generates a text file containing a list of files in the target directory (including subdirectories) and their hashes.
(You want to make the hashing operation relative to the current directory so that the output file contains relative paths rather than absolute paths.
The command line to use would look something like this:
You can then compare the two text files (using, e.g., windiff) to identify any discrepancies.
I would simply create a DFS replication between the two servers.
I want to identify files that either don't match or are present on one server but not the other.