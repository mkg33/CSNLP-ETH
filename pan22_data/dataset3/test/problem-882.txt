There is a great book written by Andrew Ng, called "Machine Learning Yearning".
Even if you wanted to use the test dataset, you cannot.
In the image below, you would make four different splits on your training set, where blue would represent training cases and orange the validation cases.
The datasets you have are the training and the test.
I strongly recommend to read this chapter, since sampling methods are extremely relevant in practice.
The idea is that you reserve a truly "exogenous" dataset (never used during training) to test how well your model does in the end.
In order to balance groups etc, you can use a stratified sampling strategy.
Cross validation (CV) usually means that you split some training dataset in k pieces in order to generate different train/validation sets.
Keep in mind that the above Test set has nothing to do with the test file that you got from Kaggle.
Reading the chapters 5, 6 and 7 of the linked book, you will get a lot of great advises on how to handle the training data.
You run the same regression/classification on each of the four blue parts, and obtain predictions on each of the four orange parts.
During training and model tuning, your model should not see the test data!
Most tools/methods come with some CV options if relevant.
Chapter 5 of "Introduction to Statistical Learning" covers CV and bootstrap.
You can download it for free here https://www.deeplearning.ai/machine-learning-yearning/
Since sampling strategies can be relevant, it is advisable to let some tool like sklearn do the CV splits.
There are different strategies to deal with data characteristics.
The target column is missing, because this is the file that you need to use the model and predict the values to upload it on Kaggle.
If you open it, you will see that only the features are there.
If you use your test set during training, information may leak from your test set to the model, and you cannot demonstrate exogenous validity of your model anymore (becasue information from the test set leaked to your model).
By doing so you can see how well a model learns (and is able to make predictions) on different samples of a training dataset.