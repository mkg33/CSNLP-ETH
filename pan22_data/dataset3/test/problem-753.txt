The you have to customize your gradient to use it in your TensorFlow implementation.
You may find its mathematical foundations in the original REINFORCE article by Williams or in Ian Goodfellow's seminal book on Deep Learning (section 20.9.1 Back-Propagating through Discrete Stochastic Operations).
This can be achieved thanks to the log derivative trick.
The first question is to defined the definition domain of your J in which apply finite difference discretization, I should have taken your different layers (but I have never tried this).
You can have a good idea here and may be have look in this tensorflow API.
The second question is linked to the fact that your error is "backpropagated" so when constructing your definition domain you should first think of the direction of your domain (from First layer to last or from last to first).
Finite difference is a way to calculate the derivative of a function according to its value.
You may consider using a score function estimator (also known as REINFORCE), which defines an estimator of the gradient of a scoring function that does not need to be differentiable.
So I suggest you to see here and to see in StackOverflow if other people has already ask for what you want and if you can find more elaborate answers.