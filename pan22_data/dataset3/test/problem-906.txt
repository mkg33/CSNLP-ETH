The most useful view is a memory usage table of how much memory was allocated in the last time period.
I had a hard time with that recently, because the process(es) that the oom-killer stomps on aren't necessarily the ones that have gone awry.
It highlights processes that ar 80%+ in blue and 90%+ in red.
Can anyone give me some pointers at how to diagnose what may have caused the most recent incident?
Seems you can set priorities to prevent oom-killer killing certain processes (sshd would be a good start for a VPS!)
While trying to diagnose that, I learned about one of my now-favorite tools, atop.
Looking at the logs, I saw that oom-killer had killed these processes, possibly due to running out of memory and swap.
Over a pre-set time interval, it profiles system information.
This article on taming oom-killer looks particularly useful.
I have a small virtual private server running  CentOS and www/mail/db, which has recently had a couple of incidents where the web server and ssh became unresponsive.