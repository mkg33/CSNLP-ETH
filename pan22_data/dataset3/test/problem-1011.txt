This is of course based on a zero-caching model - if you can do it all from memory then life gets suddenly easier but you don't mention volumes so let's assume the worst.
Now this CAN be achieved using the right chipset but you're not talking low-spec/cost servers here, I use HP DL980's for this sort of thing and they're not cheap.
It can be done (I have lots of servers doing about half this performance day in day out), but you need to know that this isn't a cheap or fast project.
You also have to understand your write profile as that'll really get in the way of your read performance if it's even remotely heavy.
If you let us know what you're actually trying to achieve here (as this smells a lot like homework to be honest) then we may have some more creative ways of helping but I'd say this is a ~$100k/~2-month project to get going.
Actually creating a single volume that's theoretically capable of this kind of performance on a single server isn't that hard - you need a number of PCIe-based flash drives such as the FusionIO ones I use myself - bound together using LVM or similar (ZFS for instance).
Then you come onto the biggest issue, interrupts - there are lots of pretty clever ways of dealing with the interrupt-storm you're creating here but even if you manage to recompile your kernels (Windows, even 2012, will struggle with all this) you're really pushing the actual deliverable, consistent capacity of these servers today - you're asking so much of just about every part of the chain and the weakest part if the most complex, the OS.
That gets you the actual volume up and running but the issue isn't just that, it's with end-to-end performance.
that's ~80Gbps, so you'll need at least a single 100Gbps NIC on a dedicated PCIe 3.0 x16 bus just to clear that amount, plus the same on the storage bus/es of course.