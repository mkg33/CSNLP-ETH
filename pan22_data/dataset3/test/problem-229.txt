You can then perform $k$-means on the projected data and color each group with different color to facilitate visualization.
So I wouldn't be surprised if the results are barely interpretable.
Both methods make most sense if the input variables are continuous and of the same scale.
Principal components are the results of projections of your features on 3 directions that carry a lot of a variance and a minimum of information loss ( when you take a projection and reduce it from 3-D to 2-D for example, you lose a bit of information ).
To obtain them, you project the data onto the $3$ leading eigenvectors of the covariance matrix.
What I'd be more concerned about is the input data, as it is not particularly well suited for neither k-means nor PCA.
Just run k-means and project it to 2d for visualization with PCA.
The principal components are directions that are orthogonal to each other and the first $3$ components should carry the most variance with them.
They you are largely seeing the data the same way as k-means (if you only use the rotation, not the scaling!)
You could say ,in other words, that PC 1, PC 2 , PC 3 are somewhat a combination of all your features you used for PCA.
K-means minimizes sum of squared errors, and PCA finds the projection with maximum sum of squares.