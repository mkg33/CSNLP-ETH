While wget was already mentioned this resource and command line was so seamless I thought it deserved mention:
I find Site Explorer is useful to see which folders to include/exclude before you attempt attempt to download the whole site - especially when there is an entire forum hiding in the site that you don't want to download for example.
Generally speaking, most browser caches are limited to a fixed size and when it hits that limit, it will delete the oldest files in the cache.
I believe google chrome can do this on desktop devices, just go to the browser menu and click save webpage.
Typically most browsers use a browsing cache to keep the files you download from a website around for a bit so that you do not have to download static images and content over and over again.
Free Download Manager has it in two forms in two forms: Site Explorer and Site Spider:
This saves them the trouble of hitting these sites every time someone on their network goes there.
I'll address the online buffering that browsers use...
Also note that services like pocket may not actually save the website, and are thus susceptible to link rot.
wget -P /path/to/destination/directory/ -mpck --user-agent="" -e robots=off --wait 1 -E https://www.example.com/
ISPs tend to have caching servers that keep copies of commonly accessed websites like ESPN and CNN.
This can speed up things quite a bit under some circumstances.
The venerable FreeDownloadManager.org has this feature too.
This can amount to a significant savings in the amount of duplicated requests to external sites to the ISP.
Lastly note that copying the contents of a website may infringe on copyright, if it applies.