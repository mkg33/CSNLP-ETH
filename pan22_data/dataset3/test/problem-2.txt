For detail, see the last figure in http://www.astroml.org/sklearn_tutorial/dimensionality_reduction.html#id2
At the end you get a picture preserving the similarity of the original data with some degree of precision.
(Correct me if I'm wrong, but I don't even think there is a large effort by the ML community to use t-SNE to aid classification; that's a different problem than data visualization though.)
Sometimes, it is meaningful to visualize high dimensional data since it may tell us physics.
I'm just very largely confused why people make such a big deal about some of these visualizations.
For example, if you project your data down to principal components generated by PCA, those principal components (eiganvectors) don't correspond to features in the dataset; they're their own feature space.
Some of these embedding (manifold learning) methods are described here.
It is easy possible to implement this method at your own using some optimization tool (e.g.
Taking a slightly different approach than the other great answers here, the "pretty picture" is worth a thousand words.
Since this data set has large dimensions, it's difficult to visualize it.
From the arts, look at Marshal Ney at Retreat in Russia.
A visualization method provides only different “glasses” to see a high dimensional space.
Ultimately the charts are simply communication, and for better or worse, human communication is often times focused on conflation, simplification, and brevity.
Similarly, t-SNE projects your data down to a space, where items are near each other if they minimize some KL divergence.
I ask because the projection down to this embedded space is usually meaningless.
What possible insights can someone grab by trying to visualize this embedded space?
Based on the statements and the discussions, I think there is an important point to distinct.
This is what books like Freakonomics do - there's little to no math, no data sets, and yet the findings are still presented.
There is at least one example in astrophysics where you project your data down to principal components generated by PCA and those principal components correspond to much physical insight about the galaxies.
That doesn't mean we cannot help the person to understand, at least a general concept or a piece of the reality.
A good thing to “trust” a visualization method is to understand the internals.
http://iopscience.iop.org/article/10.1086/425626/pdf
However, the first 4 components from PCA reveal much physics about the spectra (see sections 4.1-4.4 in the paper above).
The authors apply PCA to many spectra (e.g., 10,000) from a telescope.
A transformation to a lower dimensional space may reduce the information, which is something different from making the information meaningless.
Ultimately, you will need to convey your findings to someone who is not as statistically literate, or who simply does not have the time, interest, or whatever, to grasp the full situation.
So you can see how the method words, you may measure the error of the result etc.
And we go through the motions of projecting the data down to a 2D or 3D space, so we have a "pretty pictures".
Observing (2D) pictures of our world (3D) is a usual practice.
There are many techniques for visualizing high dimension datasets, such as T-SNE, isomap, PCA, supervised PCA, etc.
This massive oversimplification of the Napoleonic wars nevertheless conveys great meaning and allows people with even the most ignorant knowledge of the war to understand the brutality, the climate, the landscape, the death, and decorum that permeated the invasion of Russia.