As a programmer, my approach would be to create a small app which parses the file line by line and checks for the number of commas that are in the line (you should, I assume, know how many a correct line will have)
In my experience, trying to fix this from your current data, will take any time you want, and there's no warranty, that your result is identical to the initial data.
It might be easier, to wait until the database is available again.
The obvious fix is to export the data again in a different, non comma reliant format, but access to that SQL database is more or less impossible at the moment...
You should also think about extracting only some columns and a unique key value (record number?)
You could try to identify data values of characteristic columns, e.g.
We have a large database with customer addresses that was exported from an SQL database to CSV.
If it matches the expected figure then output to a new file with tabs replacing commas.
in the first step, and later matching the different pieces.
If it doesn't match, display the line with an option to exclude which commas should be converted to tabs and ouput based on that selection.
The basic problem is, that the conversion to a simple CSV format is not bijective - there's simply no direct mapping back from the CSV file to the original data.
You might have some records, where no field value has an embedded comma.
This should then give you a tab separated file with commas in some address fields.
(The 'space' heuristic is a good one to start from anyway).
Proceeding stepwise in such a manner should allow you to transform your data into a format that is better suited, e.g.
I've tried a few tools and brainstormed about combining things to fix this, but I figured asking couldn't hurt.
You can still use some heuristics (like "if you have space before comma, it's probably in address") for rows with extra commas, but they are heuristics, and they will miss.
Otherwise you will make things worse instead of better.
Extract these first, even if there few, it's a starting point.
I can only help by writing a shell script which will seek erroneous lines and offer to edit them.
You can try to cut this problem down using some heuristics, but it will need a certain amount of scripting or programming.
It also depends on the platform and tools you have available and - last not least - on your skills.
May be you find a heuristic to distinguish at least some of the "embedded" commas by field separating commas.
Unfortunately, there are so many instances of this (and commas in the second address line) that the whole CSV (~100k rows) is a huge mess.
There is no obvious solution to this problem because if fields that contain commas aren't marked some special way (and you did not mentioned that) it is basically impossible for a computer program to determine if a comma is in address or it is not.
Working from there, you could narrow down the number of columns which need more careful inspection.
Write down, document, keep a record of what you are doing.
In the event that a company has a comma in their name, it (predictably) throws the whole database out of whack.