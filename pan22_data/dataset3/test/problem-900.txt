It's very dependent on the specific situation and the problem you want to solve.
Feature selection performance can be evaluated by the overall performance of learning task for example one can select features with different methods and then use these different feature sets for classification and compare the precision of obtained classifiers.
So, I don't think there's general rule (as always in ML), but this is a case by case problem.
What would be an appropriate method to compare different feature selection algorithms and to select the best method for a given problem / dataset?
I always consider features selection as a step to a final result.
A further question would be, whether there are any metrics known that measure the performance of feature selection algorithms?
Many published algorithms are also implemented in the machine learning tools like R, Python, etc.
There exist some general rules, for example wrapper methods are more flexible and also more prone to overfitting.
Another important factor in some scenarios like some biological applications is the interpretability of selected features and the results, for example  in a clustering problem, meaning of selected features and resulted clusters is a very important measure of performance.
There are several feature selection / variable selection approaches (see for example Guyon & Elisseeff, 2003; Liu et al., 2010):
Hereunder, I somehow mix features selection and dimensionality reduction, which might have some goals and can be confused.