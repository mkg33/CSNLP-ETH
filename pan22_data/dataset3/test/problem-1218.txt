(Here are some examples, https://arxiv.org/pdf/1711.06861.pdf, https://arxiv.org/pdf/1804.04003.pdf)
By setting the problem as an auto-encoder, they can train the decoder by passing the target sequence (equal to the input sequence) into the decoder.
Some papers I have seen use an auto-encoder to train the encoder and decoder components separately.
Is this sensible and what are the pros / cons of doing so?
Î¥ou will have accumulated error propagated and amplified in every new prediction, making your prediction to diverge from the ground truth sooner or later.
The problem is I don't have parallel data between the two styles so I need to train the model in an unsupervised setting.
I am trying to build an encoder-decoder model for a text style transfer problem.
Instead of an auto-encoder, I would like to know if it's possible to train a decoder by feeding its predictions at time, t-1, into the input at time-step t. I would pass the generated output into a classifier to check the style and to obtain a training signal.