So my guess is that they are concatenating the output of the conv layer and the shorcut which would explain the output size.
On the other hand, MaxPooling does keep the input num_filters (though in this case reducing the size).
My guess is that the input to the pooling layer is actually a concatenation of 2 of the previous layer's outputs.
In this case it would have input shape (batch_size, 128, s).
Question RE this research paper if anyone has experience with CNN's, pooling & skip connections: https://arxiv.org/pdf/1606.01781.pdf
Generally, all conv layers have a number of filters, thus determining the output size (num_filters, size) regardless the inputs.
However, my guess is that this is not due to concatenating 2 previous layers (I don't really see an specific reason to do this here) but because of concatenating the ResNet shorcut.
However the output from the pooling step has shape (batch_size, 128, s/2).
However, the paper does not appear to clearly specify at what outputs are concatenated...
In the paper, note that the num_filters is doubled at the output of all convlayer except for the one that does not keep the ResNet shorcut (last 512 conv layer).
How can a pooling step increase the number of parameters on axis 1 from 64 to 128?