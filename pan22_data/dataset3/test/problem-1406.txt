Because when you test the visibility of a fragment during rasterization you already know (or can easily calculate) all the details that raytracing will give you.
But it is the additional lighting beyond the first hit where raytracing is helpful.
However each ray is only a small part of what contributes to the lighting in a scene, so you need a lot of them per fragment.
As others have mentioned, there are pros and cons: Some advantages include that rasteriser can be extremely efficient at handling the first set of intersections and, for shadows, you don't spend effort pre-rendering portions of shadow maps that may never be visible in the final render.
If I'm not mistaken, you can make a hybrid renderer which lets a rasterizer solve the visibility problem, then shade all of the projected shapes with a raytracing algorithm.
Those two properties make raytracing not very atractive for real-time rendering without some dedicated acceleration.
On the downsides, you are typically constrained to the standard pinhole camera model.
Indeed there are rendering systems that can do this: Imagination demonstrated hybrid rendering on their "Wizard" Ray Tracing/Rasterisation system.
So doing the ray-triangle collision calculation is redundant.
However, it limits greatly what you can do - depth of field, motion blur, participating media, refraction, only basic camera models, no adaptive or custom sampling, etc.
Scroll down to "Making ray tracing happen" and there is a screen grab of a hybrid demo.
And each ray has relatively large cost to compute.
It seems that this isn't commonly done however, what's the reason for that?