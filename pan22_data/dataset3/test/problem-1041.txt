I say this because they seem so structured and tag-like.
Actually the way you already tried is a valid way to do it.
two titles with similar terms or meanings might be in two different classes.
Batmobile has 100% High labels, Paint has 50% and Batcave has 0%.
Count words/phrases based on Bayesian approach (just count the frequency of words happening in each class) and normalize them to probabilities.
It means using something like Tf-IDF approach but this time taking classes into account.
Have you tried the boring, straightforward approach?
Means that semantics are not correlated with classes i.e.
How can you predict that based on this most probably "Write Comments" is High or Low?
where $N_{high}$ is the number of times the word $w_i$ appeared in class High and $N_{low}$ is the number of times the word $w_i$ appeared in class Low.
The constant $1$ is just a smoother to avoid a zero denominator.
Problem 2: The "Tactic Titles" Are Just Fixed Tags (Maybe irrelevant to score)
How would you want to know that "Partner Launch" is low priority but "Review Comments" is high if you have no meta-information form the organization behind it?
i did not consider the normalization which is better to be considered, etc.)
For instance if you get meta-data about which department is doing any of this tasks, you may end up with a more correlated/causal set of features.
Of course this is a simple score and you can improve it by modification (e.g.
Get a list of all words and count how often they occur with a high or a low label.
If you use it be careful about overfitting as you are learning features in a very implicit and hand-crafted way.
The long story short: Maybe the information is not in TEXT but some other aspects.
Now you have most likely data given classes and the probability of classes.
To avoid overfitting you may prune your found words based on a min_count or max_count limitation.
The words that occur more than once are Paint, Batmobile, Batcave.
A new data comes and $P(C|D)=P(D|C)\times P(C)$ is calculated easily.
(Exclude words that occur only once or twice, and also the words that occur very often).
.If you argue that Machine Learning is supposed to learn that latent variable behind, then I would say yes!
Another idea could be using information theoretic approaches.
Either you have the label of the new data in the dictionary so you just extract the class, or you don't have it so you don't know about it!
Here you are going to look for words/n-grams which appear a lot in desired class (High) and not a lot in the other class.
For prediction you can also simply use naive Bayes.
but please note that maybe that latent phenomenon is not correlated with Text.
For instance in text analysis using NLP techniques you mostly deal with texts in which "Review Comments" and "Reviewing Comments" and "Comment Review" are supposed to be considered the same.