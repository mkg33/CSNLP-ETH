In a similar situation, -after trying some alternatives- I had to build a language classifier in front of all learning and classification steps.
I suggest to acquire some multilingual training and test data and do experiments whether it is better to train on mixed languages or doing language detection first and use monolingual models.
I'd prefer not to have to maintain different training data for different languages if possible, so my question is:
The fact that you use some NLT techniques (like the stemming mentioned) suggests the latter path.
My educated guess is that you should be able to detect most spam without going this far.
The current approach is the normal stemming, TF-IDF and LSA for pre-processing, then a two-level classifier: an ensemble of normal classifiers that's used as input for a linear classifier that will make the final decision.
What's a reasonable approach to doing text classification for multiple languages?
The company I'm working for runs social network sites, and we're classifying messages sent one-to-one as spam or not spam.
For semantic features you can use multilingual word embeddings, so your content can be treated by the same classifier, regardless of language.
Example of a message that should be classified as spam:
Some discriminative features like presence of URLs, frequency of proper punctuation and spelling mistakes translate easily.
The issue is that it's been trained exclusively on German training data, since most of the networks we operate are local to Germany.
A spam message in our cases usually contains obfuscated links and subtle references to other websites.
Soon we will need to support multiple languages for this, since the one trained on only German will often think messages in other languages are spam as well.