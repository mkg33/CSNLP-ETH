Model parameters are the properties of the training data that are learnt during training by the classifier or other ml model.
Consider for example the task of clustering, specifically Gaussian Mixture Modeling (GMM).
The source of confusion stems from the use of $M_{\mathcal{H}}$ and modification of hyper-parameters $\mathcal{H}$ during training routine in addition to, obviously, the parameters $\hat{\Phi}$.
1) Weights or Coefficients of independent variables in Linear regression model.
Model parameters differ for each experiment and depend on the type of data and task at hand.
In this scenario, the hyper-parameter, $N$ becomes part of the set of parameters $\Phi = \{\bar{\mu}, \bar{\sigma}, N \}$.
Typically, cluster validation is used to determine $N$ apriori, using a small sub-sample of the data $D$.
Model hyper-parameters are used to optimize the model performance.
Since, model $M$ and loss-function $\mathcal{L}$ are based on $\mathcal{H}$, then the consequent parameters $\Phi$ are also dependent on hyper-parameters $\mathcal{H}$.
$D$ is training data and $Y$ is output data (class labels in case of classification task).
For example in case of some NLP task: word frequency, sentence length, noun or verb distribution per sentence, the number of specific character n-grams per word, lexical diversity, etc.
The hyper-parameters $\mathcal{H}$ are not 'learnt' during training, but does not mean their values are immutable.
In machine learning, a model $M$ with parameters and hyper-parameters looks like,
Model hyperparameters, on the other hand, are common for similar models and cannot be learnt during training but are set beforehand.
The parameters set here is $\Phi = \{\bar{\mu}, \bar{\sigma} \}$, where $\bar{\mu}$ is set of $N$ cluster means and $\bar{\sigma}$ is set of $N$ standard-deviations, for $N$ Gaussian kernels.
The 'parameter' $N$ is not explicitly involved here, so its arguably not 'really' a parameter of the model.
The objective during training is to find estimate of parameters $\hat{\Phi}$ that optimizes some loss function $\mathcal{L}$ we have specified.
There are potentially several motivations to modify $\mathcal{H}$ during training.
That is, each of the $N$ Gaussian kernels will contribute some likelihood value to $d$ based on the distance of $d$ from their respective $\mu$ and their own $\sigma$.
Hyper-parameters are those which we supply to the model, for example: number of hidden Nodes and Layers,input features, Learning Rate, Activation Function etc in Neural Network, while Parameters are those which would be learned by the machine like Weights and Biases.
The important point of distinction is that, the result, say label prediction, $Y_{pred}$ is based on model parameters $\Phi$ and not the hyper-parameters $\mathcal{H}$.
Nevertheless, it should be pointed out that result, or predicted value, for a data point $d$ in data $D$ is based on $GMM(\bar{\mu}, \bar{\sigma})$ and not $N$.
Summary: the distinction between parameters and hyper-parameters is nuanced due to the way they are utilized by practitioners when designing the model $M$ and loss-function $\mathcal{L}$.
where $\Phi$ are parameters and $\mathcal{H}$ are hyper-parameters.
The distinction however has caveats and consequently the lines are blurred.
2) Weights or Coefficients of independent variables SVM.
Typically, the hyper-parameters are fixed and we think simply of the model $M$, instead of $M_{\mathcal{H}}$.
I hope this helps disambiguate between the two terms.
An example would be to change the learning-rate during training to improve speed and/or stability of the optimization routine.
Herein, the hyper-parameters can also be considers as a-priori parameters.
Model Parameters are something that a model learns on its own.
However, I could also modify my learning algorithm of Gaussian Mixture Models to modify the number of kernels $N$ during training, based on some criterion.
You may have intuitively recognized the hyper-parameter here.
A typical set of hyperparameters for NN include the number and size of the hidden layers, weight initialization scheme, learning rate and its decay, dropout and gradient clipping threshold, etc.