I think it has to do with the information density on typical platters.
When the disk's data is sufficiently "broken" then the disk will give up and you've lost the data.
and if a sector is failing then the drive will retire that sector.
If there is a failure with the read head itself or some other mechanism than the bits on the disk, then you might be hard pressed to actually detect that.
I know that the new file system, ZFS, actually reports when it finds bad sectors on your hard drive.
So, I guess the moral of the story is use something like smartmontools to monitor the lies.
Maybe the problem isn't so much the hard drives themselves as lack of a modern enough file system.
As far as I know, typically you'll see that errors can be detected (using a type of hash check?)
SpinRite tests the written data, and optionally refreshes or even recovers it.
It's only when the drive runs out of spare sectors that the typical OS will notice, and at that point your data is at risk.
Modern harddisks use SMART but this only works up to a point.
The designers assume that there will be flaws in the platters, and design the firmware around that - if a sector fails, it's automatically re-written and no data is lost.
Hard drives do detect bad sectors sometimes, and re-map them to good ones, but it's clearly not enough.
There are tools like GRC's SpinRite that can look past SMART - and these can sometimes rescue your data even when hope seems lost.