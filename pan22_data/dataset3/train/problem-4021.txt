So that you can avoid such type of problem for unseen data.
Now in order to  handle variable categories in OneHotEncoding, as mentioned above by @OMG, always use maximum possibility of values while training your model.
When preprocessing this data into an appropriate NN input, OHE is recommended because it doesn't assume any order of the categories.
["Man", "Woman", "Diverse"] has no order to it so having one input that represents them all within one dimension makes little sense.
How would one process this input to feed to the NN without hard-coding all possible categories and still be able to handle varying numbers of categories in the input set?
A solution is set a maximum length for the number of categories and fill the future categories in order to see them in your data.
When using cross validation, the dataset often gets split into a lot smaller subsets.
You should save all the pre-processing models that you have used for preparing the training data, so that  you can apply these to unseen data for prediction.
It can also lead to different categories taking different positions in the NN.
Hence, that part of the categories which are not seen in the sampled data would be set to zero.
Otherwise you will always face such type of error as you have mentioned.
When using OHE of sklearn, the input set is used to determine the dimensionality.
This can lead to unpredictable column counts of the networks data input.
let's assume an input dataset that is a mix of categorical values and real values.
OneHotEncoding, according to me its a part of pre-processing activity.
But sometimes it's not feasible to determined all the possible classes in advance: