It's quite feasible to allow any 3D object to have a 2D widget object hierarchy attached to it which renders into a buffer and then which is displayed on the 3D object as a texture.
This can be used for in-game computer terminals, floating partially-3D HUDs (like in Dead Space), or so on.
The neat thing about integrating your UI widgets into your in-game objects is that you can use other components in your UI.
Using game objects and components for widgets does make some sense, though you should still definitely have a separate hierarchy of UI elements outside of the game scene itself that walks the element tree and renders appropriately.
Many games have completely separate UI systems that don't use game objects at all.
UI is often made of a great many little tiny sprites and the like; having each object in that hierarchy draw itself is terrible for performance.
For instance, you could attach a 2D physics components, or allow drag-and-drop of 3D objects onto 2D receptacles, or let your engine be trivially used for both pure 2D games and 3D games.
The rendering of the 2D nodes should - just like regular game object rendering - be handle by a separate render graph in either case, though.
You can also then have special "fullscreen roots" to a 2D hierarchy to use for an overlay HUD or main menu.
Note that aside from input, UI is just 2D rendering.