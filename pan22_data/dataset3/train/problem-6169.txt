The problem is that every time a bus wire is driven high or low, that transition must propagate to all other devices on the bus, and this takes quite a while as the transition literally bounces around for quite some time, reflecting off of every 'impedance discontinuity' anywhere along the trace.
The speed of PCI is limited by the time required for the voltage on the bus wires to stabilize on every clock cycle.
This includes the low impedance pin driving the bus wire to Vcc or GND, the high impedance pins of every other connected device, the pins of any unpopulated PCI connectors, etc.
For example, 32 bit PCI running at 66 MHz provides 2.128 Gbps, which is about the same as the 2 Gbps provided by a single lane of PCIe gen 1.
PCI express is designed to run much faster, and part of that required moving away from a shared parallel bus to dedicated point to point links built with proper high frequency design in mind.
Coupled with embedded clocking to eliminate issues with time delay differences between different 'lanes', and the resulting interface can provide orders of magnitude more bandwidth.
This takes quite a few nanoseconds to calm down, and as a result places a hard upper bound on how fast the interface can run.
These days, 16 lanes of PCIe gen 4 provides 256 Gbps, a figure only possible due to the modern extremely high performance serial links.
That means controlled-impedance transmission line with a single impedance-matched transmitter at one end and a single impedance-matched receiver at the other end along with properly designed controlled-impedance connectors to minimize reflections.