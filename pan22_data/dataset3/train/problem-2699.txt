To this problem, POSIX provides us with a wonderous set of magic tools, which can be used to compose concise -- and very powerful -- preprocessing scripts.
Given these concerns, I'm very suspicious of any method or process that treats data cleaning in a superficial, cavalier or full-automated fashion.
Yet, there are lots of API's that puts you halfway through with the problem.
Are missing or erroneous values randomly distributed or are they concentrated in any way that might affect the outcome of the analysis.
One reason that data cleaning is rarely fully automated is that there is so much judgment required to define what "clean" means given your particular problem, methods, and goals.
On the other hand a column "monthly savings" could reasonably contain negative values.
There is an entire course devoted to this in Coursera.
For example, for people who deal with data coming from social websites (twitter, facebook, ...), the data retrieval usually yields files with very specific format -- although not always nicely structure, as they may contain missing fields, and so.
Such API's are sometimes made public by their writers, such as IMDbPY, Stack Exchange API, and many others.
There is nice R package knitr which helps a lot in reproducible research.
From my point of view, this question is suitable for a two-step answer.
You can check my assessment prepared for Reproducible Research course at Coursera.
There are many devils hiding in those details and it pays to give them serious attention.
If you spot errors, you should place some automated process which can spot  these kinds of errors in subsequent months, freeing your time.
Notice that this could be the analysis itself, in case the goal is simple enough to be tackled in a single shot.
The first part, let us call it soft preprocessing, could be taken as the usage of different data mining algorithms to preprocess data in such a way that makes it suitable for further analyses.
: Your column "Income" might contain negative values, which are an error - you have to do something about the cases.
This allows for building up API's to select specific data in a very straightforward and reusable way.
In such cases, it is usually better to use scripting languages (other than shell ones), such as Python, Ruby, and Perl.
Of course, not all research could be fully reproduced (for example live Twitter data) , but at least you can document cleaning, formating and preprocessing steps easily.
If you will always deal with the same data format, it's commonly best to write an organized script to preprocess it; whereas, if you just need a simple and fast clean up on some dataset, count on POSIX tools for concise shell scripts that will do the whole job much faster than a Python script, or so.
This is true when there is explicit representations of uncertainty and ignorance, such as Dempster-Shafer Belief functions.
Also, some data analysis methods work better when erroneous or missing data is left blank (or N/A) rather than imputed or given a default value.
Since the clean up depends both on the dataset and on your purposes, it's hard to have everything already done.
You might want to go over the techniques they mention and the important part is to know when to use what.
Where you can and should automate is repeated projects.
Finally, it's useful to have specific diagnostics and metrics for the cleaning process.
Such diagnosis often requires manual analysis and inspection, and also out-of-band information such as information about the data sources and methods they used.
The second part, the hard preprocessing, actually comes prior to any other process, and is may be taken as the usage of simple tools or scripts to clean up data, selecting specific contents to be processed.
In case simple the source file has too many nitty-gritties, it may also be necessary to produce a bundle of methods to clean up data.
Such errors are highly domain dependent - so to find them, you must have domain knowledge, something at which humans excel, automated processes not so much.
From the magic set, one may also point out grep, sed, cut, join, paste, sort, and a whole multitude of other tools.
I think that there is no universal technique for "cleaning" data before doing actual research.
In these last two cases, the data looks good by outward appearance but it's really erroneous.
By doing reproducible research, if you used cleaning techniques with bugs or with poor parameters/assumptions it could be spot by others.
It may be as simple as imputing values for any missing data, or it might be as complex as diagnosing data entry errors or data transformation errors from previous automated processes (e.g.
It's always better to do it manually instead of automating since each dataset has its own problems and issues and not all steps are applicable to all the datasets.
It's useful to test the effects of alternative cleaning strategies or algorithms to see if they affect the final results.
For these cases, a simple awk script could clean up the data, producing a valid input file for later processing.
On the other hand, I'm aiming for doing as much reproducible research as possible.
About automatic cleaning: You really cannot clean data automatically, because the number of errors and the definition of an error is often dependent on the data.
So, answering the question: are there any best practices?