For the detailed algorithm of how it works, read word2vec, doc2vec
Feeding that distribution into the naive bayes algorithm as features.
Gensim provides a nice python library for word embeddings both word2vec as well as doc2vec models.
It isn't full proof, but it might help you get better predictions.
You should, probably, remove all non-relevant words from reviews, and that includes not only stop words, but all person names (since they are less of a sentiment marker except, maybe, justin bieber, who is a really negative marker to anything :) ).
My guess is that the LDA does a decent job of picking up on sarcasm.
It is something of a hack but it seems to work really well for the limited sentiment analysis that I have done.
Word embeddings is gaining its popularity in NLP, due to its interesting characteristics of vectors generated.
Mainly the accuracy depends upon pre-processing steps, features extracted and the learning model used.
These are some of the things that I would try at the very least.
I have also had a lot of success using latent dirichlet allocation to predict the distribution of topics for a particular sentence.
Pre-processing steps normally includes removal of stop words and that is fine.
There are lot of learning models from naive bayes, svm to neural network models.
The accuracy of it depends upon the dataset used and the features generated and so each models need to be tested under trial and error method.
Given that exact case, I would assume that you are getting negative decision due to names, mentioned in the review (in your training dataset actors were more often met in negative reviews).
I'm assuming that you are using bag of words, you can try adding bigrams and/or trigrams (or really any other arbitrary n-grams) to your vocabulary.