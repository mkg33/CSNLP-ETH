But, Big-table approach is not the end of story either, it is still the balance between time and space, in terms of PB volume data to manage, some developer also began to seek the balance back to space efficiency, that is why there are works done to normalize some data in BIG-TABLE like structures.
I've been pretty good with designing schemas (IMHO :) and normalized only to remove unnecessary redundancy but not where it impacted speed i.e.
I agree that memory constraints did bear a direct correlation to normalization (there are other benefits too :) but in today's time with cheap memory and quad-core machines is the concept of DB normalization just left to the texts?
it is still that debate you want time efficiency or you want space efficiency.
At the time Relational Database theory is brought up, the disk storage is expensive, people obviously don't want to spend that much money on this, that is why at that time relational databases are the one who stands firm amid adversities
So obviously we can tolerate more redundancy compared with old days, this is also WHY the BIG_TABLE approach appeared.
I totally agree that ORM frameworks are merely approaches getting things done, but I don't think it is these frameworks causing the de-normalize trend.
(Note: I'm not talking about datawarehouse's star/snowflake schemas which have redundancy as a part/need of the design but commercial systems with a backend database like StackExchange for example)
As DBAs do you still practice normalization to 3NF (if not BCNF :)?
Just how should one make the case "for" normalization if it's still relevant.
They stipulate that it's just better to have everything as one big table no matter if it has a ton of nulls "todays systems can handle it" is the comment I've heard more often.
With the advent of some ORM frameworks like Ruby's ActiveRecord or ActiveJDBC (and a few others I can't remember, but I'm sure there are plenty) it seems they prefer having a surrogate key for every table even if some have primary keys like 'email' - breaking 2NF outright.
In my opinion, it is still just about balance between normalize & de-normalize.
if joins were a performance hit, the redundancy was left in place.
In a word, the normalization approach is not dead definitely, but compared with the old days it is definitely overlooked.
in order to seeking more time efficiency , the space efficiency has to be sacrificed.
Okay, I understand not too much, but it gets on my nerves (almost) when some of these ORMs (or programmers) don't acknowledge 1-1 or 1-0|1 (i.e.
Is "dirty schema" design good for production systems?
I've been brought up old school - where we learned to design the database schema BEFORE the application's business layer (or using OOAD for everything else).
Now days things are quite different, storage are very very cheap.