Usually, if two samples are very close to each other and they have different labels, one of them is incorrect, or it is a noisy sample.
Now, from your question, you have two parts, the first part control the distance between the margins, the second part controls the error in miss classification.
Overfitting can be more serious if you have noisy data.
If the distance is too small, then you are trying to fit " seperate between" the samples that are very close to each other.
Changinging " increasing" lambda will give more attention to the error rather than the distance.
In fact, many people like Svm, many use it, many understand the idea of svm " maximize the distance between the two margins "subjects to the condition of classifying all the samples correctly!!
The more you reduce lambda the maximum the distance between the margins you can get.
Because you plot the hyper plane between the two margins, if you can plot the margins, you will see how changing lambda is affecting the distance between the margins.
Quite easy but can get a little bit more complicated when we are dealing with none separable data and overfitting problems.
In Svm, you can think about the distance between the margins as an indicator of how much you are fitting the data.
In your question, you mentioned that this parameter is helpful to perform regularization, but why we need regularization?