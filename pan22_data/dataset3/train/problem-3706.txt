And so you may see random changes in weights for each batch.
This doesn't make any sense for conclussion you wrote in this post.
This accounts for some very small reduction in loss.
If that is the case, the loss function would be low throughout the valley but there would be many weight combinations that would all yield similar performance on the training dataset.
One possible explanation is that the model error surface has a big, wide valley.
Once a model has reached an acceptable loss function value there is no reason to continue training, just take any set of weight values.
And this change in loss is updated on the weights using the equation above.
What optimizer do you use and with what parameters, how many epochs and experiments did you run, what is your loss function?...
Due to the random prediction of your model, At every batch, Some points tend to get predicted as correct class randomly.
I've also made some real examples with mnist data which I computed without optimizer and the results are as follows:
Since there is no optimizer in your code, It's technically not possible that "cost/loss function drops drastically and approaches 0".Your model's loss stays in point B forever
When you didn't use any optimizer to optimize the loss as you have said, Technically it's not possible for the cost/loss function to drop drastically and approach zero.
Here you can clearly see the red line(loss) stays on top of the graph forever.
It's only because of the optimizer that the model works with the objective of reducing cost/error or in simpler terms from gradient descent hill analogy, optimizer finds"descending the hill in what way accounts for the most reduction in error".
Your model just stays at the top of the hill forever!!!.
The weights in a model do not need to converge to stop training.