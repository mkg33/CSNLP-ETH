So is machine learning models, when our model doesn't recognize durian, just feed more durian(increase weight as there are few durian in our life compared to apple or banana).
Most of the scikit-learn metric functions have an option to take into account sample weights.
Sample weights change the probability of each sample to be fed into model training.
Just feed more durian and non-durian samples, you might loss the capability to distinguish apple from banana, but you shall survive.
What is the purpose or use case to use this options?
When(and if) you hate durian very very much (assume you would die when you smelt durian), but you can't recognize durian well, how to keep you survived?
As you wonder what's the purpose of doing that, let's imagine what our model will become when it's fed with different weights of samples.
For example, we have 4 samples A, B, C, D with weights $1, 2, 3, 4$, it means our model has two times the probability to fit on sample B than sample A, and 3 times the probability to fit on sample C than sample A.
If you're born in a country full of durian, when you grow up, the word "fruit" means something smells awful, but we all know that most fruit smells tasty.
In particular scikit-learn give weights for each sample so it allows a greater flexibility.
Unbalanced classes is definitely one use case, another purpose is cost sensitive learning, where the cost of misclassifying classes is different for each class.