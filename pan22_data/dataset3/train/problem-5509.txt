This is a bit naive of a calculation, but if something preserves 50% of the original, and you decompress it and recompress it, you got 50% of that -- i.e.
Basically have to write jack of all trades compression algorithm or write a bunch of smallers ones.
In order to go from MPEG2 (DVD) to H.264(I assume that's what you are using) directly, you have to write the program specifically to convert that.
if you're converting between 10 formats, you don't want to have 10 * 9 = 90 algorithms for compression and 90 for decompression to convert from every format to every other format, but you'd rather have 10 algorithms to go to a decompressed format, and 10 more to go down to another compressed format.
This doesn't capture some of the other aspects, but it does explain why recompression (aka transcoding) is rarely recommended; if you need it, try to recompress the original instead, if possible.
Furthermore, you don't want your algorithms to explode; i.e.
Imagine having to write this software: You have to read a compressed file that could have been compressed in a variety of formats, and then recompress it in one of another variety of formats.
It's because Handbrake is free and the compression algorithms only go so far.
And after that, it's your choice how to recompress -- and no matter what you choose, you'll lose a lot more than if you hadn't done anything at all.
The compression is lossy, and in order to understand how to compress it, you first have to decompress it, to see what it's saying!
Neither ones sounds like a winning solutions to me.