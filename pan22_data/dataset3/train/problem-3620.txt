As previously mentioned, the robots.txt spec is pretty simple.
However, one thing that I've done is create a dynamic script (PHP, Python, whatever) that's simply named "robots.txt" and have it smartly generate the expected, simple structure using the more intelligent logic of the script.
However Googlebot supports wildcards, so you could have section like this:
You can walk subdirectories, use regular expressions, etc.
there is no official standard for robots.txt, it's really just a convention that various web-crawlers are trying to respect and correctly interpret.
You might have to tweak your web server a bit so it executes "robots.txt" as a script rather than just serving up the file contents.
Alternatively, you can have a script run via a cron job that regenerates your robots.txt once a night (or however often it needs updating)
since most web-crawlers won't interpret wildcards correctly and who knows how they interpret it, it's probably safe to isolate this rule just for googlebot but I would assume that by now every large search engine could support it as well as whatever Google does in search becomes de-facto standard.