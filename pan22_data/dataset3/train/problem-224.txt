You can simply walk through the values in argv, and produce output from them relatively directly, with no need to store all the inputs and results before printing them out.
The primary shortcoming that occurs to me is in the basic design of the code, particularly the use of a fixed-size array.
zero effort, and will stop these kinds of mistakes.
Although this is a point of some contention, it's (generally) not
You can then compute all the linearly interpolated values based on the previously saved value and the count of intermediate values you need to produce.
make a minor modification, and forget that there are no braces:
is breaking things down into small pieces, writing those small
To overcome this issue you could use two varibles first_valid and last valid to mark the interval of non null values.
the user doesn't input enough arguments, or if a flag is passed;
This reduces storage from O(N) to O(1) with a small constant factor (you basically only store the start value, end value, and count of intermediate values).
had to squint a little bit before I figured out what is was doing.
In a real project this is a very dangerous design choice!
live for longer than they should, and means parts of a program cannot
In particular, when you encounter a "real" number in argv, you can save it and print out.
This will initialize each value of your array to 0.0.
You can feed it a vector (where missing values are represented by 0n) and it should print an interpolated vector.
part of your post about needing to use 0n as a "missing" value, and
There's a little trick for array initialization in C that comes in
performs the interpolation should also be inside its own function:
a lot of the time everything fits neatly into main.
It makes your program harder to follow, lets variables
A lot of older C code uses it (because there was no other option),
You are using the float value -1000.0 as a sentinel to represent null values.
Here, there are a few things that should exist as functions.
This sort of information should ideally be printed out (either when
#define is a very crude mechanism that can cause a lot of headaches.
you start writing slightly larger programs, this starts to become
Prefer to wrap even single line statements in { }.
but it is still a good idea to keep its use to a minimum.
When you start writing programs, they're generally quite small, and
Write all them out, and then repeat the cycle by saving and printing out the second real input you received.
Your program will give unexpected result if -1000.0 is a value coming from the interpolation or from user input.
When doing your interpolation, you only really need the two values you're interpolating.
Prefer to use const instead of #define whenever you can.
This is likely to be negligible if the number of values you process is small, but could obviously be much more significant if the number of values being processed is larger.
Otherwise use a FLT_MAX instead of -1000.0 as a sentinel.
When you encounter a string of 0n inputs, you count them until you get to another "real" number.
...or else (usually preferable) a design that doesn't store all the data at all.
I'm trying to become more familiar with C (been working through the MIT Practical Programming in C course and K&R) so decided to write a simple command-line linear interpolator.
Instead of having to loop over an array, initializing it
It's tempting to omit braces when you're only using a single-line
all too easy for you (or someone else) to come back to the code to
easily be reused (and further along in your programming career, it
seen as very good practice, especially in a language like C. It is
pieces separately, and composing them together into a solution.