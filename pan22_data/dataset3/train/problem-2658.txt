There's more advanced samplers that attempt to deal with this problem as well, such as a sampler that occasionally attempts to sample far from the current mode in the hopes of hitting another mode.
All Bayesian methods that use MCMC techniques face this issue, and it's really not that much of a problem, as under standard conditions, you can put bounds on your MCMC error.
Odds are that if you find a mode, it's probably a pretty good solution, even if not optimal, but that is certainly not insured!
As such, standard sampling methods will typically find a single mode and sample around that single mode.
In regards to making results replicable, if you want get the same solution, you have to start from the same place, with the same random seed.
But this is not one of those standard conditions, as Latent Dirichlet Allocation leads to a multi-modal posterior!
The answers about Gibbs Sampling are misleading, in my opinion.
When you are using mallet, you can fix a random seed using the command line flag --random-seed.
The sampling techniques may well miss other modes, which may in fact have a higher posterior probability than the one found.
The reason for this is that Gibbs Sampling leads to some error due to resampling.
and what can be done to solve this issue or gain more stability across runs?
It does not and cannot remove the fact, that different random seeds produce different topic models.
Often, in these types of problems, several starts from different initial parameters are run and the solutions are compared.
If I'm not wrong, topic modeling (LDA) is not replicable, i.e.
Where does this come from (where does this randomness come from and why is it necessary?)