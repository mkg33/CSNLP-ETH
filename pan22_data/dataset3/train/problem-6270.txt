A more robust method involves forcing all web traffic through a proxy like squid and having a filter like dansguardian running on top.
The Squid website has a good section on how to setup transparent proxying
The issue with this is you'll need a machine to have this installed on - you can't just squeeze it onto a router (unless you have a separate machine as your router).
What would be the best way to block the sites on the linux router ?
There's a bunch of good tutorial about it around the web.
You can find a somewhat old howto floating around.
I've been using Squid as a transparent proxy and using the proxy to filter access to website and as a local cache.
The two together do work well and you'll find plenty of tutorials around the web.
The simplest method is throwing a spanner in the DNS to make sure that sites you don't want showing never resolve.
The added benefit of a proxy is it can cache data at network level to save you some external bandwidth and it can speed things up quite a bit if lots of users are using the same sites.
We would like to block users from accessing sites like facebook and all from our network.
All the connections are through our router(linux machine).
You can do this by using somebody like OpenDNS (there are other services that do the same) or you can keep it local and just edit the local machine's hosts file (which would be enough if your users have limited accounts and no personal computers).
The big advantage of it is that you can rules based on regular expression on the whole URL (meaning you can restrict access to portion of site if you want to).
I have not verified if it's still accurate enough but it was my reference when I learned how to set it up.