The motivation is that all such substrings will be replaced with an identifier.
Notice that (alphabetically) sorting is another way to "cluster" similar strings together.
However, specialized clustering algorithms can perform better.
Since a JSON file can be regarded as a tree data structure, you can use the XBW-transform for trees, which is an extension of the Burrows-Wheeler transform for strings.
I would recommend LSH-like clustering that can be implemented with a single pass over your data.
Burrows--Wheeler transform is a well-known compression algorithm that works by reordering the characters in the string to be compressed.
An alternative is to use zstd, which is (i) faster, (ii) obtains higher compression ratios, and (iii) does not have limitations on the block size (and thus, compresses strings equally well irrespective of input ordering).
Finally, it gets a order choosing a path which has the lowest sum of weights.
Tho optimize compression, one would need to minimize the number of distinct k-mers (substrings of size k) in every block.
It uses an edit distance algorithm to calculate the distance between each word.
I saw an algorithm some time ago which maybe can be useful.
There are a number of ways to define such a similarity; let me describe a reasonable one that works well in practice.
To improve gzip compression, you want "similar" strings to be close in the list.
Thus, it builds a graph which each edge weight is this distance.
Thus, your data will be split into blocks of 64K bytes and every block will be compressed independently.
While the above problem is hard in theory (it is a variant of hypergraph partitioning), there exist fast practical algorithms.