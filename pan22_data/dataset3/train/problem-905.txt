I managed to get the topics but they are in the form of:
Then, you could run the k-means on this matrix and it should group documents that are similar together.
Do you have any ideas of how I could "strip" the textual information from e.g.
Assuming that LDA produced a list of topics and put a score against each topic for each document, you could represent the document and it's scores as a vector:
To get the scores for each document, you can run the document.
In order to apply a clustering algorithm, and correct me if I'm wrong, I believe I should find a way to represent each word as a number using either tfidf or word2vec.
I want to use Latent Dirichlet Allocation for a project and I am using Python with the gensim library.
For instance the way I see it if the word Minister has a tfidf weight of 0.042 and so on for any other word within the same topic I should be to compute something like:
You could use soft clustering mechanisms that will give you a probability score that a document fits within a cluster - this is called fuzzy k-means.
 https://gist.github.com/mblondel/1451300 is a Python gist showing how you can do it with scikit learn.
K-means by default is a hard clustering algorithm implying that it classifies each document into one cluster.
After finding the topics I would like to cluster the documents using an algorithm such as k-means(Ideally I would like to use a good one for overlapping clusters so any recommendation is welcomed).
0.041*Minister + 0.041*Key + 0.041*moments + 0.041*controversial + 0.041*Prime
0.041*0.42 + ... + 0.041*tfidf(Prime) and get a result that will be later on used in order to cluster the results.
a list, in order to do so and then place them back in order to make the appropriate multiplication?