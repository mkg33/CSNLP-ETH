After cleaning the data, I'm using a sparse matrix of Word2Vec features (Feature size is 300)
onevsrest = OneVsRestClassifier(SVC(probability=True) , n_jobs=-1)
I'm trying to train a text data for multi class classification which comprises of 1 Million rows.
Try chi-square or enthropy based classification, i.e.
Why don't you first try with taking TF-IDF in place of Word2vec.
Still I faced the same issue ( Model keeps training for 2 days and gets killed)
Let's say you have a sentence D1 with terms T1,T2 and T3.
lr = LogisticRegression(penalty ='l1' , C=1 ,dual=False , solver='saga' , n_jobs=-1)
This link might help https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/
I was running this model for nearly two days and it got killed automatically
Then you will know if the problem is with memory issues or your word2vec approach.
Generally I have seen SVM perform well with Text Classification tasks.
You can also clusterize your base of documents and/or features (words, terms, vectors).
Converting Numpy Array to Sparse Matrix of (1114220, 300)
They are more robust, precise and humanly relevant than linear models (regressions or SVM).
Another idea is to go through topic discovery so you can identify closely related in significance manner terms.
Or is there any better way to do this type of problem?