Ideally, you should have more than one layer, and use the sigmoid activation function.
Here is an application of lightgbm to the iris data: https://github.com/Bixi81/Python-ml/blob/master/boosting_classification_iris.py
I suggest you check boosting which usually works okay with small data.
Share your results so that we can explore more on this.
Use a sigmoid function or set your last layer to 2 neuron.
A reason might be that you are running a single-layer neural net.
You are using a softmax function with only one neuron in your last layer.
at the output layer sigmoid will be fine because it is a binary classification.
You could also check Logit with L1 regularization: https://chrisalbon.com/machine_learning/logistic_regression/logistic_regression_with_l1_regularization/
With your data, you may have a hard time to fit a NN with reasonable results.
Probably because you are using a single hidden layer which is unable to learn that many good parameters to differentiate between the classes.
It is normal that your output is the same, one property of softmax function is that the sum of output across neurons equals 1 (1 neuron = 1 always).
The reason usually is, that the model is unable to distinguish the two classes well, and resorts to one (often the majority) class.
Pin pointing exact reason would be impossible without looking at the data.
It happens often, that some model only predicts one class.