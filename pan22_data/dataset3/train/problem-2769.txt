Though it's easy to say, it's better to treat the environment that changes as variables, describe/estimate your algorithm's performance base on these variables.
In my opinion there are three different factors that must be controlled:
I think that the best way to describe or analyze experiments (as any other systems, in general) is to build their statistical (multivariate) models and evaluate them.
The same steps as previous case, but compare results across models, corresponding to different environments.
So, answering your question, if you want to compare different experiments and state to what extent your distributed algorithm outperforms others, you should try to replicate as accurate as possible the same environment (data and architecture) where the experiments were carried out.
Analyze it (most likely, using regression analysis).
This is a very good question and a common situation.
Depending on whether environments for your set of experiments are represented by the same model or different, I see the following approaches:
Compare results across variables, which determine (influence) different environments.
If this is not possible, my suggestion is that you test your algorithm with public data and cloud architecture so that you become a referent as you are facilitating the comparison of future algorithms.
Of interest, Experiments as Research Validation -- Have We Gone too Far?.
Define experiments' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints).
The following general answer is my uneducated guess, so take it with grain of salt.