In particular, for inexact algorithms, you can generally trade off accuracy against computational cost, and often actually get still quite good results with only a fraction of the cost of a publication-level run.
the space of images is an infinite-dimensional $L_2([0,1]^2)$ Hilbert space.
So, discuss these points intensively, and make a presentation of how much better the students' results look when run by you in higher resolution on a solidly powered machine.
Most computionally intensive problems can be stripped down to something that's just as instructive but runnable on any normal computer.
When doing numerics one quickly (IMO, often too quickly and without proper thought) restricts everything down to a finite-dimensional subspace thereof, but that's to some degree arbitrary â€“ the subspace just needs to be big enough (and suitably constructed) to still represent the features that are important for the use case.
If the results aren't so great with a stripped-down accuracy, well, that's actually particularly instructive, because this kind of tradeoff is absolutely crucial in real-world applications too.
You may have more processing power available in a research of commercial situation, but you'll also have much higher demands in terms of data amount and needed accuracy.
If necessary, choose only a smaller set of images with clearer distinguishable features, that the algorithm can pick up quickly.
In an image-processing example, you should definitely check how much you can downsample the images that the students should work with and still get somewhat sensible results.
I always like to emphasize that stuff like physics simulations (which is what I work on, and have also done assignments for students) or image classification should in principle, mathematically speaking, have infinite computational cost: an image is a function from continuous 2D space to colour space, i.e.