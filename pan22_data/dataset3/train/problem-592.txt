I ended up creating a Perl script using HTTP::Proxy.
To find google's cache -- use your file manager (thunar / nautilus / whatever ) to navigate to /home/yourusername/.cache/google-chrome/Default/Cache (This is a hidden folder)
Now that you know where all those downloaded images and stuffs have gone lets see if we can get them out of there, and rename them with proper extensions (your gonna have to rename each one to something comprehensible) HERE Goes... -->
I have a little command I use to move that list to somewhere.
Not really what you asked for, but I think it bears mentioning.
The best extension I have seen is actually for Firefox and it's called BatchDownload.
It was pretty easy using the examples, but there is the hassle of changing your proxy settings every time you need to use it.
Example 1: to find JPEG files under /home/yourusername/.cache/google-chrome/Default/Cache
Save it as move_google_cached in /usr/bin or /usr/sbin and chmod 755 move_google_cached
It provides wildcards to generate download URLs (like curl) and will also allow you to do substring matching to all the links on a given page.
I'm sorry, I haven't found an extension, but in linux you could raid google-chrome's cache for the images - if only from 1 site, clear your cache first then load the pages you want to get images from.
cd /home/yourusername/.cache/google-chrome/Default/Cache