Does it need any transforming/normalizing these features?
Although the functions produce these features’ values are different and their maximum value can be 1, there are four features (both in training and test data) that have very low values.
Other features are distributed normally (they range between 0 and 0.9).
So the difference between these two kinds of features is high, I think this causes trouble in learning process for logistic regression.
You should normalise features used in logistic regression, if you are using a gradient-based optimiser (e.g.
We know that it is a supervised method and needs calculated feature values both in training and test data.
they range between 0 and 0.1 and are never 1, even more than 0.1!!!.
When the derivatives vary too much, you will need a lower learning rate to compensate (making learning slower, and more likely to get stuck) or the optimiser will not converge - it may oscillate or start to diverge instead.
Thus these features’ values are very close to each other.
You do not have to normalise features used in logistic regression, but sometimes it can help.
That is because the optimiser will perform better when partial derivatives of the cost function are of similar magnitude in each direction.