There are machine learning algorithms such as: PCA, LDA, cross-correlation, etc.
You can try something like empirical mode decomposition (EMD).
This means pulling out important statistics from your data or converting your data into other formats in order to it being more representative.
The most important part of any machine learning project is to extract the most telling features from your raw data.
And of course even more data if you have  a very large feature set.
The number of output classes makes the necessary amount of data samples to increase exponentially.
As a general rule of thumb, the amount of data that I suggest to have for shallow machine learning models is $10 \times \#features$.
Which will select the features that are the most representative and ignore the rest.
I suggest you use a general machine learning technique such as SVM.
From all the information you extracted from your data not all of it will be useful.
The community generally believes that this is way too little and it is not feasible to expect good results when training a CNN with very little data.
For example if you are using a technique which is sensitive to range, then you should normalize all your features.
Only recently a group published a quite controversial result of training a CNN with 100,000 examples.
For deep models I usually suggest $100 \times \#features \times \#outputs$.
And even more data if you have many output classes.
n your case if you have a signal you might want to extract some information about the temporal and frequency domain transforms.
Data pre-processing goes from your raw data and remolds it to be better suited to machine learning algorithms.
Ask yourself in your signal what are the telltale signs that should tell your model what class the signal was drawn from.
You should try to use some feature extraction techniques, then you should try to use a model better suited for the size of your dataset.
Try these techniques instead and see what results you get: k-NN, kernel SVM, k-means clustering.