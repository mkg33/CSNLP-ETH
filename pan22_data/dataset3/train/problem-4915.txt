I applaud you on your short hash; previous systems I've worked on have taken the sha1sum of the given file and spliced directories based on that string, a much harder problem.
Imagine Java method which runs scan on the folder.
Found out that flat directory performs way better while being way simpler to use:
Both of your proposed options creates up to three inode entries for each created file.
Trying to store millions of files in a Ubuntu server in ext4.
Speaking of that, here's an open source one called smallfile.
Benchmark with your application simulating something like the real workload is ideal.
Certainly either option will help reduce the number of files in a directory to something that seems reasonable, for xfs or ext4 or whatever file system.
The way full scan is done is that for each folder there's one run of the function so JVM will exit the function, deallocate RAM and run it again on another folder.
It is not obvious which is better, would have to test to tell.
To me, this means either option will perform the same.
Also, 732 files will create an inode that is still less than the usual 16KB.
Otherwise, come up with something that simulates many small files specifically.
It will have to allocate large amount of memory and deallocate it in short period of time which is very heavy for the JVM.
It won't show the many small I/Os or giant directory entries associated with very many files.
This is just example but anyway having  such huge folder makes no sense.
In my experience, one of the scaling factors is the size of the inodes given a hash-name partitioning strategy.
The best way is to arrange the folder structure the way that each file is in dedicated folder e.g.