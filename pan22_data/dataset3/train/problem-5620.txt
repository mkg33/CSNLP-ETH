For instance, people often worry about the variance of stochastic gradients $\mathbb{V}[\nabla \mathcal{L}]$ (especially when using score function/likelihood ratio gradient estimators), which refers to how much the gradient (treated as a random variable) varies, either over time OR as a function of the minibatch.
$$ \widetilde{\mathcal{L}}(\theta,D) = \frac{1}{|D|}\sum_{(x,y)\in D} L_s(f_\theta(x), y) $$
Note that $\theta$ (and $f_\theta$) a random variable, dependent on the random variable $D$, computed by some (potentially stochastic) procedure like
               \mathbb{E}_{D\sim P_D}\left[ \xi f_\theta(x) \right] \right) \\
\mathbb{V}[\mathcal{L}] = \mathbb{E}_{D\sim P_D}\left[ \mathbb{E}_{\theta \sim A(D)}\left[ \left(\mathcal{L}(\theta, D) - \mu_\mathcal{L}\right)^2 \right] \right]
where $\mathbb{E}[\xi]=\mu_\xi$, $\mathbb{V}[\xi]=\sigma_\xi^2$, and $\xi$ independent from $x,y$.
A common simplification for this case is to assume that $P_D$ samples from a fixed set of paired values $(X\in\mathcal{X}, Y\in\mathcal{Y}=\mathbb{R})$, related by
Presumably, this looks something like stochastic gradient descent.
This is pretty oversimplified and honestly non-standard, so
where $p\geq 1$ and $R$ is a regularizer, for instance.
For example, one possible definition of $\mathcal{L}$ could be
meaning the variance in the loss as a function of the training set is given by
The comments above discuss (very appropriately) an important special case of this, namely the Bias-Variance Decomposition for squared error.
Essentially, for a fixed (random) training procedure, we compute the variance in the model performance as the training set varies.
Now, let $f_\theta : X \rightarrow Y $ be a learned function parametrized by $\theta\in \Theta$.
\arg\min_\theta \frac{1}{|D|}\sum_{(x,y)\in D} (y - f_\theta(x))^p + R(\theta) , $$
&= \mathbb{E}_{D\sim P_D}\left[ h(x)^2 - 2h(x) f_\theta(x) + f_\theta(x)^2 \right] + \mathcal{V}_\xi + \mathcal{B}_\xi \\
This is the variance in model performance with respect to changes in the training data.
Note that the "classical" bias-variance decomposition (as on Wikipedia) is a special case when $\mu_\xi = 0$.
Suppose $\mathcal{L}: \Theta\times \mathcal{D}\rightarrow \mathbb{R}$ is some loss function defined on the space of learned functions.
    &\;\;\;\;\;\;\;+ \underbrace{\sigma_\xi^2}_{\mathcal{V}_\xi} + \underbrace{\mu^2_\xi + 2\mu_\xi\mathbb{E}_{D\sim P_D}\left[ f_\theta(x) - h(x) \right]}_{\mathcal{B}_\xi} \\
&= \underbrace{\mathbb{E}_{D\sim P_D}[h(x)]^2}_{h(x)^2} + \underbrace{\mathbb{V}_{D\sim P_D}[h(x)]}_0 - 2h(x)\mathbb{E}_{D\sim P_D}[f_\theta(x)] \\ &\;\;\;\;\;\;\;+ \mathbb{E}_{D\sim P_D}[f_\theta(x)]^2 + \underbrace{\mathbb{V}_{D\sim P_D}[f_\theta(x)]}_{\,\mathcal{V}_f} + \mathcal{V}_\xi + \mathcal{B}_\xi \\
We write $x,y\sim D$ to mean $D\sim P_D$ and $x,y\sim\mathcal{U}(D)$ where $\mathcal{U}$ denotes the discrete uniform distribution.
   &\;\;\;\;\;\;\;- 2 \left( \mathbb{E}_{D\sim P_D}\left[ h(x)f_\theta(x) \right] +
$$y = h(x) + \xi, \;\;\; x\in X, \, y\in \mathbb{R},\, \xi\sim\eta(\mu_\xi,\sigma_\xi^2)$$
Variance in machine learning appears in many places, but in all cases it is of course simply an application of the mathematical definition.
If you took the same model, with many different minibatches, ideally you'd hope that the variance of the resulting gradient estimate would be low (sometimes true with large enough batch sizes).
\mathbb{E}_{D\sim P_D}\left[ \mathbb{E}_{\theta \sim A(D)}\left[ \mathcal{L}(\theta, D) \right] \right]
Let $\mathcal{D}=(\mathcal{X},\mathcal{Y})$ be the space of possible datasets and $P_D$ some distribution on that space (so $D\sim P_D$ where $D=(X,Y)\in\mathcal{D} $ is a random training dataset).
We can consider $\mathcal{L}$ as a (random) function of $D$ by first assuming that we can compute $\theta$ given $D$, presumably by some stochastic training algorithm $A$, so that $\theta \sim A(D)$.
We still therefore denote $D\sim P_D$ to mean sampling a random training set (meaning, draw $n=|D|$ random points $x_i\sim X$ and then perform  $y_i=h(x_i) + \xi_i$).
$$\theta^*(D) \leftarrow \arg\min_\theta \mathcal{L}(\theta,D) =
if you are interested in the formalities of this kind of thing, look into Probably Approximately Correct (PAC) Learning Theory and Empirical Risk Minimization, where this is formulated properly (meaning, rigorously).
where $L_s : Y\times Y\rightarrow\mathbb{R}$ is some loss function for specific examples.
This is one possible form, but a specific one is not necessary.
\mathbb{E}_{D\sim P_D}\left[ (y - f_\theta(x))^2 \right]
where we used $\mathbb{V}[r] = \mathbb{E}[r^2] - \mathbb{E}[r]^2$ for any random variable $r$, $\mathbb{E}[uv]=\mathbb{E}[u]\mathbb{E}[v]$ for independent $u$ and $v$, and the fact that $h(x)$ is deterministic (meaning $\mathbb{E}[h(x)]=h(x)$ and $\mathbb{V}[h(x)]=0$), since $x$ is fixed and we are varying the epxectation over $D$ (which only affects $\theta$).
    &\;\;\;\;\;\;\;+ \mathbb{E}_{D\sim P_D}\left[ f_\theta(x)^2 \right] \\
A specific training example is $(x,y)\in D$ with $x\in {X}$ and $y\in {Y}$.
In any case, we get that the expected error of our regressor on a fixed input (where we consider the expectation over possible datasets) is a sum of four terms (two variance and two bias):
&= \mathbb{E}_{D\sim P_D}\left[ h(x)^2 - 2h(x) f_\theta(x) + f_\theta(x)^2 \right] \\
So anyway, I got a little sidetracked, but all of these derivations were merely just to show that the variance in ML is in fact literally a simple mathematical variance.
&= \mathbb{E}_{D\sim P_D}\left[ h(x)^2 + 2h(x)\xi + \xi^2 \right]\\
\underbrace{\left( h(x) - \mathbb{E}_{D\sim P_D}[{f_\theta(x)}] \right)^2}_\mathcal{B_f}+
Our expected squared error on a single fixed pair $(x,y)$ is then