However, since your goal isn't stegnography, you would simply use this to expand your glyph set.
Using a (12, 8) shortened Hamming code on standard graphing paper, you might only fit 187 bytes, encoding only 124 bytes of data.
There was a simple checksum, per line, for error detection.
If you can use custom stationary that has alternating column/row colors or a chessboard-style checkered grid with lettered columns & numbered rows for quick look-ups, that would further increase copying accuracy.
I just wasn't sure if you were intending on encoding megabytes of data like this.
But it could be transcribed very quickly (a slash for 1, nothing for 0) and provide single error correction.
Mostly files were short, less than 100 lines, but I do remember at least one which had 300 lines or more.
I'd recommend, along the lines of Oliver's suggestion, that you increase your data density by borrowing a page from Bacon's cipher, which prison gangs often use to encode hidden messages in missives written in 2 different script styles--usually either upper vs. lowercase characters or print vs. cursive characters, e.g.
Using a standard hamming code like (15, 11) or (31, 26), you get even better efficiency with 137 and 156 bytes of data per sheet, respectively.
Optical Mark Recognition has been used for decades to create machine-readable handwritten forms.
After reading your comments, that sounds more reasonable.
Normally all but the last line was fixed length, so the end-of-line marker served as a check for insertions and deletions.
Though if your main priority is accuracy, I would use a binary encoding + Hamming code.
The Wikipedia page has links to several Open Source versions.
A binary encoding would also be easier to read (aloud) and OCR/OMR.
Tacking on an extra parity bit (13, 8) would provide SECDED (single error correction, double error detection).
Of course, among the first programs transferred this way was a downloader ;)
Schools have long used OMR for testing; the forms are simple to use and to read, and accuracy is typically better than keyboard input.
However, since all glyph counts greater than 15 and less than 256 are essentially the same for a straight cipher of binary data (meaning, you'll still need 2 characters to represent each byte, giving you a data density of 4 bits per character in all cases), you can use the extra 98 glyphs / 12740 code points for error detection/correction.
Doing this, you could have up to 114 glyphs just using print & cursive alphanumeric characters, or 12996 code points using dual-character encoding.
Alternatively, you could use the extra glyphs for additional compression:
Even higher code rates can be achieved, depending on how accurate you think your transcriber can be.
For higher accuracy, commercial manufacturers like Scantron and ReMark can create custom forms.
if odd columns are always capitalized, if the transcriber finds themselves writing lowercase letters in odd columns, then they know they've made an error and can start tracking back to see where it happened.
To further reduce copying errors, I would display the encoded content in gridlines and copy onto graphing paper.
You can also combine an alternating grid layout with alternating character styles as an easy form of error detection.