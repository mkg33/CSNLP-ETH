i would recommended using PL/SQL batch delete, several hours is ok i think.
"We typically end up purging between 10 and 50 million rows per month"
Every month you would split the top partition to create a new partition for the next month's data (you can easily automate ths with a DBMS_JOB).
As such the extra overhead in extra partitions for the index won't matter.
So rather than a query using 5 IOs to return a single record, it was using 10,000 IOs.
If that index is partitioned into 50 segments and you don't have the partition key as part of the query, then each of those 50 segments will need to be checked.
deletion of 50 million records per month in batches of 50,000 is only 1000 iterations.
It will be a significant amount of processing to partition "after the fact", but there's no sense crying over spilt milk - the advantages to doing so far outweigh the costs.
You probably aren't using indexes to identify individual rows, but rather sets of them.
a scheduled task to run the query you posted but remove the loop so it only executes once should not cause a noticeable degredation to users.
If you've got 30+ btree indexes, I suspect most of your time is spent in index maintenance.
We actually spread it out a little more 10,000 records every 10 minutes, which executes in about 1 or 2 second running on our Oracle unix servers.
A standard Btree index, all in one segment, might have to do four jumps to get from the root block to the leaf block and a fifth read to get the row.
when dropping a partition, you leave global indexes unusable, that need to rebuild, the rebuild of global indexes would be a big issue, as if you do it online, it will be quite slow, otherwise you need downtime.
This has an impact on the usefulness of partitioning.
It is a very cheap operation in Oracle to drop a partition (it is analogous to a TRUNCATE in terms of load because that is what you are really doing - truncating or dropping one of these invisible sub-tables).
The logic with 'A' and 'B' might be "hidden" behind a virtual column on which you could do the partitioning:
You mention indexes - well each partition gets its own partitioned indexes too.
Each segment will be smaller, so you may only have to do 2 jumps but you may still end up doing 100 reads rather than the previous 5.
The classic solution to this is to partition your tables, e.g.
One aspect to consider is how much of the delete performance result from indexes and how much from the raw table.
We do about the same volume of records in our manufacturing plant that runs pretty much 24/7 and it meets our needs.
If they are bitmap indexes, the equations are different.
If you have not come across them before, a partitioned table is like several identically structured tables with an implicit UNION when selecting, and Oracle will automatically store a row in the appropriate partition when inserting it based on the partitioning criteria.
Every record deleted from the table requires the same deletion of the row from every btree index.
And with partitions you can also exploit parallel query and partition elimination, which should make your users very happy...
if you do 1 delete every 30 minutes it should meet your requirement.