It means that at the end you want your system to learn a mapping between stimulus and response.
If you are referring to the experience replay, as I mentioned to you, the reward sometimes doesn't come "on time".
In Reinforcement Learning the learning signal comes from the reward which might come delayed, sometimes not at all in that particular trial (but in another yes) etc.
So as you stated, you want to learn to avoid enemies not locations and for this if you sample experience from the buffer the network's training will be more "intuitive".
The reward cannot be the input to your system as it is your learning signal.
Instead we prefer to build a buffer with experience and sample from that.
Deep-Q learning which basically is Q-learning with function approximation is a Model-free RL.
So we need to decorrelate states,actions and sequences and that's why we don't update the network at every single time step.
In Supervised Learning the learning signal comes from the difference between true response and model's response ("teacher's supervision").