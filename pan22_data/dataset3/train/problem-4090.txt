( 4) MySQL-Workbench allows choosing of tables which should be exported in a dump so it wasn't really necessary anymore to move the data to a new DB. )
My goal is to create a randomized but fixed subset of the data in the DB for machinelearning that can be dumped and hashed for reproducability.
In the meantime I found a solution that at least serves the main goal, but it is not a very direct solution and quite slow as I am far from proficiency with RDBMS so if anyone can give a better answer that doesn't involve so many single steps I will accept that but until then, this is what I ended up doing:
So I thought it would be handy if I could draw a randomized subset of the data and move it to a seperate DB.
3) Loop over the newly inserted records and fill the new feature-table with the associated feature-data (this was quite timeconsuming >1h to copy around 1.8M rows out of around 10M rows):
2) Draw random sample from the "main-record-table" and insert into the new record_rand-table.
1) As backup of a randomized fixed sample set was the greater goal, it was sufficient to first store the information in seperate tables in the same DB so I created two tables records_rand and features_rand of the same structure as the originals.
I have a MySQL-DB containing 2 tables "records" and "features" linked by one foreign key in features referring to the primary key "id" in records.
(How to copy entire tables to another DB can be found easily via web-search.)