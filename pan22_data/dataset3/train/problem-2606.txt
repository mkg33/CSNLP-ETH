Generally speaking the best-practise approach is to have a set of test data that isn't based directly on live data, particularly if you work in environments where you might be holding sensitive data such as personal or financial information.
If you make the process customisable then you can use it for both simple testing (a small DB for new dev work) and performance testing (a large DB to make sure your code scales in that direction).
Of course maintaining the scripts to generate this sample data, fixing bugs and adding support for new features as your application grows, becomes extra effort that you need to factor in, and you may still occasionally need to test against "more real" data, so it isn't all "win" unfortunately.
You can also make sure that your sample data includes all the edge cases you have ever come across (or expect to come across) so it can become an important part of any automated regression testing you perform.
For instance in our training and competence recording system you'd want to ask it to "give me a DB with 10 teams spread over 4 branches of around 8 people/team in random statuses (trainee, full comp adviser, ...) and suitable records going back 6 months" or "give me a DB with 1000 teams spread over 600 branches of around 8 people/team in random statuses (trainee, full comp adviser, ...) and suitable records going back 24 months".
This way you have nothing to whitewash and each developer can create a new test set to work with easily & quickly whenever they need to.
The bulk of such a data-set can be created in an automated manner by putting together a set of scripts that generate records following set patterns.