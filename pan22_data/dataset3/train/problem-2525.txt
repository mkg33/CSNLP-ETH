720 lines on a 40" TV comes to 18 lines (or pixels) per inch.
For example, a JPG originally in 2048x1536 in a camera will have artifacts then when resized down and resaved as a smaller JPG, more artifacts will be introduced.
In fact scaling up then downs is how may antialiasing algorithms work, and also how fractional dpi scaling works in many OSes like iOS or macOS.
For example on a screen at 150% dpi the interface will be rendered at 3 times the physical resolution, and then it'll be scaled down to a half the scaled up version, resulting in a 3/2 scaling ratio
So, to answer the question, most people will not notice much difference between the two videos, if at all.
Theoretically, 1080 lines compressed down to a set/monitor that can only read 720 lines (that's what 1080/720 refers to; the number of lines in your picture) should improve the image.
For smaller TVs such as 21" or even smaller, they could just toss away every other pixel in the 2048x1024 signal, thus making an effective 1024x512 resolution which is plenty for a small TV like on a kitchen counter.
I also hate that frames with a lot of motion tend to be very blurried as that is not HD.
Personally I think that 1080 and 720 thing was a screwup in multiple ways.
Depends on your hardware and whether or not it does upscaling/downscaling.
Viewing 1080p video on a 720p screen may cause the quality of the video to be distorted as it tries to work out where to cut pixels.
For example, if you have a 720p display and you play a youtube 1080p I can see the difference when things moves.
Can your eye tell the difference with an object that small?
So a 1080p video may look better on a 720p screen, provided that the 720p is the original version and was not scaled down from 1080p or a higher resolution
But the default algorithm in most situations will produce a sharper image.
1080 lines on a 40" TV comes to 27 lines per inch.
Then again, someone with less trained eyes probably won't see a difference.
So for example if we took a 2048x1536 still image (3MP) and downsized it to 1024x768 (3/4 MP).
Scaling down has another advantage that some algorithms may use subpixel rendering which increases the effective resolution a bit
Also, you have to take into account how the video file was processed (you mentioned MKV, which is a video format created on a computer and is therefore dependant on the quality of the video card) and whether or not top-quality hardware/software was used.
Chances are it won't be noticeable, but there really is no benefit of 1080p videos over 720p while using a 720p screen.
Aside of that 1080p requires more rendering power to display, and the quality increase, if it is there, is just not worth it.
My recommendation would be to go for 720p no matter what.
Why not make it 2048*1024 so that is a perfect 2:1 aspect ratio and cuz it is well known that computers love powers of 2?
The quality is especially good if the downsized image is an exact 50% resolution drop in each dimension.
Real HD would have freeze frames of motion shots just as sharp as non motion frames but they are not even close in quality.
But to answer your original question, it depends on many factors.
Using still images as a basis for moving images, there are competing factors: starting with higher resolution and then resizing down usually yields a better quality image as noise (such as in a blue sky) is cleaned up.
The other competing factor is the more stages of processing (in an attempt to save storage space of the image), usually the worse the image quality gets.