Old slow Sun Ultrasparc (higher latency for a single ping) outperforms new cheap gigabit desktop used as dev server when under 70% 100mb bandwidth load.
If they use the same switch backplane than I would only pay for the increase if the traffic levels exceeded the lower bandwidth.
Lower latency doesn't always correlate to faster delivery.
This will show that there is virtually no difference in latency at the different speeds.
Set the NICs mathching speeds of 10mb, 100mb and 1000mb.
A valid test of latency differences at different speeds would be between two identical NICs connected with a cross-connect cable.
Having a slower NIC, or a NIC set to a slower speed, could actually help a server with concurrent bursts by throttling the input to the server using the switches cache.
Compress the pages, html, css, javascript, images to decrease page load times.
The switch would be the next biggest change with cut-through significantly faster than store and forward for single transmit tests.
I have seen larger changes in ping latency from driver differences than any other changes; bandwidth, switches, offloading NICs, etc..
All packets travel at the same wire speed regardless of max bandwidth being used.
Let's assume 300 byte packets (if you use -s 300 it would be actually bigger because of headers).
The ping needs to run 24hrs and be averaged to see the real latency change.
These three changes will give larger overall gains ongoing than paying for bandwidth not being used to get a .04ms latency reduction.
However, a well designed store and forward switch may overtake the cut-through switch in overall performance under high load.
This depends on the type of switch you're connecting to.
Find out if your provider is using different switches for the 100mb vs. 1gb connections.
Any other traffic may affect the latency of your test.
In the evening under high load the old switch clients clearly outperform the new overloaded system.
During late night, users on the new system are faster.
Ping tests are practically irrelevant for performance analysis when using the Internet.
You mention MySQl in the 20 requests to serve a single page.
If part of your page is streaming you may see drastically different performance between the page and the stream.
A single re-transmit may negate any decrease in latency.
Compare that to sending the data across the internet.
That traffic shouldn't be on the same NIC as the page requests.
Once you add switches with store and forward caching everything changes.
Yes, switching from 100mb to 1gb might be faster (lower latency) due to hardware changes, different NIC, different switch, different driver.
Reduce the number of requests per page to reduce page load latency.
That means to me that your latency is caused by a 90% of several overhead factors.
Testing latency through a switch must be done with only two connections to the switch.
At the end of the day, the latency is going to come down to the forwarding rate on the switch and the particulars of the switch's architecture (where the ASICs are placed on the board, how locking is handled between line cards, etc).
This is not to say that a current tuned server running gb-gb isn't faster than old hardware, even able to handle larger throughput loads.
Desktop has faster gb NIC, faster connection gb-gb, faster memory, more memory, faster disk and faster processor but it doesn't perform as well as tuned server class hardware/software.
0.024ms is approximately the actual time required to send the frame (not counting medium access time nor headers).
Smart switches now do adaptive RTSP type throttling with small initial bandwidth increases and large transfers throttled.
I'm not sure if you will see much difference with Gb.
There is just more complexity to the question of "higher performance" than you seem to be asking.
A probably less than 1ms difference will not on be noticeable a webserver.
Now users that remain on the old switch have higher consistent performance.
You may find a better test would be to perform a host-to-host test using a 100Mbps and 1Gbps link (i.e.
Otherwise you may find that in short time many other users will switch over to the gigabit and the few users left on the old switch now have higher performance - lower latency, during high loads on the switch (overall switch load, not just to your servers).
I mean Cisco), ICMP packets will flow back to the CPU (gag).
Production performance testing is much more complicated than just a ping.
I think you have a fundamental misconception about bandwidth latency and "speed".
Anyway could you try with a really big packet like 1400 byte?
In a ping-pong sequence it would take twice that time (0.048ms), plus the time for the OS to process the query.
High performance switches are computers and under high load behave differently - change in latency.
Apples and oranges example: Local ISP provided a new switch for bundled services, DSL and phone.
The internet has a much lower latency connection, but to match the "speed" of the connection to the shipment you'd have to have recieve at 9.6MB a sec (reference example from this source).
Moving all internal traffic to an internal network will reduce collisions and total packet counts on the outgoing NIC and provide larger gains than the .04ms latency gain of a single packet.
They are quick tests to get a ballpark idea of what's happening on the transport at the moment of the test.
Even then the switch may roll-over logs, adjust packet type counters, update internal clock, etc.. Everything may affect latency.
In your case upgrading to higher bandwidth would allow you to serve more concurrent users but not improve the latency to any individual user.
Usually medium to high-load traffic levels are important, not single ping tests.
In the early days of gigabit I'v seen 10mb high performance backplane switches with lower latency than cheap gigabit switches.
Depending on your page sizes (graphics, large html/css/javascript) you may see initial connection latencies/bandwidth much lower/higher than a large page or full page transfers.
For instance consider a shipment of data on DVDs shipped across the country taking 3 days to arrive.