The advantage to doing this is that the backup process can run using multiple threads and therefore finish faster as well as having much smaller files that can be moved across the network or copied to a CD or DVD.
But sometimes with large databases it takes a long time to run and it is difficult to copy this very large file across the network or even to your backup tapes.
One thing to note is that to maintain the I/O throughput you should keep the writing of your backup files and the reading of the database file on different disk arrays.
Have you ever wished you could get your backups to run faster?
I would also do some testing on your backup target latency without even dealing with SQL Server.
Referencing the post Backup to multiple files for faster and smaller SQL Server files
Well there may be a way, by writing to multiple files.
A 200gb database is not really that large and (in my opinion) shouldn't take anywhere near 20 hours.
I have a 200 GB SQL Server database (including a file stream) which is taking more than 20 hours to backup on a nightly basis.
Creating SQL Server backups is pretty simple to do; you can use the SQL Server management tools or you can use T-SQL commands to issue the backup.
Or do I need to isolate the database which is currently on shared infrastructure?
Or can I make some coding changes to remove file stream data into blobs?
What is the most cost effective way of decreasing this time?
Another advantage to writing to multiple files is if you have multiple disk arrays you can write your backup files to different arrays and therefore get better I/O throughput.
In addition to writing your database backup to one file you have the ability to write to multiple files at the same time and therefore split up the workload.