Read these prepared arrays with the wrapper that trains your model.
I recommend numpy.save for its simplicity and transparency.
I use numpy arrays to work with deep learning images.
This depends on the size of individual images in your dataset, not on the total size of your dataset.
Can someone suggest me how to work with large data for eg.
The memory required for zca_whitening will exceed 16GB for all but very small images, see here for an explanation.
During image preprocessing in Keras, you may run out of memory when doing zca_whitening, which involves taking the dot product of an image with itself.
It provides an interface like NumPy, Pandas, or Python iterators for larger-than-memory operations.
Dask is designed to manage these types of workloads.
Other options are discussed here: Stackoverflow - persisting numpy arrays  .
Do all image preparation and data augmentation during preprocessing and save the result as arrays of one or more samples (up to mini-batch size).
An example of using Dask with TensorFlow can be found here.
To solve this you can set zca_whitening=False in ImageDataGenerator.
I'm worried about RAM during preprocessing and training, while i do batch processing with my GPU
But as the data gets bigger, I'm facing issue with RAM even before training the model when using techniques like data augmentation.