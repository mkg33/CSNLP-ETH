While there is no specific way to share models, it totally depends on the model type, one thing that's used is making a binary of model (pickle in python) and share that file which you can also encrypt in case you are floating around sensitive data.
As a data scientist who recently joined a new team, I wanted to ask the community how they share data and models among their colleagues.
Talking about the stack he uses for the City of San Diego: (08:50) "The way we move data around ... we use Airflow [...] and Airflow is just Python."
In case it is an unsupervised learning model you can directly share the codebase.
You can try using dvc, which stands for data version control.
To store and share the data amongst colleagues, cloud storage is the option we use (s3, google storage) where you can just have a folder structure to store all your datasets.
Listening to the podcast Partially Derivative Episode "Data Science On The Silicon Beach" the host interviews Maksim Percherskiy, Chief Data Officer for the City of San Diego.
For models I also tend to send a weights file over to my colleague and share my github.
Currently I have to resort to storing data in some central server or location where all of us can access (which means unix permissions etc).
Percherskiy continues characterizing the data sharing problem in the context of city government.