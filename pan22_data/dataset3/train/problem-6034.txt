In addition, ReLU has an attribute which can be seen both as positive and negative depending on which angle you are approaching it.
The fact that ReLU is effectively a function that is zero for negative inputs and identity for positive inputs means that it is easy to have zeros as outputs and this leads to dead neurons.
In a way, the ReLU does a similar job of what an L1 regularization would do which would bring some weights to zero which in turn means a sparse solution.
A good practice when using ReLU is to initialize the bias to a small number rather than zero so that you avoid dead neurons at the beginning of the training of the neural network which might prevent training in general.
The state of the art of nonlinearity is to use rectified linear units (ReLU) instead of a sigmoid function in deep neural networks.
The sigmoid function becomes asymptotically either zero or one which means that the gradients are near zero for inputs with a large absolute value.
However, dead neurons might sound bad but in many cases, it is not because it allows for sparsity.
This makes the sigmoid function prone to vanishing gradient issues which the ReLU does not suffer as much.
Sparsity is something that, lots of times, leads to a better generalization of the model but there are times which has a negative impact on performance so it depends.