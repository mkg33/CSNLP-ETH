As a very short term bodge to see us through this situation until I can get back and sort things properly I ran up a cron script that restarted mysqld if it became unavailable this worked a little, MySQL won't restart at all without a server restart so that idea is defunct.
Can anyone advise what the issue causing the crashes might be or how to gain more insight?
It looks to me that it is to do with innodb buffer pool but will reducing this help?
Notice that the buffer_pool_size below that line is ignored.
I'm away on leave and mysql on an EC2 (m3.medium general purpose) based server I have The displeasure of managing has suddenly started crashing.
That much space can be allocated for a SELECT that needs a tmp table.
And, keep in mind that user root does not execute the init_connect.
I am still pretty new to db administration and have optimised my.cnf as best as I could understand -  We are running magento which is very demanding.
- the number of processes running now:0 looks significant but I don't know what it is telling me about what has happened.
This happened before when my restart script triggered but this restart was successful.
Error_log from the failed attempts to automatically restart.
A side note is that Yesterday I noticed that there were hundreds of sleeping processes from the magento site and wrote another script that would kill these if they had been around for ages- again a temp fix while I am away.
Do you have other applications (aside from Apache, PHP) running?
I have very intermittent internet connectivity through my phone to restart the whole server (which does resolve issue temporarily for a few hours).
Try the script again after fixing the memory problem