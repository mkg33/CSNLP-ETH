I know the OP asked for only one problem per post, but the RTA (Rewriting Techniques and their Applications) 1 and TLCA (Typed Lambda Calculi and their Applications) conferences both maintain lists of open problems in their fields 2.
Computer processor designs are increasing the number of processing units, in the hope that this will yield improved performance.
But proving that this is the case would require separating some notion of parallelizable problems (such as NC or LOGCFL) from P.
Given a polynomial $P$, we define its $\tau$-complexity $\tau(P)$ as the size of the smallest arithmetic circuit computing $P$ using the sole constant $1$.
If fundamental algorithms such as Linear Programming are inherently not parallelizable, then there are significant consequences.
Are NP-completeness in the sense of Cook and NP-completeness in the sense of Karp different concepts, assuming P $\neq$ NP?
(Unconditionally; we already know that BPP=P assuming pretty reasonable complexity assumptions)
P-complete problems include Horn-SAT and Linear Programming.
These lists are quite useful, as they also include pointers to previous work done on attempting to solve these problems.
For a univariate polynomial $P\in\mathbb Z[x]$, let $z(P)$ be its number of real roots.
Problems that are P-complete are not known to be parallelizable.
The problem is the following: Given an arithmetic circuit computing a polynomial $P$, is $P$ identically zero?
This problem can be solved in randomized polynomial time but is not known to be solvable in deterministic polynomial time.