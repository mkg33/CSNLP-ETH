Are you trying to find the probability of each binary class?
When you say "future data", do you mean future training data or future test data?
Since your data is heavily skewed, you will have a lot of data for one, and not so much for the other.
You want the weights to be $M/N_i$ where $N_i$ is the number of samples in each class $i$ and $M$ the total number of samples.
Most models in python have a method (eg predict_proba() ) that will produce a probability and will let you decide on class based on that probability.
Finally, you could have a final model that takes in as input the discriminator(s) (how certain it is that the data point will fall left or right), and the output of both models.
I would suggest you produce a number of validation plots for this case.
Then, you can build models for each "side" of the mean.
Since your data is skewed, you could also have a discriminator for the median, which might perform better.
Fit a curve on the histogram of that data, and use the inverse as weight.
If it is future training data then yes you may have a problem.
If it is test data then you do not need to worry at all about his effect.
You do not want to do model building on a training set that is different from the training set you would actually use in the productionized model.
If you already know the true distribution of your data.
There are a few things which are unclear so I am going to have to make some assumptions.
For this reason I do not think that you want to have the weighted average be 0.5 but the "probability" represent the true probability.
That final model should smooth out some of the mistakes done by the discriminator(s) and both models.
This is an interesting problem because obviously, you want to train a model that performs well on unseen data, and therefore you'd like to train it on data resembling the one you'll encounter later on.
Please refer to this tutorial http://scikit-learn.org/stable/modules/calibration.html
This is a way to do some sort of pseudo-upsampleing and have the classes be weighted by the number of samples.
What you could also do if you know the true mean of your distribution is build some type of "discriminator",  which guesses whether a data point will fall on the "left" or "right" of the mean.