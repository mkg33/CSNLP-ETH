Then I use a Linux box as my gateway, and force all port 80 traffic to a Squid proxy via iptables, and use SARG reporting to see where people are surfing.
I personally hate content filters, but I can afford to be sanguine about them, since these days I'm always high enough on the tech totem pole to circumvent them.
That will knock out a ton of stuff, and force your users to justify why they need X, Y, or Z.
I'd set up an iptables rule to look for "BitTorrent protocol" and force that through Squid.
only open the ports up that are critical to business (80/443, mail, etc).
you can also run OpenDNS to do a lot of content blocking.
Well, IMHO the best practice (for individual ports/protocols) is to filter EVERYTHING, and then selectively allow things.
You can block bittorrent ports, that can be defeated by tech-savvy users.
if you were tasked with deciding what traffic to filter, then you should look at your company's usage pattern instead, and make decisions based on that, and not on what others are doing and certainly not follow the "deny everything first" foolishness.
Web filtering will have to be done via some sort of content filter (like Websence or similar), and those can be configured to death to block pretty much anything you want to block.
I use only SNMP enabled switches/routers and do MRTG graphs for everything.
It's never a popular step, but it makes sense from both a security and a utilization standpoint.
Add in streaming media (YouTube/etc) although it may be more difficult to detect, depending on the solution you're using.
It's also possible that someone somewhere may be able to give you a legit business case for some of these things (I'm of the belief that one can construct a legit business case for almost anything if one is sufficiently determined) so being able to define exceptions should be a prereq.
To help yourself with that you can start collecting network flows from your border router(s)  during normal (non-filtered) usage and use them to analyze your utilization.