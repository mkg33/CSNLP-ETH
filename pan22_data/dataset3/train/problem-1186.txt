You can read some interesting stuff here and here.
I would never want my network to output (after denormalization) that if the current age was 70, the person would have 50 years left.
It actually comes with many names: survival regressions, event-history models, duration models, etc.
However if I just normalized age through feature scaling, I would have one input node, one hidden node and one output node.
The drawbacks of one-hot encoding is that it generates very sparse matrices.
If survival analysis is used, the current age can be inputted in the model directly in a single input node.
It might be helpful to first take a known distribution such as the Gaussian distribution (which of course life expectancy is not), generate some training data from that, and see if you can approximate it.
Through this augumentation, I'm hoping to model the distribution of current age vs years left to live through the data.
I use it only when it's absolutely necessary, and avoid it anytime I can.
My question then, is that should I skip the one hot encoding, and just scale the age and use for example 50 hidden nodes?
Then just feed it the data you mentioned: for each age $a$ feed it the training data $\{(0, a),\ (1, a-1), \ \ldots,\ (a, 0)\}$.
However, what I think you're going to need is a statistical technique called survival analysis.
A common survival analysis model is Cox regression model which can be adapted to neural networks.
However when I'm 70 years old, I'm expected to live until 83 (13 years left).
I would never recommend one-hot encoding for ANNs.
Neural Networks hate sparse data: gradient descent optimizers give their best on continuous variables, while weight updates do not work very good when there is so little variability.
I'm not sure binning the results would be helpful in this case.
My personal experience as a data scientist tells me sparse data matrices lead to generally bad results.
The task of predicting how many years a person has left to live is called survival analysis.
In my dataset, I have the true age of when someone died, and it follows a distribution.
Now, the best practice of an MLP states that 1-2 hidden layers is enough, and amount of nodes in the hidden layers should be between the amount of input and output layers.
However I never happened to try these models in practice.
Theoretically, because age is continuous as not as distinct nor discrete as say "cat" and "robot" then one-hot is not necessary, but is there any drawbacks if I choose to one-hot encode?
It's a technique invented by biostatisticians several decades ago, that is meant to estimate the probability of the death of a patient, given how much time passed and external variables.
If I was 70 years today, I would probably never activate the node that will tell me I'd have another 50 years to live.
I'm not a neural network expert, but my intuition says that you could try to model it with a network with a hidden layer containing say 100 hidden neurons.
Thus, survival analysis needs a special type of loss function.
There is one specific feature I'd like to discuss: current age.
Somebody developed Neural Survival models, using ANNs to estimate the survival probability of a process.
I'd be able to use 50 hidden nodes through good practice.
My task is to predict how many years a person has left to live using an MLP.
Survival analysis is a type of time to event analysis.
It can be generalized as a technique that is meant to estimate the likelihood of a termination event happening.
We know an MLP can compute any function on a compact support (up to any degree of accuracy).
This class of models (ANN or not) will let you treat age as a continuous variable.
One of the best books ever written on the topic is this one (it's written for social scientist but it's super useful for anybody.)
This is realistically not enough, I'd expect that I need a lot of hidden nodes.
Example: When 0 years old, I'm expected to live until 70 years old (70 years left).
It was later applied to many fields of science that have nothing to do with medicine.
Mathematically speaking, it is estimating a 'survival probability' of a process through time.
If I used one hot encoding that represents current age from 0 to 100.
There is plenty of information you can find just googling.
I have thus augmented my data, if someone died at the age of N, then there will be N datapoints of current age(feature) from 0 -> N and years left to live (target) from N -> 0 accordingly.
An appropriate loss function would avoid predictions like 50 years left when the current age is 70.