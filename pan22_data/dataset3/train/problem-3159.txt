I can't back it up with anything, but from my experience it just smells like trouble.
It may actually mean that the drive was indeed disconnected (i.e.
I don't have any physical access to the NAS at the moment, but tomorrow someone will replace the defect HD.
If you replace the bad HD, the system will automatically rebuild the RAID from parity bits.
There might be light indicators telling you which drive is bad; or when the system is shut down, someone can inspect the connectors to verify solid contact.
It could mean the drive failed, the SATA (or power) adapters are not seated properly or the controller that's reading the drive is bad.
"Disconneted" is a vague term, you should probably consult synology documentation to find out what do they mean by this.
If the system sees a problem, you'll be in the same spot you are now.
In one notification email (regarding the worsened condition of system-volume) it says I should restart the NAS, because this will repair the system on startup.
In that case the drive itself should be ok, you can plug it back in and perform an array rebuild.
Drive failure detection techniques vary between vendors, and I do not know how Synology tells if the drive is faulty or not (it probably is based on failed reads/writes, bad block/reallocated sectors counters or something like this) but I would strongly recommend replacing it with the new one.
However, someone really needs to physically inspect the NAS.
As @Tigran Balyuan indicated, 'DISCONNECTED' is a vague term to inform you the NAS couldn't communicate with one of the drives.
Or should I just turn off the NAS until the HD is replaced?
I have set up a weekly SMART fast test, which did not show any errors on last Wednesday.
Usually, if the HD is bad, the system will give you a more specific error; like the HD failed or has bad sectors.
Of course, "disconnected" could mean more serious issues with your NAS: port failure on the backplane, partial controller failure, or some sort of catastrophic HDD failure, when the drive just suddenly stopped working.
If, however, "disconnected" means "failed", then it is not a good idea to rebuild RAID over it (especially RAID5).
No way to tell unless you physically inspect the device, but in most cases (in my experience with this message), the problem is due to the NAS and not the individual HDs.
If it is and you get a more specific error about a particular HD, you can replace the HD and the system should automatically rebuild the RAID on next reboot.
So, if I were to speculate, I would say that someone just pulled the drive.
RAID5 is generally resilient enough to stay functional up to 1 failed disk.
This "failed" drive may still live a long and happy life in some PC, but not within RAID.
If you need anything more fault tolerant, I'd suggest going to RAID1 or RAID1+0; note; going to RAID1 or RAID1+0 is generally more expensive, because you're sacrificing more disks for tolerance.
Restarting your system will give you a quick indication of whether or not the RAID integrity is in-tact.
And finally, I do not think that rebooting NAS with failed soft-RAID (it is a soft-RAID isn't it?)
Today I got notified by email as one HD (out of 4 identical 2TB HDs) in my Synology NAS was "disconnected".
RAID5 just indicates your striping and mirroring configuration.
There's no indication that restarting or replacing a HD will resolve a problem.