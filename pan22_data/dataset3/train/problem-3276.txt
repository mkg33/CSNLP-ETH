Most images on the internet are encoded in sRGB, which is gamma corrected.
When displayed, the monitor will remove that gamma correction.
If you would download a random image from the internet, you will more than likely have an image that is in sRGB and thus gamma corrected.
Images that are saved to a file or that are sent to the monitor, are generally gamma corrected.
If you want to read more about it, you can do so here.
Left is an image that was gamma corrected before being saved to 8 bits and right is an image that was not gamma corrected before 8 bits.
Most color profiles do have a gamma correction, but not every color profile has that (raw for example).
However, our eyes cannot see small differences in bright parts of the image, so all that precision in the bright parts is simply wasted.
We only have 256 values per RGB component to define the color of a pixel.
Gamma correction is a very useful trick to make our images with finite bits, look better.
Before we convert the image to 8 bits, we first gamma correct it so that we allocate more precision to that darker parts of the image than the brighter parts of the image.
The monitor receives its image in the color profile of the monitor (let's just say sRGB), it then decodes the image from sRGB to the actual intensity of the pixels and then displays it.
If we try to save an image without any gamma correcting as an 8-bit image (thus 256 values per RGB component), we end up with having the same amount of precision for dark parts in the image and bright parts in the image.
Our eyes are very good at picking up small differences in dark parts of an image, but relatively terrible at picking up small differences in bright parts.
You can clearly see the lack of precision in the darker parts without gamma correction, but not in the bright parts.
Basically, the monitor receives a gamma corrected image and removes the gamma correction just before it displays it.