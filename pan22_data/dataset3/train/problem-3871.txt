The other powerful approach is Bayesian Optimization Methods https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a It allows to move in parameters values space, taking into account previous results of optimization.
It also might be useful to provide feature analysis.
It might be the reason, why it's impossible to get particular model performance.
Note: it's crucial to consider parameters as a whole, but not just separately (if value $a$ of the parameter $A$ is not the best choice, it doesn't mean, that it won't be the best option in combination with value $b$ of the parameter $B$)
For example in this article https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53 you can find, how parameters can be chosen.
It would be reasonable to check outliers in the data (objects which appear to be inconsistent with the remaining instances).
As you have not mentioned the strategy, which you use for parameters choosing, I'd suggest to attend to it.