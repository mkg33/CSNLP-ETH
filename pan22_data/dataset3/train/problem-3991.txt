You then used downsampling (which is questionable, but I won't get into this) on dataset A to get what you refer to as dataset C. You should then train your classifier on dataset C and evaluate its performance on dataset B.
If what you are referring to is a validation set (i.e.
You have split your dataet into A = training set and B = test set.
Downsample dataset C, train your classifier on C only, then validate on D. Once you have optimized to D, then downsample A, retrain your classifier on A (using the exact same model building process as you did with C and D), and report your final, unbiased performance measure on B. Repeat this entire process if AUC scores are volatile to get more estimates of final model performance.
Since you have unbalanced data, I strongly recommend stratified sampling when creating your validation and test sets.
Then, split A again into C = inner train, D = validation.
The test AUC is the AUC you find after predicting the held out test set (dataset B).
not the test set = dataset B) then what you need to do is as follows: split your dataset into A = outer train, and B = test.