My goal was to build a low cost, low power consumption server, so hopefully the 500W power supply isn't drawing 500W.
A power supply converts energy that comes in as alternate current with a "high" voltage (230 V and 50 Hz in Europe, 120 V and 60 Hz in North America) into the same amount of energy in a direct current mode (i.e 0 Hz) with a very low voltage (3.3 V, 5 V, 12 V) but a higher amperage.
I choose a mid-grade OCZ 500 watt modular power supply for it and it works great.
Yes, a power supply draws power relative to how much is being used.
Simply speaking the higher the watt output the higher the loss due to "overhead".
How much it draws will vary from power supply to power supply.
This especially applies to cheaply built power supplies.
So if your PC hardware is only using 200W, your 500W power supply won't draw 500W.
This is the power your power supply should be able to provide.
You can compare electricity flowing through wires and cables with a liquid flowing through pipes.
So as a conclusion: there is a trade off between cheap and expensive power supplies.
This difference of pressure is the voltage in electricity.
(This strongly depends on how well crafted the power supply is.)
The power supply in this picture is just a kind of turbine that uses one stream of water that has too high pressure but is flowing slowly to produce a second stream with less pressure but flowing faster.
Your system might crash if you need to more power than your power supply can provide.
This is exactly the energy that is emitted as heat from the power supply.
It's the product of voltage (difference of pressure before and behind the mill) and the amperage (amount of water flowing through the mill) that gives the power the machine needs to do its job.
So my question is, does my 500W power supply draw 500W just because thats what it is?
A hypothetical power supply that really draws only that amount of energy from its source that is provided to the "real" consumers, would stay cold (at room temperature).
I want to put the focus on a certain aspect when it comes to price.
The higher the maximum power is that a power supply can provide, the higher is the amount of energy it takes to heat up itself.
As a general rule of thumb the power supply should provide much more power then the device needs.
Or does the power supply only draw as much power as is needed to power the computer components?
A machine that "consumes" energy (like a CPU, a light pulp or an electric engine) corresponds to a mill that is driven by flowing water.
So, simplified: The power supply will draw only as much energy from its source as needed.
It will just draw some extra energy to heat up itself that a less powerfull power supply wouldn't need.
The amount of water flowing through a pipe within a second corresponds to the amperage.
But all in all you should know, that the power supply draws more energy from the net when you need a high amount of power, and it will draw less energy when your system is idle.
I could not find a modular ATX power supply out there that was less than 450W.
The case I choose is a small case but only a full size ATX power supply would fit it.
A more powerful power supply will not damage your system.
It varies according to the power efficiency rating of the PSU, in short - yes, it only draws the current its asked for by the computer, but the efficiency of converting the power from the wall socket to something the computer can use is roughly 80% (ie your PC uses 80W, it'll draw 100W from the socket)
Therefore I start with a small "correction": a "500W power supply" doesn't refere to the power supply "drawing" 500W from the outlet but that it's able to "provide" up to 500W to the device connected to the power supply.
That being said the OPs goal was to build use inexpensive power supply in terms of initial cost (the price) and running costs (energy consumption).
The cheap ones are usually more expensive in operation especially if they are oversized.
But due to certain laws of physics this ideal power supply is impossible to be built.
To make water flow through a pipe, you need to have different amounts of pressure at both ends of the pipe.
This  efficiency rating varies also according to how much power is drawn and the 'size' of the PSU, eg a 1000W PSU will not be very efficient when supplying 50W.
But a too weak power supply can have negative effects.
Look for APF (active power factor) for more details.
For the technical details I would refer you to the other answers, they are quite through.
It means: Find out how much power your system will need (take the maximum that is possible) and add some extra margin to be on the save side.
I recently built a small mini-itx Intel ATOM-based Ubuntu home server.
One mini-itx ATOM board + two SATA HDDs = less than 100W I figure.
When you have the same pressure ar both ends, no water will flow.
Both streams carry the same amount of energy per second, and both carry just as much as the "mills" (CPU etc.)
A power supply is just a transformer or a transducer.