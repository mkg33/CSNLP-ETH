There's no point to do this, if you're trying to save space in a database: hard disks can store hundreds of gigabytes, so a measly 15 columns probably won't make any noticeable difference.
You definitely don't want to do this, if you're going to feed the result into a classifier: most classifiers will perform worse after this transformation.
Ex: 1914 would yield the list [2, 3, 11, 29] which would let you know that the user with the 1914 value has property 2, 3, 11, and 29 but nothing else.
I guess this is less about machine learning algorithms and more about storing and retrieving data.
Optimizations should be guided by data; otherwise you risk implementing complicated optimizations that make little difference overall but add needless complexity and epicycles to your data processing workflow.
By definition of one hot encoding, only one of them may be set at a time.
And if you seriously have big data where you truly need to do some optimization, the very first step is to measure: measure how much space/time you're actually consuming, and what the dominant contributors to that total cost is, and then focus on optimizing those dominant factors.
Sparse data can be stored compactly by compression and sparse formats (which pretty much reduces to above approach).
Just save the additional 15 columns and spare yourself headaches.
But the storage must fit you access pattern or performance will be bad.
3) retrieve data by factorizing the value generated by multiplying unique prime numbers
1) use the first 15 prime numbers as indexes for the 15 categories
I understand this is limited because BIGINTs can only hold the product of the first 15 prime numbers, but would it not still be useful in some situations and save time when searching the database?
2) store data by multiplying the prime numbers of the categories that otherwise would have a value of 1 in one-hot encoding
Rather than creating 15 additional columns full of sparse binary data, could I:
But thrmain question is how do you (plan to) use the data, not so much how you encode it.
The cost of your time to program this up and troubleshoot all of the problems it causes downstream will almost surely exceed the miniscule cost of computation or storage to store the data in the natural format.