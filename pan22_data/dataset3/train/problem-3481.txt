The text is displayed alright to the user, but the DOM objects still contain gibberish.
As in PDF documents there is no requirement that the text in the document must use a standard character set.
It need only use one that has code that map to the glyphs in the embedded font.
They must do this so that more PDF documents are displayed correctly.
You can try capturing a screenshot using the browser's crop tool and then use any of the readily available online ocr sites to convert it to text.
See what happens when you turn Javascript on/ off!
http://www.scribd.com/doc/143886351/OCP-Upgrade-to-Oracle-Database-12c-Student-Guide-vol-1-Exam-1Z0-060
To my amazement, I saw that even copy/ pasting text copies gibberish to the clipboard!
To check out what was wrong, I turned off javascript in the browser and then loaded the same document again.
There is no ability to search within a document, let alone being able to download the same.
And so, it looks like the javascript from scribd somehow decodes the gibberish text and then displays it in the browser.
Using javascript, they load pages on demand in the browser, and so the browser's "save as" feature does not help much.
Lately, I have seen that scribd makes it very difficult for users (free users) to browse through a document hosted on their site.
Now, my question is, even after javascript is enabled, and the text is rendered properly in the browser, if I go and look at the DOM objects corresponding to the text I select, I still see the gibberish text.
So the question is, what kind of javascript hooks/ code is the site using, so as to be able to retain the gibberish in the DOM objects and still render the decoded text?
My intention is not to reverse engineer the algorithm to decode, but to locate where the decoded text is being stored?