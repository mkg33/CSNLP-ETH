"The message reduces our uncertainty as to what state the system is in, thus reducing the entropy (of statistical mechanics) of the system.
One pays a price for information which leads to a reduction of the
I just found the following paragraphs from J. R. Pierce, An Introduction to Information Theory, p.206, which I surmise would also support the thinking that one could practically use a computer and a deterministic program to turn a string A into a string B of higher Shanon entropy:
It is always just high enough so that a perpetual motion machine of the second kind is impossible."
It takes a particular energy per binary digit to transmit the message against a noise corresponding to the temperature T of the system."
"We can regard any process which specifies something concerning which state a system in in as a message source.
However the conceptual backgrounds are important; probability of events (Shannon) and probability of a particle being in one of $N$ states (Boltzmann) under a thermodynamical context.
This price is proportional to the communication-theory entropy of the message source which procudes the information.
It may not be practical yet, but the idea of algorithmic cooling uses the link between these two concepts, and has indeed been experimentally demonstrated.
The reduction of entropy increases the free energy of the system.
In practice the only difference is that Boltzmann entropy deals with a thermodynamical constant $K_B$:
"This, I believe, is the relation between the entropy of communication theory and that of
As has already been answered, Shannon entropy and Boltzman entropy are the same thing, although they are measured in different units.
Such a source has a certain communication-theory entropy per message.
This source generates a message which reduces our uncertainty as to what state the system is in.
There is theoretical work in making the formal link between the two notions of entropy.
This entropy is equal to the number of binary digits necessary to transmit a message generated by the source.
But the increase in free energy is just equal to the minimum energy necessary to transmit the message which led to the increase of free energy, an energy proportional to the entropy of communication theory."
i assume you already know that if $K_B=1$ you have Shannon entropy in a different base.