There are also very good surveys comparing anomaly detection methods for time series and I would strongly recommend to read at least the most recent ones to get an idea of the state of the art methods.
There are already many algorithms that are based on the discretization of a time series and some of them actually convert a time series to words.
Signal2vec (I am one of the authors) includes two steps.
Some of the most popular time series representations are PAA, SAX, BOSS, COTE and most recently Signal2Vec.
The first one is to discretize the time series, which can happen using a clustering algorithm or any other discretization method.
I would also recommend the Matrix Profile, which is very simple to implement and is very robust.
The second step is the model of Word2vec, which can be applied either on each symbol or on words that are constructed by the symbols of the discretized time series.
Thus be discretizing a time series (regarding the values it can take) we can have a sequence in the discrete space.
In general the domains of NLP and time series are very similar in the sense that they are both sequential data.
As far as anomaly detection is concerned, you can use any of the abovementioned time series representations.
The main difference is that text is discrete, whereas the values of a signal belong to the continuous space.