Using the 16 x Nvidia k80 should be a lot quicker than using a single Titan X, but how much quicker is hard to say.
TensorFlow does utilize multiple GPUs very efficiently if using an appropriate script e.g.
SLI doesn't seem to work with CUDA so am I stuck with 1 GPU at a time?
https://www.tensorflow.org/tutorials/deep_cnn/#training_a_model_using_multiple_gpu_cards
Amazon offers GPU instances like p2.16xlarge with e.g.
Now I was wondering if TensorFlow utilizes multiple GPUs efficiently or would I be just as fast using a desktop with a single Titan X?
We need to train large networks using TensorFlow that take several days to complete on a GPU.
The workload is divided up and distributed across the GPUs, and they have taken into account things like the transfer of data between GPUs being relatively slow by getting the CPU involved to help compensate.
Those GPU instances are quite expensive so I'm looking to build a machine myself (Linux based).
If you are happy for it to take longer, then obviously don't spend the money - it is up to you whether the time saving will justify the cost.