But, and the point is, you don't need to start your medical training with ideas from 100 or 1000 years ago, even though many of the things learned then are still valid today.
Recently, I taught Introduction to Biological Computing and used Python in the course and that experience prompted me to think a bit about why not use it in PF for CS students, but No!
I was one of the worst coders of the class and despite hating programming (to bits, in fact bytes) I found C# and Java pretty easy due to the effort poured in by our teachers in C and C++.
Those two groups of languages cover much, though not all, of the "higher level thinking" in programming today.
If you examine a few statements in a low level language, like C, they certainly seem to be easily implementable on a Von Neumann architecture.
Somebody who wants to make video games should probably take a different path than somebody who wants to build a website for their home business, or somebody who wants to build an Android app.
Not that it is impossible, but it is a harder climb.
That may have been true at one time, but isn't necessarily true anymore.
Every programmer needs to know something at least of a whole range of abstractions.
I got a chance to teach Operating Systems to students of category 2 and needless to mention how badly they failed at comprehending basic system programs written in C++ (not even C).
Moving from C++ to Scheme or Standard ML, for example, requires a completely new way of thinking about programs.
But generally speaking, you should start with something engaging.
Nor do you need to start your education at a low level.
Wherever you start learning you will eventually, after totally grokking that language, want to go to other levels.
The view of a C program is very different from that of a Java program.
The difficulty in moving upwards in the abstraction scale is that it is often difficult to give up the low-level constructs that you have become used to for others that are more appropriate in the new language.
Actually programming is not about using low-level devices, it is about learning to make things with what you have at hand.
I think in terms of reference and message, not in terms of pointer and function call and don't care if they are similar or distinct.
Certainly compilers take this view, but humans don't need to.
In a higher level language (Java), polymorphism can be implemented more directly using certain design patterns (Strategy/Decorator) and helper objects.
As far as I see among my students, printing "Hello" 10 times, summing the elements of an array, or concatenating strings doesn't really fall in that category anymore.
Different languages offer different kinds of abstractions and different levels of abstraction.
You don't want students bogged down in understanding the heap when they are trying to master stuff like recursion, looping, conditional logic, and variables.
But rabies is nearly always fatal, so it is unethical to set up a scientific experiment in which the control group gets Pasteur, but the experimental group gets (only) a vaccine.
I just know that it gives me access to an object so that I can send messages to that object.
You may not be able to discern the precise type of an object without that trace, and a distinct execution of the program may take a different path.
It works, but is a dreadful process for the patient.
For example, polymorphism exists in most languages.
It allows you to learn the fundamentals, then OOP, and allows you to "graduate" to other languages like Java or Javascript, or even C or C++ if you really need to.
On the other hand, if a program describes what is to be accomplished rather than how to do it, an optimizer has many more options for coming up with both a high and low level set of strategies for execution.
It took me more than 3 lectures to explain them only the syntax of pthread_create() since they had no background of pointers, function pointers and even passing by reference and how could they given they begun programming with Java.
In fact, thinking at the lower level can lead me astray.
Compilers today can execute tens of millions of instructions to globally optimize programs, something that a low level programmer can't do, and something that low level coding often inhibits.
Each programming language that you learn is built on a set of ideas that encapsulate what a program should look like.
With multi-level cache (both data and instruction) and multi-processing on (say) graphics processors, the relation between the programmer notations (the program) and what is executed gets farther and farther apart as time goes on.
You don't, then, progress to leeches and bloodletting, following an historical trail.
Processing is built on top of Java, but allows you to ignore a lot of the boilerplate until you really need it.
Once they've learnt the basics (variable, loops, problem decomposition) they can learn low-level things.
One misconception that people often have about high level languages is that they can only be understood in terms of some (supposed) implementation in a lower level language.
Obviously doing Python after C is very easy, but its not other way around.
Based on your history and preferences, you have a particular view about what it means to be a programmer.
In a Java program, with overridden methods, it is not possible, in principle, to know which version of a method will be invoked without a complete trace of the program.
Thus you can learn how things are represented (at a low level) and how things might be hidden/regularized at a higher level.
No wonder that as per TIOBE indexing, C language has made a whooping 6.62% growth in last 12 months.
It depends entirely on what the person wants to learn.
When I program in Java, I don't need to think about what the compiler will do with a reference  variable (is it a pointer?
It is true, however, that some medical practice is only considered valid because it was handed down from antiquity but actually has no scientific basis.
I know a few programmers who can validly say that, but most C++ programmers cannot, outside a fairly narrow range of languages.
You say you can learn any language using the "paradigms of C++".
If a language is minimally useful it will provide a complete set of abstractions that permit you do any computation (Turing complete) solely within that set of abstractions.
If you start at a low level then you have only one direction to move, but if you start at a higher level you can move both higher an lower.
If I write a program describing explicitly how a problem is solved (typical in C), the optimizer is pretty much limited to following that instruction stream (not precisely, I realize, but that isn't global optimization).
about creating even higher level abstractions than those in the base language.
If you want to become a medical doctor, you don't start by  gathering herbs and chanting ancient songs.
However, once you choose a language, the programs that you build within that language are usually (always?)
The second reason that low level programs are only apparently efficient is that modern computers are only barely recognizable as Von Neumann architectures anymore.
Some people think that low level languages are more efficient than high level languages.
It is much easier to maintain and grow such motivation for the subject if your students can quickly develop some programs they are proud to show.
Programs consist of many many statements and it is easier to write and reason about complex algorithms in a high level language, especially one that is purpose built (bespoke) for that domain.
In low level languages like C, the polymorphism is ad-hoc, implemented by setting and testing flags.
Funny examples come more quickly with higher level environments.
At some point in history, something was tried and it worked.
In a few cases it isn't ethical anymore not to use some relatively ancient practice, so science is blocked from advancement in that area.
This experiment, thankfully I stopped it before implementation.
That is why we don't name our variables v1, v2, etc and our functions f1, f2, etc in our programs.
Start with a high level language, probably either a good OOP language (Java, Python, Scala...) or a good functional language (Scheme, Racket, ...).
Adopting this new way of thinking is actually inhibited by what you learned well in C++.
Our variables are things like size and done and function names are like compute_capacity.
Processing is designed for novices, and makes it easy to create visual and interactive programs without a ton of boilerplate.
I would agree with you (based on my experience): In BS, we took C in 1st semester, then OOP and Data Structures with C++ and gradually switched to C# and Java in later semesters.
We try to write programs "in the language of the problem we are solving" and choose names (abstractions) accordingly.
Similarly, in my pedagogical career, I have found two roadmaps:
But the programs written by those who started low, too often still use only ad-hoc methods, which potentially leads to programs that are difficult to read and understand (too-deep nesting of structures).
Here is an odd historical note about medicine, that I'm pretty sure doesn't really apply here:
I have somewhat the same history, but come to a different conclusion.