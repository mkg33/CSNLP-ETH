Jeff Darcy has a good article on high performance server design from a software point of view.
The TCP receive buffer is only one component to a connection.
Given that, there is no theoretical limit to number of concurrent TCP connections a Windows 2008 server can handle.
I am just looking for perfect utilization of server resources.
Unfortunately, memory is not unlimited (and I want to utilize only physical memory).
Only thing will happen is, with each connection there will be memory consumption in server.
Even then the real world tends to pop up things you had never thought of, which you add to your load profile and start the testing cycle again.
If I can decide on max size of request buffer per connection and max number of requests to allow in queue per connection.
The problem with your question is that it is largely dependant on your application, hardware, os configuration and use case.
This is the real dilemma in server design bugging me badly for last many days.
So my answer is: load test, load test, load test, then load test some more.
As you note in Case 2, there are any number of variable that can blow your Case 1 calculations out.
Server resources will always be constrained first.
There are finite limits to the number of concurrent TCP connections a server can achieve but they are so large they don't really count.
Are there any standard guidelines or empirical data available with someone who can share with me please.
The OS will maintain other data and state per connection and your application will have other requirements per connection, or for a running request in addition to all that.
Then, based on available server memory, it will then automatically set limit on max number of concurrent connections.
The only real way to get an idea of what your setup is capable of is to come up with a set of representative load tests and run them as you develop your service.
How to decide on these limits to achieve best performance and throughput?