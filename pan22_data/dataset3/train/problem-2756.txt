There is no time like the present to begin learning this vital skill.
One you have a completed table (4 or 5 entries), you can start to look for a pattern.
Using program correctness for grading has the desirable qualities of being objective and automation friendly.
Then give each student access to the code (s)he submitted for the homework as a starting point for completing the exam exercises.
Do you know any research papers (or books or posts) that tackle this issue and/or recommend other assessment techniques / exercise types?
I am wondering how much of this is because they can not express in natural language (don't know terminology).
In ether case another pupil is the instructor, that tells turtle to move and turn.
Also getting pupils to pretend to be the turtle can help, or teacher is turtle.
So how could you keep these qualities while testing for actual understanding?
While I agree that good code structure, variable naming, and possibly comments can offer proof of understanding, they are all highly subjective.
Disclaimer: I work in industry and have no research to support this.
I assume that they do this by some combination of brute force trying things and SO search.
You can discuss what the best strategy is [binary search].
Testing and debugging are absolutely essential skills when developing software, and the sooner that students practice those skills, the better for us all.
Have a student present his code in front of class and talk about how he made it work.
I've seen too many cases when a student has completed a programming exercise (without cheating) but can't say, for example, what type of values a variable contains during program runtime (in case of Python programs).
For example: Trying to find the angle of a triangle, when drawing using turtle graphics.
What I would do instead is construct exam exercises that are relatively small requirements changes to homework assignments that students have completed earlier, ideally at least 2 weeks earlier.
Tune the original homework size and the exam requirements changes so that a student who understands the code and has well-written code can complete the exam exercise quickly, but one that started from scratch would not likely be able to complete the work.
This is important, to allow them to communicate with a larger team, to allow them to look stuff up on an internet search, and to answer some of the exam questions.
If I understand your problem correctly, it's that students can create programs that behave correctly without understanding why they behave correctly.
How much is because of just fiddle until it works programming.
The number of SO posts with non-trivial code, but demonstrating no debug skills whatsoever, is embarrassing.
Too many cannot, (or will not), summon up enough programming skill to print out variables that may help diagnose a problem, never mind acually use a debugger.
Educators should be pressing home the message 'If you cannot debug, you cannot develop programs'.
This is an important technique to use some of the time.
Awarding a large marks boost for details of testing and debugging renders SO/web copypasta answers much less useful.
It looks like the ability to solve exercises should not be the only (or even main?)
While doing this they gain a deeper understanding of how the code works.