The priority setting only comes into play when a resource is limited - so if both 1G servers were transmitting at full speed to another 1G host there would be twice as much traffic as the destination link could handle.
I assumed that, since one port has higher priority then other, the transmission speed will not be equal, and the traffic going throughout the tcp port with higher priority will be significant faster.
I run two instances (per computer) of Network Benchmark application on two computers being in the LAN.
The switch would then, in theory, reference the relative priority of the frames to determine which would be transmitted and which would be dropped.
One computer acts like server and other like client.
Then I run tests, so that server sends some amount of data to client showing current transmission speed.
Note that there may be multiple mechanisms that you can select to perform evaluation and drops and that each may yield different results (not very familiar with HP's network gear) - for example a 2:1 ratio of high to low priority packets (vs all high and no low)
The switch is seeing 1Gb (or 100Mb) of input, sees a path at that same speed and sends the data along happily.
The speeds are nearly equal (half of bandwidth) and non prioritizing seems to be applied.
In benchmark applications I set two ports configured earlier on the switch.
Priority on the Switch will have no effect in your test because the sending computer's NIC is the bottleneck.
There is a QoS option for prioritize traffic by TCP/UDP port.
I suppose that I misunderstand some QoS basics and this is a reason, but I will be grateful if someone clarify this issue for me.
Then I performed tests to ensure that prioritizing really works.
I set two ports: one with highest priority and another with lowest.
If it has to choose what data to send, then it would prioritize.