So during training, your model is acting as both a global model (all parameters subject to that final global loss) and a model composed of sub-models with specific terms applying exclusively to your auxiliary tasks.
In the second page, they had mentioned about Auxiliary Losses which can speed-up the model convergence and as an additional regularizer.
An early reference would be how Google's Inception model uses auxiliary classifiers.
The general idea is that you take an intermediate output of your model and use it as the prediction in a separate loss function.
I am reading Character-Level Language Modeling with Deeper Self-Attention from Rami Al-Rfou.
To quickly run through the auxiliary losses in the Al-Rfou et al.
As an example, say we have an input sequence ABCDEF.
However, I cannot find any information or reference explaining auxiliary losses.
Auxiliary losses are additional terms added to your global loss to induce learning further upstream in your model and can be seen as a way to combat vanishing gradients.