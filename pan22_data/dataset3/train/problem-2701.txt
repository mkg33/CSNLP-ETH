You can never do better than the Kolmogorov complexity of a string (this is by the definition of Komogorov complexity).
Yet that description is nowhere near a million digits long.
Because almost all data that real people are actually trying to compress is highly structured -- it doesn't look like a random file.
Half of all such strings can be compressed by at most 1 bit.
1/8 of all such strings can be compressed by at most 3 bits.
To see this in action, generate a random file using some random process.
The description of it fits in the previous sentence, and a computer could reconstruct it from that description (or one very much like it).
It will either stay the same size or get bigger, almost all of the time.
1/4 of all strings can be compressed by at most 2 bits.
can be compressed at the ratio of 2:1 -- very, very few.
The secondary fact is that almost all human-generated data is super, super compressible because it is so structured.
Now try to compress it using your favorite compression algorithm.
On the flip side, there are highly compressible strings.
The more random looking the data, the harder to compress.
The fact is that strings with that property (of being highly compressible) are extremely rare among all possible strings.