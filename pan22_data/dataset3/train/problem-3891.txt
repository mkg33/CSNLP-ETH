Total functions are determined by their graph, as we're taught in set theory 101.
Do you want to give all non-terminating functions the same semantics?
See Andrej Bauer's answer here and his answer on a related question for some examples.
In particular, many theories of types have a syntactic formulation.
Giving a calculus a type-theoretic semantics is not trivial.
There's probably an earlier reference but for one thing, the colon was used in the Pascal programming language:
In some theories of types, types are sets with certain properties, but in others, they're a different kind of beast.
You might be willing to conflate integer and $\mathbb{Z}$, but (x := 0; while true; do x := x + 1; x) is not an element of $\mathbb{Z}$.
Because what's on the right of the colon isn't necessarily a set and what's on the left of the colon isn't necessarily a member of that set.
That second thread has other answers worth reading.
The Haskell wikibook has a good presentation of the topic.
For example, an expression in a typed programming language has a type even if it doesn't terminate.
Bertrand Russel discovered a paradox in naive set theory, and he worked on type theory as a way to limit the expressive power of set theory to avoid this (and any other) paradox.
In a typed calculus, to say that the types are sets is in fact to give a semantics to the types.
For example, suppose you're defining a language with functions.
Perhaps the way in which the distinction between types and sets is the most apparent is that the most basic rule for sets, namely that two sets are equal iff they have the same elements, usually does not apply for types.
I don't know when the colon notation arose for types.
I suspect it wasn't the first, though, because many theory papers from the early 1970s use the notation, but I don't know of an earlier use.
Over the years, Russel and others have defined many theories of types.
When the typing rules used as a foundation for a theory, it's important to distinguish what the typing rules say from what one might infer by applying additional external knowledge.
Interestingly, this was soon after the concepts of types from programming and from logic had been unified — as Simon Martini shows in Several Types of Types in Programming Languages, what was called a “type” in programming languages up to the 1960s came from the vernacular use of the word and not from type theory.
One of the seminal papers in this domain is Church's A Formulation of the Simple Theory of Types (1940)
You can't interpret types as sets for a calculus that allows recursive functions until you've answered that question.
Values have types, but so do other things, such as expressions and variables.
Type theory started out in the early 20th century as an approach to the foundation of mathematics.
There are rules that cause a thing to have a type.
Giving programming languages or calculi a denotational semantics was a difficult problem in the early 1970s.
The seminal paper here is Toward a mathematical semantics for computer languages (1971) by Dana Scott and Christopher Strachey.
As I wrote above, a second part of the answer is that even if you have managed to give types a set-theoretical semantics, the thing on the left of the colon isn't always an element of the set.
This is especially important if the typing rules are a foundation for a proof theory: theorems that hold based on set theory with classical logic and the axiom of choice may or may not hold in a constructive logic, for example.
Algol didn't use it, but the heavily Algol-inspired language Pascal did in 1971.
It's now standard in semantics, and common in programming languages, but neither Russel nor Church used it.