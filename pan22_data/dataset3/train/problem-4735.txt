One other source to back up my opinion: in the PyTorch implementation it seems the bias is added the output of the convolution's result.
You can see from the animations of various convolutional operations here, that the transpose convolution is basically a normal convolution, but with added dilation/padding to obtain the desired output dimensions.
For that reason I would add the bias after the convolution operations.
We are performing a (transpose) convolution operation that returns the same input dimensions that produced the activation map in question, with no guarantee that the actual values are identical to the original input.
This is standard practice: apply a matrix dot-product (a.k.a affine transformation) first, then add a bias before finally applying a non-linearity.
We are going backwards in the sense that we are upsampling and so doing the opposite to a standard conv layer, like you say, but we are more generally still moving forward in the neural network.
With a transpose convolution, we are not exactly reversing a forward (downsampling) convolution - such an operation would be referred to as the inverse convolution, or a deconvolution,  within mathematics.
The trick is to retain the mappings of localisation between the pixels.
In the paper from which those animations are taken, they explain how a transpose convolution is essentially the convolution steps performed in reverse: