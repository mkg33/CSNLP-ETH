For example, stratified sampling and low-discrepancy sequences are two examples of correlated sampling schemes that almost always improve render times.
We know that samples that are not chosen independently can be beneficial in terms of noise.
Are there any other considerations that could influence whether sample correlation is beneficial or detrimental (e.g.
Most descriptions of Monte Carlo rendering methods, such as path tracing or bidirectional path tracing, assume that samples are generated independently; that is, a standard random number generator is used that generates a stream of independent, uniformly distributed numbers.
So the question is: How does sample correlation influence the variance and the convergence of a Monte Carlo estimator?
All of these rendering methods can prove beneficial in certain scenes, but seem to make things worse in others.
It's not clear how to quantify the quality of error introduced by these techniques, other than rendering a scene with different rendering algorithms and eyeballing whether one looks better than the other.
However, there are many cases where the impact of sample correlation is not as clear-cut.
Can we somehow mathematically quantify which kind of sample correlation is better than others?
For example, Markov Chain Monte Carlo methods such as Metropolis Light Transport generate a stream of correlated samples using a Markov chain; many-light methods reuse a small set of light paths for many camera paths, creating many correlated shadow connections; even photon mapping gains its efficiency from reusing light paths across many pixels, also increasing sample correlation (although in a biased way).