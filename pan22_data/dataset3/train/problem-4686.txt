The aim is to simulate a single coin toss while minimizing bias.
Suppose we have $n$ identical coins with bias $\delta=P[Head]-P[Tail]$.
The bias of the algorithm is defined as $Bias(A)=|E[A=0]-E[A=1]|$ where the expectation is taken over the distribution defined by $n$ i.i.d bits ${x_1,\ldots,x_n}$ such that $Prob[x_i=1]-Prob[x_i=0]=\delta$.
The algorithm potentially requires an infinite number of coins (although in expectation, finitely many suffice).
The simulation must be efficient in the following sense: An algorithm running in polynomial time looks at $n$ random bits and outputs a single bit.
This question concerns the case when the number of coin tosses allowed is bounded.)
This question seems very natural to me and it is very likely that it has been considered before.
Is anything known when a weaker class (in $AC_0$, etc.)
(Von Neumann gave an algorithm that simulates a fair coin given access to identical biased coins.
If you want an even number of coin tosses to be unbiased with a biased coin, the easy way to remove bias is reverse the result of every other toss.