rather than forcing an end-end retransmission of data by transport or application layer protocol.
This allows one NIC to fail, still leaving you with connectivity.
In a datacentre, this is probably best done by getting some IP space, and announcing it via BGP to your upstream transit/peering providers.
In fiber case it is overhead for network to transmit extra bits.
You can also do lots to make your application layer reliable.
Better still would be to connect 2 NICs to different switches.
(as well as making sure that your servers have dual power supplies and at least 2 disks in a RAID level >= 1 !)
For extreme reliability, you want to look at multiple power feeds into multiple racks, in multiple globally diverse datacenter locations, each rack having all of the above redundancy built in.
Now you can lose a single NIC and a single switch.
Multiple servers on multiple switches, connected to multiple routers.
Some of your servers might need to run something such as memcached, to allow you to share session information and allow your users to seamlessly transition between servers.
A link-layer reliability is often used for links that are prone to high error rates such as wireless etc.
By diverse I mean the traffic takes a different route entirely out to the internet.
Ok, so you have a server.. And you want pretty good availability and redundancy, so you connect it with one NIC to a single switch.
Better redundancy would be to connect 2 NICs to the same switch, using STP Bridging to make sure you don't get a loop in the network.
That's a good start, networks are fairly reliable if left alone.
When a link-layer protocol provides reliable delivery, it guarantees to move each network layer data-gram across the link without any error.
WAN redundancy provided by more than one diverse route.
For greater redundancy you need to look at how the network gets beyond the LAN.