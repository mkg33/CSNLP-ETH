Coccurance matrix of $nXn$ dimensionality when converted into $nXd$ dimensionality, makes for word vectors of $d$ dimensions.
Source: "From Word Embeddings To Document Distances" Paper
The generalized solution consists of the following steps -
I'm looking to solve the following problem: I have a set of sentences as my dataset, and I want to be able to type a new sentence, and find the sentence that the new one is the most similar to in the dataset.
they managed to compress the semantic, syntactic global feature of a sentence into some latent space expressed maybe with some finite 10 to 30 independent random variables (factorized distribution).
the novel idea in this work, they interpolate between two sentences.
on each sentence to measure similarity with others.
Are those actually viable for use in this specific case, too?
One ways is to make a co-occurrence matrix of words from your trained sentences followed by applying TSVD on it.
Are there any other techniques/algorithms to solve this (preferably with Python and SKLearn, but I'm open to learn about TensorFlow, too)?
I've read that cosine similarity can be used to solve these kinds of issues paired with tf-idf (and RNNs should not bring significant improvements to the basic methods), or also word2vec is used for similar problems.
For 1. word2vec is the best choice but if you don't want to use word2vec, you can make some approximations to it.
WMD is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors.
There is some recent work based on Variational Auto-Encoder in RNN models.Generating Sentences from a Continuous Space, with pytorch implementations: github code.
Word Moverâ€™s Distance (WMD) is an algorithm for finding the distance between sentences.
Once you get word embedding of each word, you can apply any of the similarity metrics like cosine similarity, etc.
For your problem, you would compare the inputted sentence to all other sentences and return the sentence that has lowest WMD.