I would just like to suggest some further options, which were generalized in that answer as feature engineering.
Since we have unlabeled dataset, should I look for labeling techniques?
Try doing clustering (one of the unsupervised algorithms you were talking about) to see if there are any in-betweens or irregularities.
I can't see any way to magically identify unreliable data in this context.
The more familiar you are with the data, the better interpretation you can provide to any output you get from the methods you use.
Local outlier factor tries to detect samples that behave like an outlier within a group.
You saw a family in survey results in that group and said 65k usd per year, you can easily say that there might be a misstatement since families like that have higher income.
Despite these misstatements, we have some features in the dataset which are certainly reliable.
Therefore, these fraudulent statements in original data will lead to incorrect results and patterns.
I have read @MaximHaytovich's answer and it is a good one.
What is the nature of the data that you are 100% sure is reliable (it is mentioned in the question you posted that there is available data that is reliable) and how informative are they for the goal?
But these certain features are just a little part of information for each household wealth."
This probably varies by country and hence culture and if the information is used directly to affect the welfare benefits received by that individual, it may not be.
Is there any way to figure out these misstatements and then report the top 10% rich people with better accuracy using Machine Learning algorithms?
Or, should I work with semi-supervised learning methods?
Please introduce me any ideas or references which can help me in this issue.
I'm not sure it is possible to generalise as to how people may mis-state financial information they report to governments.
Let's say average income of the group is 150k usdÂ per year.
Try doing some outlier analysis on the incomes or any features that may be fraudulent that may make sense to do outlier detection on.
These households misstate their income and wealth in order to unfairly get more governmental services.
Is there any idea or application in Machine Learning which tries to improve the quality of collected data?
How should we deal with unreliable data in data science?
You can find more information in Wiki and sklearn.
Unreliable data means that households tell lies to government.
I would suggest trying to do the obvious first and analyze the data before transforming it to become ready for any machine learning algorithm.
If there is data on household income from a more trusted source, tax receipts for instance, that might be used to normalise the self reported data.
However, we know that these collected data is not reliable due to many misstatements.
"According to the Survey on Household Income and Wealth, we need to find out the top 10% households with the most income and expenditures.
First look at the data and learn yourself and hypothesize about what patterns could be indicators of fraud.
I don't know what are the survey questions but for example, let's say living in downtown with 3 children and going a cinema 4 times in a week form a group of people.