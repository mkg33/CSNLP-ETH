-- there are many ways to do this; the simplest would be to start up pyspark with Databrick's spark-csv module.
Then transform your data so that every item is in the correct format for the schema (i.e.
-- Excel files are not used in "Big Data"; Spark is meant to be used with large files or databases.
The last step is to make the data frame from the RDD.
From there, using a local instance I do the following:
So d0 is the raw text file that we send off to a spark RDD.
The other method would be to read in the text file as an rdd using
"Also, please tell me how can I import xlsx file?"
Reference: http://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.Row
If you have an Excel file that is 50GB in size, then you're doing things wrong.
Excel wouldn't even be able to open a file that size; from my experience, anything above 20MB and Excel dies.
"How can I import a .csv file into pyspark dataframes ?"
In order for you to make a data frame, you want to break the csv apart, and to make every entry a Row type, as I do when creating d1.