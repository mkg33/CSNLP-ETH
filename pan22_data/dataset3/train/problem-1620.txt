Your dataset most likely has high variance, given the large jump in accuracy based on different validation sets.
This means that the data is spread out, and can result in overfitting the model.
The performance of a classifier depends on the training set, therefore the performance will vary with different training sets.
Common techniques to reduce overfitting in random forests is k-fold Cross Validation, with k being between 5 and 10, and growing a larger forest.
Finally you will want to test the trained classifier on another dataset - evaluation set - which has not been part of the dataset used in crossvalidation.
To find the best parameters for a specific classifier you will therefore want to vary training and test split (such as in crossvalidation) and choose the parameter set which achieves the best average accuracy or AUC.
What you are experiencing is not a problem, but rather an inherent attribute of all classifiers.
Cross validation is used to help negate this, by rotating the training and validation sets and training more.
While training, your model will not have the same output when you train with different parts of the dataset.