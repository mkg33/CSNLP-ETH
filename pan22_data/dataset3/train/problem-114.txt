If your naming is relatively evenly distributed you can do this by the first few characters of the filename to keep it simple.
As far as I am aware, there is not a maximum limit to the amount of files in a folder, however NTFS has a a limit of 4,294,967,295 files on a volume.
It is all very well posting theoretical NTFS limits as some other posters have done, however you will find that in reality if you put 100k files in a folder, performance on accessing those files significantly drops.
If this is the case then you may want to look at combining files to match it.
Those numbers are large, but not near the limits for the NTFS filesystem.
So, while you won't get errors, you'll likely run into performance problems for example when retrieving directory listings.
This is the case not just with NTFS but all the popular Linux filesystems as well.
In general about 6k-10k files in a directory will start to slow things down due to memory consumption.
The Ext filesystem has at least performance problems if you have too many files in the same folder.
Above about 300k files, the creation of files will also slow down due to 'short file name generation' having trouble finding unique names.
You want to create these folders in one go, not on the fly, so that the root folder is not fragmented.
There is also the issue of wasted space in blocks if you files are small and do not align to the block size.
See here and here (scroll down to "NTFS Size Limits") for other relevant limitations of NTFS.
If you're using Windows 2003, then presumably your disks are NTFS formatted?
It seams like a good idea to limit the number of files per folder to something reasonable by having a deeper folder tree.
"Something reasonable" has to be tested and measured in your environnement.
I dont know for sure, but I would think that the same problems will arise with NTFS.
But this is one of those situations where if you're getting anywhere near the limits then you're almost certainly doing something wrong.
Storing files in a MySQL blob is possible, but in my opinion it's slow and grows your database significantly.
I would suggest creating a deeper directory structure, and migrating your current structure to it.
The easiest method is to divide them into subfolders holding no more than 10k-20k each.
I would go for below 10K, but that's just a hint ...