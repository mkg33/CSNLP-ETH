I also checked bandwidth, but cannot see any information on t2 instances in there.
I cannot type anything anymore in terminal windows, websites stall (its a web server) and I can't connect to it.
This is a one off transfer, so I'm looking to get an idea of how much I have to slow down the transfer to make it happen safely.
Finally I looked at CPU credits, but presumably it should not completely stall?
I have a 9.2GB file which I want to transfer into my AWS t2.small instance for backup purposes.
At the same time I'd like to get an idea of limits for management of this web server.
Some time down the track the instance always locks up.
When I start scp, it copies the file at around 3.4MB per second, which results in about 45 minutes expected transfer time.
From this documentation I cannot see that I exceed IOPS or throughput for the disk.
Typically with large amounts of data copy, file system cache can use up all available memory (t2.small only has 2GB), resulting in swapping, which might cause the system to become unresponsive.
Not sure if there is a way to bypass file system cache with scp though.
I investigated EBS limits: I have 2 RAID10 200GB gp2 disks attached.