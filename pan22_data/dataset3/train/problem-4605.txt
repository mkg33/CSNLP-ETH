I approached the same problem by using 10-Gigabit iSCSI with a dedicated 6-port switch (HP 6400cl-6XG - $2200) and Intel dual-port CX4 NICs (Intel EXPX9502CX4 - $650).
There are very few pre-built solutions available and most are expensive.
Personally, I like the price/performance of InfiniBand host adapters, and most of the offerings at Supermicro (my preferred hardware brand) have IB as an option.
Most comments about iSCSI over IB talk about iSER, and how it's not supported by some iSCSI stacks.
Linux has had IPoIB drivers for a while; but I don't know if there's a well-known usage for storage.
The performance is amazing and the large amount of available bandwidth gives you great peace of mind and flexibility.
So, does anybody have some pointers about how to use IB for shared storage for Linux servers?
If products like Open-E introduced native IB support into their software (specifically srp) you would have an easy hardware solution.
If someone was to introduce some storage appliance software similar to Open-E with full SCST SRP target support it would open up the mainstream market to IB and I for one would be very happy.
The difficulty with IB when building a SAN is to manage the srp target.
to work in a mixed Linux, Windows and OpenSolaris environment.
In my experience, as a pure RDMA storage network, without IP, there is nothing that can beat IB and if you shop around, you can set something up for a very reasonable price.
The client side is very simple to set up on RHEL and it works perfectly.
Use one for backups, one for main storage etc etc and use them simultaneously without any loss of performance.
Yes you are still limited to the speed of your array, but with IB you can connect multiple arrays without losing performance.
I've just had to deal with an IB SAN using Mellanox NICs.
I'm contemplating the next restructuring of my medium-size storage.
We have a test system up and running now which is performing at 600MB/s consistently and under high load.
In this case, very little was needed to get drivers, etc.
The cost per server came down to the NIC and a $100 CX4 cable.