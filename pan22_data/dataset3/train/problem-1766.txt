One way is to use iconv with UTF-8 as both the source and destination encodings.
This means that if data could be UTF-8, you'll rarely get false positives if you assume that it is.
In a shell script, you can use iconv to perform the converstion, but you'll need a means of detecting UTF-8.
never found something giving me better results than this.
with this simple command, it will recode your file, no matter what's inside to the desired encoding :
Despite beeing an emacs user, i'll recommend you to use vim today.
If the file was valid UTF-8, the output will be the same as the input.
This bash one liner uses the above command as the input for recode and loops over multiple files:
If the number of funny characters is not excessive, you could use egrep to scan and find out which files need recoding.
UTF-8 has strict rules about which byte sequences are valid.
Both ISO-8859-1 and UTF-8 are identical on the first 128 characters, so your problem is really how to detect files that contain funny characters, meaning numerically encoded as above 128.
Don't worry about converting existing utf-8, recode is smart enough to not do anything in that case and would print a message:
I'm a bit late, but i've been strugling so often with the same question again and again... Now that i've found a great way to do it, i can't help but share it :)
This message is quite old, but I think I can contribute to this problem :
You can detect and guess the charset of a file by using
So, if you like to run it recursively and change all *.txt files encodings to (let's say) utf-8 :