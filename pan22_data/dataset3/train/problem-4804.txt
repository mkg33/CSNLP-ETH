I'm going to assume this is English (seeing as that's all we do).
For example, going by your formula, an entropy of 3 for a string of 5 characters should be fine but an entropy of 3 for a string of 8 characters is poor.
Is there a better / more elegant / more accurate way to calculate the entropy of a string?
Wouldn't it be better to keep a HashSet<string> of stop words (the most common words in English that don't convey meaning), tokenize the string into words, and count the number of words that aren't stop words?
I whipped up this simple method which counts unique characters in a string, but it is quite literally the first thing that popped into my head.
That would give a more accurate measure of entropy.
Efficiency is also good, though we never call this on large strings so it is not a huge concern.
But, your formula wouldn't be able to differentiate between the two results.
Why not divide the number of unique characters in the given string by the total number of characters in that string.
We're calculating entropy of a string a few places in Stack Overflow as a signifier of low quality.
Whereas, the above given formula would do so to give a more accurate measure.
You can probably expand this to something like bi-grams and tri-grams to get things like "sdsdsdsdsdsdsdsdsdsd" (although yours would catch this as well).
Would a bayesian approach like spam filters do be appropriate for something like what you're trying to achieve?