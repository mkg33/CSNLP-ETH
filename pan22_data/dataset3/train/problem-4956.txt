Since the code is not mine, I can only test against the data instead of the code.
If the code were mine, then when I find a problem with the files, the first thing I would do is file an issue/bug, write a test to catch the mistake in generation, fix the bug, then move on.
On that page there are links to technical papers (such as arXiv:1611.09477 stat.AP) and shorter, more readable articles linked from that page where they "[...] demonstrate some of the things that can go wrong with data, and explore ways to address those issues using the R statistical language."
One of my projects includes a year-long weekly rhythm, where I get a new set of simiarly-structured input files.
(I'm sure other unit-tests could be similarly adapted.)
The process that executes on these input files is rather lengthy, and it might take days for the first symptom of the problem to break out (thereby losing days of processing), so finding violations of my assumptions early is rather important.
While I can think of a million other ways that the data might go wrong, I cannot guard against (or check) all of these.
(I won't go into how to do write a personal function to adapt these types of tests onto a dataset, the website is pretty robust with examples.)
Furthermore, testthat provides good mechanisms for automation and returning problems in easily-digested (and if desired verbose) formats for quick identification of problems.
See this paper from 2011: MICE: Multivariate Imputation by Chained Equations in R
There is no one-size-fits-it-all checklist that you can work through by crossing things off.
If there are obvious boundary values (e.g., always non-negative) then I can incorporate those, but for the most part any test after those defaults are to guard against a failure that I've actually seen (and not just imagined).
They focus on preparing data for statistical modeling (which is different from preparing data for, say, an ERP system, or for a scientific computing task).
So my template starts with the basics: type, size, and sets.
It's not hard to conceive some of these tests up-front: type (int, string, bool), min/max size (exactly 1, no less than 10, no more than 52), and sets (only allow specific values).
Another collection of  recent material can be found here: A Statistically Sound 'data.frame' Processor/Conditioner â€¢ vtreat by R experts Nina Zumel and John Mount.
Since the names and structure of each of the files is static-enough for the tests, it's rather straight-forward.
Substituting NA values correctly or smartly is also called "imputing" and that can be a very technical topic.
To be honest, I think I am an experienced intermediate R programmer and I still struggle with these issues.