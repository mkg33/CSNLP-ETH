One good reason is that most gamers don't go outdoors much.
You are talking about a different kind of realism here.
The point here is that, the more you aim for realism, the bigger your players' expectations will be.
The realism you are referring to, however, is that an object exactly represents the corresponding real-world object.
It's also pointless in any game which portrays an imaginary world, only making sense in games or simulations which have real settings.
As I'm sure you can understand, it's incredibly difficult to make every single detail like this accurate.
How many users are ever going to know that the stars are not correct?
It's much easier to just hit random noise on a texture and be done with it.
For all the rest, I would probably intentionally make it unrealistic, so I can keep my focus and that of my players in the parts of the game that actually matter.
The focus in modern game graphics is realism, rather than authenticity, because realism is what enhances the sense of immersion into the game's world, whereas authenticity is more of an intellectual aspect.
(Or given it's on a flat screen, perhaps a photograph is a better analogy?)
Even though I would say the vast majority of games don't get their stars right is out of straight laziness and/or ignorance.
And while player expectations have no limit, budget and time do.
The same goes for firearm sounds, physics (ever played a racing game?
When referring to games, "realism" usually means that when an object appears it looks like it's actually there.
So even if I fix that, more players would then criticize that you can't see the ISS, or that some star has an apparent magnitude of 4.5, but in the game it looks like 4.8.
If your game requires realism in some aspect, then by all means dedicate your time and money to maximizing this realism.
So if you have a brick wall with some mold on, by your definition it must only appear in exactly the way mold grows in real life.
Or if the game is set in New York, every shop must be exactly what is in the real city, and every trash can and subway entrance must be in exactly the correct place.
All the current answers are very good, but I want to propose a different point of view.
Players who notice that, will then criticize that the stars are 3 degrees off, or that based on the vegetation you see in the game, you can estimate the latitude where the game is taking place, and that the star field doesn't correspond to such a latitude.
However, the sky is realistic by the usual definition, in the sense it looks like real stars and are not pink blobs in the sky.
The effect is usually obtained via high resolution textures and so on.
Having a lower-resolution texture, or using tiling tricks to get a higher resolution, makes it harder for the "realistic night sky" to look good while looking "right".
Have you noticed how ludicrous the moon is on the night sky in the games that have it?
So suppose I got a star chart for my game, and I use that as my skybox.
Once you start to strive for realism in the night sky, you want a very, very high-resolution texture to pull it off.
), history (for historical games) and many other fields.
The reason behind this is that the more you aim for realism, the larger the player expectations will be on that realism.
However, if I were directing a game, I may not aim for realism in certain elements such as the star field, and I would do so intentionally, on aspects that are not crucial to gameplay.
To check that the star display in a game is indeed authentic, they will probably Google it, ironically.