"Greedy" or "Multi-Fragment" algorithm (worst-case approximation ratio
There are in fact more than two ways of analyzing an algorithm, and one of the theoretical ways which has been neglected is expected run time, rather than worst case run time.
The worst-case running time is doubly-exponential (in the number of variables).
In practice however, especially for well-structured problems, the F4 and F5 algorithms are effective (i.e.
5.5 of C. Okasaki's book "Purely functional data structures", so I thought I should share info about them.
It is conjectured that it is related, somehow, to the volume of the Newton polytope of the underlying ideal.
It is really this average case behaviour that is relevant to doing experiments.
Here is a very simple example: Imagine that you have an algorithm for an input of size n, which takes time n for each possible input of size n except for one specific input of each length which takes time 2^n.
I just discovered them just today while reading Sec.
[1] Fredman, M. L., Sedgewick, R., Sleator, D. D., and Tarjan, R. E. 1986.
The pairing heap: a new form of self-adjusting heap.
that defy the theoretical asymptotics: Jon Bentley's
In practice, they are extremely efficient, especially for users of merge.
As n gets large the problem has roughly the same run time in the worst case as the most efficient algorithm for 3SAT, where as the average run time is guaranteed to be very low.
In this paper they give experimental evidence of asymptotics (since the experiments run up to size N=10,000,000!)
By running the experiment a bunch of times, even if we take the longest run time for the sample, we are still only sampling a small portion of the space of possible inputs, and so if hard instances are rare then we are likely to miss them.
From David Johnson, a discrepancy in theoretical vs. experimental approximation ratios:
Pairing heaps, from [1] - they implement heaps, where insert and merge have O(log n) amortized complexity, but are conjectured to be O(1).
Hear the worst case run time is exponential, but the average case is [(2^n -1)n + (2^n)1]/(2^n) = n - (n-1)/2^n which limits to n. Clearly the two types of analysis give very different answers, but this is to be expected as we are calculating different quantities.
Damas-Milner type inference is proven complete for exponential time, and there are easily constructed cases with exponential blowup in the size of a result.
The Traveling Salesman Problem: A Case Study in Local Optimization, D. S. Johnson and L. A. McGeoch.
Nonetheless, on most real-world inputs it behaves in an effectively linear fashion.
It is still an active area of research to figure out what the proper conjecture even should be regarding the average or expected running time.
It is relatively easy to construct such a problem: If the first n/2 bits are all zero, than solve the 3SAT instance encoded with the last n/2 bits.
at least logN/loglogN) beats Nearest Insertion and Double MST, both of which have worst-case approximation ratios of 2.