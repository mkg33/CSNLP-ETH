You would have to go through all of the files and update any links so that they reference the local files rather than the original site.
"/mycss.css" would have to be in the main folder "mycss.css" would have to be in the same folder as the current page, etc.
Therefore, why is it that sometimes when we download a site locally, it does not work properly?
When you "save" the files out of the browser, it likely saves the response to the original request, not the current document object model that reflects what is displayed in the browser.
To see how they made the animation, however the animation is not working locally.
In short, what causes it to work differently locally than on the browser?
To give an example, I was trying to download this site:
When you do so, very often the dependencies get corrupt, links to script files don't work, images are missing.
In addition, some parts of the site may be dynamically loaded - i.e., the mark up that is actually displayed is different than that originally downloaded by the browser.
We need to remember that if the browser can read it, the user should be able to read it too.
It will download the site or part of the site and change all the dependencies to work locally as needed.
Frequently it's because the links in the HTML and/or JavaScript code, including CSS, etc., are still referencing the online site, not the locally downloaded files.
If you download a webpage you haven't necessarily downloaded linked files, e.g.
Even though some browsers have a function to download whole website, it is not perfect, and is bound to fail on dynamic websites that use server-side scripts, such as PHP.
We all know that we can't obfuscate HTML or Javascript.
If you'd like to download a whole website (or portion of one), check out HTTrack.
If the browser can display it properly, why does it breaks when it is stored locally?
Even if you did have them you'd have to change the references to them, or have them in proper locations for them to work.