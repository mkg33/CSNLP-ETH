when I do that all my zeros and ones transform in different numbers and also I don't know how to identify who is the Left or the Right category.
Do I need to standardize just the vector of features or also the vector of labels?
For the second question, normalization/standardization is always (er, always that I've seen, and certainly for your example) applying an increasing function, so that the ordering is preserved.
https://stats.stackexchange.com/questions/359015/ridge-lasso-standardization-of-dummy-indicators
https://stats.stackexchange.com/questions/290929/standardizing-dummy-variables-for-variable-importance-in-glmnet
How can I  after identify (with strings) if my prediction is actually giving me Left or Right?
labels = (labels - labels.mean()) / (labels.max() - labels.min())
(Indeed, one often-used scaler puts the data into the range $[0,1]$ anyway.)
For your normalization of binary variables then, always 0 gets mapped to the negative value and 1 to the positive one.
From what I can tell, there isn't a "right" answer to the title question.
I'm trying to do a simple softmax regression where I have features  (2 columns) and a one hot encoded vector of labels (two categories: left = 1  and Right = 0).
[-0.5633803   0.43661973  0.43661973  0.43661973  0.43661973  0.43661973 ...