A convolutional layer has a convolutional kernel of fixed size (say, 3x3) that is applied to the entire input image.
It is possible to build CNN architectures that can handle variable-length inputs.
Thus, it is possible to build networks that can handle variable-length inputs.
It's when you follow a convolutional layer with a fully connected layer that you get into trouble with variable-size inputs.
Many common neural network architectures don't use these methods, perhaps because it is easier to resize images to a fixed size and not worry about this, or perhaps because of historical inertia.
For instance, you can train and test on images of multiple sizes; or train on images of one size and test on images of another size.
For more information on those architectures, see e.g., https://stackoverflow.com/q/36262860/781723, https://stats.stackexchange.com/q/250258/2921, https://stackoverflow.com/q/57421842/781723, https://stackoverflow.com/q/53841509/781723, https://stackoverflow.com/q/53114882/781723, https://docs.fast.ai/layers.html#AdaptiveConcatPool2d, and so on.
You might be wondering, if we used a fully convolutional network (i.e., only convolutional layers and nothing else), could we then handle variable-length inputs?
During training, each parameter of the model specializes to "learn" some part of the signal.
That said, these methods are not yet as widely used as they could be.
We typically need to produce a fixed-length output (e.g., one output per class).
Most standard CNNs are designed for a fixed-size input, because they contain elements of their architecture that don't generalize well to other sizes, but this is not inherent.
I know that it's possible to overcome this problem (with fully convolutional neural networks ecc...), and i also know that i due to the fully connected layers placed at the end of the network.
That's why we can't afford to let the input shape change.
So, we will need some layer somewhere that maps a variable-length input to a fixed-length output.
I can't understand what does the presence of the Fully Connected layers imply and why force us to have fixed input size
The fully connected layer requires a fixed-length input; if you trained a fully connected layer on inputs of size 100, and then there's no obvious way to handle an input of size 200, because you only have weights for 100 inputs and it's not clear what weights to use for 200 inputs.
Fortunately, there are methods in the literature for doing that.
The training process learns this kernel; the weights you learn determine the kernel.
That said, the convolutional layers themselves can be used on variable-length inputs.
I'm studying right now Convolutional Neural Networks.
Input size determines the overall number of parameters of the Neural Network.
For example, standard CNN architectures often use many convolutional layers followed by a few fully connected layers.
Once you've learned the kernel, it can be used on an image of any size.
This implies that once you change the number of parameters, the whole model must be retrained.
So the convolutional layers can adapt to arbitrary-sized inputs.