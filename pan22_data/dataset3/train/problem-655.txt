From the output, I need to know the http response (status codes) but also benchmark the time it takes to download the different elements of the site.
Selenium and Curl are good options depending on what your goal is.
I am looking for a tool to test a website from a Linux command line.
What tool you choose depends on what you want to measure and the complexity of the site.
Also, a utility that I've come to like quite a bit is twill.
More information is available at http://twill.idyll.org/.
While this gives a better indicator of perceived performance than tracking individual HTTP requests, it relies on browser events to derive a value for the performance - but peceived performance is all about the time taken for the viewport to render (again there are tools for this - have a look at the filmstrip tools in WebPageTest).
One solution is to use http::recorder / www::mechanize.
This is a very poor indicator of performance (although it is useful for monitoring the health of a production system).
If the behaviour of the site is dependent on cookies (e.g.
A better solution is to use something like Boomerang - but that runs in a Javascript capable browser.
user needs to login) then ab / curl / wget (described in other answers) will not suffice.
It's nice as it has it's own little specialized language for filling out forms, validating links, and checking response codes.
With the exception of large/slow resources such as bulky reports, iso images, multimedia files the perception of performance has got very little to do with the time taken to process a single request - and its really difficult to measure this accurately (simply adding %D to your apache log appears to solve the problem but ignores TCP handshakes, SSL negotiation, caching effects, DNS lookup times).
Since it's just Python code, you can easily import the libraries and automate your tests yourself if you'd like to do something different.
There is also the argument about measuring the performance actually delivered to users of the site (RUM) vs synthetic testing.
All the data you are asking for is in your webserver logs - and a simple awk script will return it in a more readable form.
You may want to look at the following options of curl: