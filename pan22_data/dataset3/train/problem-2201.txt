While I cannot answer this with 100% certainty, my own intuition and experience say:
The value of these pixels goes down quite a bit by adding such a strong feature, but the value is absolutely not reduced to 0.
If we look at different problems, where we have one very important feature that holds most of the information but some other features with some signal in there the model generally still improves by adding these lower value features.
2) I think it will take around the same time to converge on average.
Fitting to the one-hot encoded feature will be very fast which means most of the time will be spent on fitting to the features, which is a similar problem as without the feature.
1) I would be very surprised if this is not the case.
I'm less certain about the second question but I think this is what you would find if you do the (interesting) experiment.
I think in theory this should lead to a strictly better model.
That is exactly what you are doing here, you have the MNIST pixels that individually hold little value but combined tell you quite a bit.
However, instead of going from 10% to 99% accuracy, we are now kind of going from 95% accuracy to 99.7% accuracy.