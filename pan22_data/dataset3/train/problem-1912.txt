Individual behaviors (outliers) are disregarded because you gain very little information from them.
Generally speaking, decision trees are able to handle outliers because their leafs are constructed under metrics which aim to discriminate as much as possible the resulting subsets.
Under this approach, what matters is to understand the general behavior of your features.
Whether you are using Gini Impurity, Information Gain or Variance Reduction to construct your decision tree does not change the outcome : all of these models aim to create as large (and homogeneous) buckets as possible.
On the other hand, null values should be treated whether it is through replacement, transformation or deletion from your observations.
Assuming that your outliers represent a tiny proportion of your dataset, it is very unlikely that a leaf will be created to isolate them (at least in the first steps of creation of your decision tree), because you will gain very little information on the complimentary subset.
However, if you have a large amount of outliers (but then are they really outliers ?
), and that they tend to have the same outcome, chances are that you may get a leaf to isolate them.