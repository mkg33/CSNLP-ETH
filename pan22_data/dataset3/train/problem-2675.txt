Then you are all set to use a deep learning technique such as neural networks, 1D convolutional neural networks, stacked autoencodders, etc...
How could I detect "malicious" patterns prior to those events (taking them as target variables) ?
Common misconception: A shallow method CAN and WILL perform better than a deep learning technique if you have properly selected your features.
So you would feed your algorithm all the instances in your dataset that did not result in a spike.
Then there are some very good techniques for extracting time and frequency information from your time-series.
This would also work and there are some good techniques that would do this.
There are a million ways you can extract data from these waveforms.
Interactions between the dimensions exist, therefore dimensions cannot be studied individually (or can they?)
Now, you have 6 waveforms in a 30 minute window from which you can extract data to get information about your classification.
Now you have a small dataset with instances that are a Frankenstein concoction which represents your 6 waveforms across the 30 minute window.
In general, I like to extract all the basic statistics from my waveforms.
Then from this your algorithm would be able to identify when a novel instance is significantly different from this nominal distribution and it will flag it as a n anomaly.
Look into envelope mode decomposition and empirical mode decomposition.
You can also use more rudimentary anomaly detection techniques such as a generalized likelihood ratio test.
Then you need to identify the time window you want to use for your time series analysis.
You can apply some dimensionality reduction techniques such as PCA or LDA to get a lower dimensional space which may better represent your data.
New statistic in P-value estimation for anomaly detection
This would mean that a spike will occur in your context.
Check out kernel support vector machines, random forests, k-nearest neighbors etc..
This is a time series and from this time series you want to identify the trigger of a certain event.
Experts in my field are capable of predicting the likelyhood an event (binary spike in yellow)  30 minutes before it occurs.
However, the nature of anomaly detection is to learn the distribution of the nominal case.
Based on the information from the specified window will a spike occur?
Now even though you have your reduced feature space you can do better!
Frequency here is 1 sec, this view represents a few hours worth of data, i have circled in black where "malicious" pattern should be.
For example, in seismic data, if you see agitation in a waveform from a neighboring town then you should expect to see agitation in your town soon.
I welcome any advice on which algorithms or data processing pipeline might help, thank you :)
Check how these statistics correlate with your labels.
You can use the raw data samples as your features, but this is WAY TOO many features and will lead to poor results.
I have tried Anomaly detection, but it only works for on the spot detection, not prior.
You will want a binary classification algorithm, luckily that is the most common.
Anomaly Detection with Score functions based on Nearest Neighbor Graphs
http://www.stat.rice.edu/~cscott/pubs/minvol06jmlr.pdf
Thus you need some feature extraction, dimensionality reduction, techniques.
I'm trying to build a supervised ML model using Scikit Learn which learns a normal rythm, and detects when symptoms might lead to a spike.
First, ask yourself, as a human what are the telltale signs that these other waveforms should have which would mean a spike would arise.
I have used empirical mode decomposition successfully on some time series data and obtained far better results than I expected.
What you will have is a set of instances (which can have some overlap but to avoid bias it is best for them to be independently drawn) and then for each instance a human needs to label if there was a spike or if there was not a spike.
feature extraction is the most important aspect of a machine learning architecture.
Get the mean, standard deviation, fluctuation index, etc.
You have done this and decided 30 minutes is a good start.