it is possible that an image shows a cat and a dog at the same time, then sigmoid is the way to go.
This means that the output of a softmax layer is a valid probability mass function, i.e.
Now, the softmax is basically a sigmoid function which is normalized such that $\sum_{j=0}^N \mathrm{softmax}(x_j) = 1$.
This is undesirable if you want to distinguish between multiple classes.
The output of the softmax function depends on all elements of the vector $\mathbf{x}$.
$$ \mathrm{softmax}(x_i) = \frac{\exp(x_i)}{\sum_{j=0}^N \exp(x_j)}$$
the output of your neural network is the probability of the input belonging to a certain class.
0-9 in MNIST, each probability is independent, that means you could have probability $p=0.9$ for the digit 1 and $p=0.5$ for the digit 3.
$$ \sigma(x_i) = \frac{\exp(x_i)}{1 + \exp(x_i)} $$
However, if multiple classes can appear at the same time, then sigmoid is well suited.
Given a vector $\mathbf{x}$, the sigmoid function is given by
The sigmoid will squash each $x_i$ into the range $(0, 1)$, which enables you to interpret $\sigma(x_i)$ as the probability of $x_i$.
the 10 digits as in MNIST, then softmax is the way to go.
This choice mainly depends on what your output represents.
A key difference is that the output of the sigmoid function applied to $x_i$ only depends on $x_i$.