I've written a browser extension to Chrome to do just this using Google Chrome's download API which'll work across all platforms (operating systems) i.e.
wget for linux is a base tool https://www.gnu.org/software/wget/
create a plain file with the list of the url you need to download (example pippo.txt):
Is there a way I can use this tool or another tool even to give it a list of the URLs, and it downloads the files for me as need be, keeping the folder structure much like site sucker does.
Feel free to fork if you want to go a different direction with it :-)
You can get the extension here, and all the code is open source on GitHub here.
I'm currently using sitesucker to pull down all the files for a client's web site.
for windows there is a bin port: http://users.ugent.be/~bpuype/wget/
My apologies if this is a poor place to ask this question :).
It's called TabSave and originally it was for this, I'm adding the ability to do some things researchers might want to do with giving PDFs titles straight from the browser but it's very light and can grab from open tabs if that's handy.
It gets me most of the pages, but the problem is that some of the pages we have aren't really accessed by link, but by a postal code finder.