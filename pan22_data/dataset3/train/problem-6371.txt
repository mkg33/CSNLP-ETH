Let's say that I have gathered Disk Transfers per second data for 2x24 hours period, i.e., instantaneous sampling of data every 15 seconds.
Unfortunately, there is no easy answer to that question.
How much growth will you have over the amount of time you want the system to last (both growth in size, and in iops)?
What statistical analysis can/should I apply to the samples if I want to use the data to, for instance, provision a storage?
NOTE: Redundancy is not a backup solution, so plan for backups as well.
Wide striping assumes that this is on some sort of centralized storage.
Or a formula involving the mean and the deviation?
Backups can (should) be isolated from your live data by time and space.
Should I simply use the peak value (which happens less than 1% of the time)?
If latency isnt important, then buying storage based on your projected growth in average iops is not a bad place to start.
The closest I can come to answering your question, is to note that if you cannot handle the instantaneous iops at any given time, you will simply increase latency.
You always size for the peaks, unless it's the kind of workload that can afford to have high latency when it's pushing a lot of IO.
Do you have time to maintain and prune you data to keep size down?
If it's local, of course you can't aggregate workload that way.
That is part of why wide striping is so popular- you can put together a bunch of workloads and size for the peak of their aggregate usage- different parts will peak at different times, so you're able to use cheaper disks to provide the same capacity.