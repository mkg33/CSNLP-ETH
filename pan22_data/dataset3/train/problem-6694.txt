you have to keep in mind that your future data will contain outliers as well and deal with them appropriately.
If you work on optimizing network performance, the packets that don't arrive can be the interesting ones.
If you work in fraud detection, the outliers are the points that interest you.
You'd have to think about where the outliers come from.
By this I mean: among $N$ training data decide $K<<N$ data points to be outliers and discard them.
Models like trees are usually somewhat robust against those while regression can go haywire.
Do you apply outlier detection in your usual routine?
There are several well working methods like disregarding everything that is more than 4 standard deviations (or median deviations from the median) out or just disregarding the n-th top and bottom percentiles.
So my advice is to not blindly throw away data out of principle but always think about what the data represent and why outliers may be present.
It is the main task to draw conclusions from the training data that can be generalized to future unseen data.
But then there is the case where outliers are not just flukes and can actually tell you something about the problem domain you are working on.