To obtain the standardized distribution you subtract every sample by the mean of the population then divide by the standard deviation of the population:
Generally, the goal of normalization is to scale a distribution in some way so that they may be compared to one another in the future (see below for more detailed explanation of normalization.)
Frequently in industry (as opposed to academia), people refer to normalization as max-min scaling where the distribution is scaled from 0 to 1.
However -- The goal of standardization is to   specifically, produce a distribution that has mean 0 and a standard deviation of 1.
Does it make sense that my model works when I do both Standardise and Normalise?
My goal is to build a binary classifier which analyses CNNs and GIST features?
This is probably how you are thinking normalization is defined.
Generally, I would suggest using standardization because it is more robust to outlier samples compared to normalization.
No, actually it doesnâ€™t make sense to apply two scaling methods to your data simultaneously.
Standardization is actually a type of normalization (see below for an explanation of normalization).
Having removed outlier samples from your dataset, you can safely use normalization instead.
Generally (and including CNNs as you ask about), its best to go with standardization.
Therefore the question is really what is the best way to perform normalization for my specific problem?
However, if you think of max-min scaling, and standardization as both types of normalization you can maybe imagine why you wouldn't want to do both.
The goal is to produce distributions that can be compared to one another.