Another way to think of this is if I ask a child to look through some multiplication examples,
We want to stop the training at the minimum loss of the evaluation set.
As another example, imagine fitting a 2nd order relationship between 2 variables with a 5th order polynomial.
If the kid learns some general rules as to how x*y=z then he will be able to answer any new multiplication question I ask him.
However, after some training there will be a point where the evaluation loss starts increasing.
Your validation error has stuck at 2.20, but training error is still decreasing.
When I sampled out 20% data (randomly) and divided it further into train-eval (90-10%, random but mutually exclusive), I observed the following graphs-
This is the point where the parameters are no longer generalizing for the data but is instead overfitting and specifically identifying points in the training data.
I am solving for a regression (using tensorflow's DNNRegressor) problem.
Try using L2 regularization, it overcome the overfitting problem, by forcing the weight parameters to be as small as possible.
Another thing, you can try using dropout technique, which will also help in interpreting model behavior.
However, if he just memorizes the outcomes of the few examples that I showed them then they will get any new instance wrong.
This is one of the typical reasons of having lower validation loss than training loss.
The behavior you notice in the graphs will always be the case regardless of the machine learning algorithm.
This will match the points too specifically and will not generalize to new data.
Dropout is a regularization technique that nulls out randomly some elements during training, hence making the model behave worse with the training data but generalize better to unseen data.
Given the lack of detail, I would say that a possible reason is the use of dropout.
Machine learning will require you to fit some parameters such that the model can capture the mappings from your input space to the desired outputs.
At that step, your model has overfitted to the training data enough to compensate for the regularization and keeps overfitting, hence the training loss keeps improving while the validation one does not.
It may be the case of overfitting, because it occurs when it overlearn the data.
Here, blue line corresponds to eval set loss and orange line is for train loss.
The total length of data is nearly 4.8 million rows, thus 20% being nearly 1 million data points.
As the model parameters are tuned using the training data, the model will tend to decrease both the training and evaluation loss.
At the start of the training these parameters will all be assigned randomly, thus we should assume random performance, thus 50% for a 0/1 classification.
This would explain with before step 150 you have better validation loss.