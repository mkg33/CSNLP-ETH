When the order of the columns differ, there is a little difference in the procedure.
In the case that the train and test data is the same in both cases, you’d likely have to see if there is a seed/reproducibility measure (in any part of your code) that you have not taken.
The root node selects the features 1st, 3rd and 18th, on both datasets the 1st, 3rd and 18th features are different in both possible datasets.
Considering you took steps to ensure reproducibility, Different ordering of data will alter your train-test split logic(unless you know for certain that the train sets and test sets in both cases are exactly the same).
The selections of these columns is done randomly: Let's say your dataset has 20 columns.
Though you don’t specify how you split the data it is highly possible that a certain assortment of data points makes the machine more robust to outliers and therefore offering better model performance.
While the ordering of data is inconsequential in theory, it is important in practice.
What LightGBM, XGBoost, CatBoost, amongst other do is to select different columns from the features in your dataset in every step in the training.
This is repeatedly done and in every step there is randomness affecting your ultimate result.