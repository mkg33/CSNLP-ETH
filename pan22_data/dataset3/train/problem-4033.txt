More advanced defragmenters will move directories first (since they are, on a per-megabyte average, accessed more frequently than ordinary files), followed by files accessed recently.
If you defrag disks for ease of data recovery, YOU'RE DOING IT WRONG!
Most files will be consolidated into one block each.
NTFs seems to fragment only slightly less than FAT but to degrade more slowly if it is.
If files are not fragmented, then all you have to do is to find the beginning of the file and either know the size, or know how to detect the end (e.g., by knowing the format of the file, or detecting when a new file begins).
(An intelligent SSD controller could merely update WHERE the data are moved, rather than moving them, each time it detects the "read old data + wite same data + TRIM old data" pattern, economizing on both write cycles AND defrag runtime.
Some of the important file are not at the front of the partition but near the middle, obviously to cut the worst case access time in half.
Unfortunately, no file-system is perfect and even the best ones are still subject to data loss and require data-recovery now and then.
When you accidentally delete a file, have a drive crash, or are hit by a virus, fragmented files are much less likely to succeed.
It may not be able to move some files (certain open and system files typically).
The "seek" times on SSDs are minimal, on the order of microseconds.
As others have mentioned, it ensures that files are stored in contiguously numbered allocation units.
One thing of note about defragmentation is that by putting files in consecutive allocation units, data-recovery becomes easier and more likely to succeed.
It depends on the file system, the defragmenter, and the HDD/SSD.
The reason for defragmenting is that random reads are considerably slower than reading a continuous sequence of blocks/clusters off the disk.
Files which have not been accessed for a long time (typically 90 days) are sometimes actively placed closer to the end, to allow for shorter seeks for files which do get accessed.
This is because if the file is split into multiple pieces that are scattered all over the drive, then it becomes difficult (often impossible) to figure out which parts go where and belong to which files.
This is even more problematic with larger drives containing more files.
To successfully recover a fragmented file, you need to know where each part of it is and the order of them, but if you had that, you wouldn’t be needing to recover it.
That's the minimum requirement and definition of "defragmenter".
With a hard-drive, ensuring that the disk is defragmented (e.g., having it scheduled to automatically defrag whenever the system is idle—which becomes less and less of a task as it cleans things up) helps improve your chances of recovering lost files.
However, not all file systems or operating systems have the same requirements for (separate) defragmentation programs.
You're looking at the wrong places if you defrag SSDs for seeks.
One way to help with data recovery is to use a file-system that is more tolerant of corruption and contains better backup and recovery functions such as NTFS over FAT32.
It may also sequence files and directories according to usage profiles, if it collects these.
On spinning media like hard-drives, this provides faster access to the data since the platter does not have to spin more and the heads do not have to thrash back and forth.
On a second thought, yes, SSDs distribute writes across chips on purpose, because that's how they are faster, and heavy fragmentation might slow the write patterns by making the writes LESS scattered.
Since SSDs are not suited to being defragmented, you will need to weigh the risks and benefits of defragging and either wear out the drive, make regular backups, or store critical data on a hard-drive.
At a minimum it will aim to put the clusters for a single file or directory into a contiguous sequence, providing that there is space available to do this.