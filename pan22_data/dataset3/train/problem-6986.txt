4-gram: "Hell", "ello", "llo ", "lo W", "o Wo", " Wor", "Worl", "orld"
The second is n_grams which is sequences of characters.
You will now have a vector representation of each of your descriptions.
And this similarity would be captured by your features.
There are two techniques, I would recommend for this, the first is bag-of-words, don't forget to only use the stem of words since coffee should be the same as coffees.
3-gram: "Hel", "ell", "llo", "lo ", "o W", " Wo", "Wor", "orl", "rld"
It segments the Strings such that roots of words can be found, ignoring verb endings, pluralities etc...
This builds a dictionary of the words it has seen during the training phase.
This can then be used with any standard machine learning technique.
First, you will need to vectorize your descriptions.
You can solve this problem using traditional recommendation system algorithms for text.
You will build a dictionary of existing words or character sequences and then you will fill this vector with the number of occurances in your short description.
n-grams is a feature extraction technique for language based data.
You can use the Euclidean distance as a metric of similarity.
2-gram: "He", "el", "ll", "lo", "o ", " W", "Wo", "or", "rl", "ld"
This method can be considered a k-nearest neighbors (k-NN) approach.
Then for a search term like "yummy food motel", you can vectorize it as $[0, 0, 1, 0, 0, 0, 0]$, then find the most similar instance in your set to this vector.
I will show you two ways how this can be implemented, one using a simple linear search and the other using some clustering approach.
Then the resulting vector would be $[0, 0, 1, 0, 0, 0, 1]$.
Thus in your example, if we use 4-grams, truncations of the word Hello would appear to be the same.
For example: If the dictionary includes the words {car, coffee, motel, hotel, world, van, soup} and we have the description "the soup in this motel is amazing!".
Then using the word the frequency of each word in the example a vector is created.