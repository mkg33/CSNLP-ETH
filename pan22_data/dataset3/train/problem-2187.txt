I am trying to classify images to more then a 100 classes, of different sizes ranged from 300 to 4000 (mean size 1500 with std 600).
I tried to use $weights = \frac{max(sizes)}{sizes}$ for the cross entropy loss which improved the unweighted version, not by much.
If you are looking for just an alternative loss function:
Is there any standard way of handling this sort of imbalance?
Focal Loss has been shown on imagenet to help with this problem indeed.
I also thought about duplicating images such that all classes ends up to be of the same size as the larges one.
Focal loss adds a modulating factor to cross entropy loss ensuring that the negative/majority class/easy decisions not over whelm the loss due to the minority/hard classes.
I am using a pretty standard CNN where the last layer outputs a vector of length number of classes, and using pytorch's loss function CrossEntropyLoss.
Link To Focal Loss Paper:  https://arxiv.org/pdf/1708.02002.pdf
I would look into using that is it seems to be really promising.