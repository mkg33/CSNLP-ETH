You must ensure that the training set and testing set are drawn from the same distribution.
This means that 20% of the data is used for testing, this is usually pretty accurate.
$k$-fold cross validation is used for two main purposes, to tune hyper parameters and to better evaluate the performance of a model.
In short, yes the number of folds depends on the data size.
Make sure to shuffle your data, such that your folds do not contain inherent bias.
That being said, selecting $k$ is not an exact science because it's hard to estimate how well your fold represents your overall dataset.
And that both sets contain sufficient variation such that the underlining distribution is represented.
This instance does not properly represent the variation of the underlying distribution.
In both of these cases selecting $k$ depends on the same thing.
For example, if you have 10 instances in your data, 10-fold cross-validation wouldn't make sense.
In a 10-fold cross validation with only 10 instances, there would only be 1 instance in the testing set.
The number of folds is usually determined by the number of instances contained in your dataset.
This should be sufficient to reliably test your model.
However, if your dataset size increases dramatically, like if you have over 100,000 instances, it can be seen that a 10-fold cross validation would lead in folds of 10,000 instances.