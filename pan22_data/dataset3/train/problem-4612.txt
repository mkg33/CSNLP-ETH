effective_cache_size tells the PostgreSQL query planner how much system RAM is thought to be used for disk cache, and will help it choose better plans.
This can have a drastic influence on performance, so a "bigger" machine can be slower.
You might be noticing a theme here - it depends on what you're doing, and no single system spec like amount of RAM can be taken in isolation.
RAM makes less difference for write-heavy workloads, it's mostly beneficial for read-heavy workloads.
Lots of slow RAM will be worse than less fast RAM for a database that fits entirely in RAM on both systems.
Once you have enough RAM to cache the indexes and tables in heavy use, more RAM generally makes little difference.
Yes, PostgreSQL performance is influenced by the amount of RAM available to PostgreSQL, among many other factors.
There's a cost to maintaining shared_buffers, so too big a shared_buffers can slow things down.
If unset or incorrectly set the planner is less likely to pick the correct plan for a system.
NUMA (non-uniform memory architecture) machines can sometimes have performance issues related to NUMA page migration because the Linux kernel doesn't understand PostgreSQL's use of shared memory properly.
It's a balance between having enough workspace and the maintenance cost of that workspace.
A machine with tons of RAM and a pathetic disk subsystem might still be slower than one with little RAM and really fast disks - depending a lot on the workload.