If you are on Linux then you might want to give a try to this command.
When you are done you still need to rewrite all the links so they don't point at .../index.html.
The reason "Lots of JavaScript & such seems to trip it up" is probably that so many companies use content management systems (Joomla, Drupal and Wordpress) , which use those to query databases for content.
I'm looking for some way to get a 100% perfect local copy of a web page.
No matter what browser I've tried (even Chrome), when saving a web page as local files, it's never quite exact.
If that is the case, you will not get the whole page like you want.
You need to download the entire website with Httrack (you need to set it so it doesn't download external JavaScripts)... just run it, then see the directories which downloaded, run Httrack again and exclude (f.e.
I offers more flexibility to download from the Internet.
You can also specify depth for downloading a website.
In other words, all CSS/images/JavaScripts should pull down as well AND the HTML references to said content should be changed to point to a local folder.
I'm primarily on a Mac but I have access to Linux.