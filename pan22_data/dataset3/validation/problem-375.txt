If you give it data it hasn't seen before, the outcome is unpredictable or random.
Simplest example would be learning to fit a line (y=mx+c), where $\phi = \{m,c\}$ to data on a curve (quadratic polynomial).
There have been a lot of good explanations about overfitting.
Summary: Yes, both sample bias and model complexity contribute to the 'quality' of the learnt model, but they don't directly affect each other.
(2) Model complexity is in simplistic terms the number of parameters in $\phi$.
The overfitted model means that we will have more complex decision boundary if we give more variance on model.
Overfit models do a great job of describing the data you already have, but descriptive models are not necessarily predictive models.
This means there will remain a difference between the estimated parameters $\hat{\phi}$ and the optimal parameters $\phi^{*}$, regardless of the number of training epochs $n$.
If you want to predict what will come next in the sequence of numbers "2, 4, 16, 32" you can't build a model more accurate than any other if you don't make the assumption that there's an underlying pattern.
$|\phi^{*} - \hat{\phi}| \rightarrow e_{\phi} \mbox{  as }n\rightarrow \infty$, where $e_{\phi}$ is some bounding value
Consequently, over-fitted model is not good as under-fitted model.
A model that's overfit isn't really evaluating the patterns - it's simply modeling what it knows is possible and giving you the observations.
Now, if you train a model on each of the datasets, you will have N models.
The over-fitted model results in parameters that are biased to the sample instead of properly estimating the parameters for the entire population.
To find whether your model has overfitted or not, you could construct the plots mentioned in the previous posts.
Similarly, if you take the mean model and then find how much it is different from the original model that would have given the best accuracy, it wouldn't be very different at all.
That's because something called bias-variance dilema.
The thing is, not only too simple models but also complex models are likely to have dis-classified result on unseen data.
In order to generalize your results to figure out what might happen in the future, you must create a model that generalizes what's going on in your training set.
Similarly, if you had fewer than the required number of parameters, then regardless of perfectly unbiased sampling and infinite training, the final learnt model would have error.
The No Free Lunch Theorem says that no model can outperform any other model on the set of all possible instances.
If the model complexity is low, then there will remain a regression error regardless of the number of training epochs, even when $\hat{\phi}$ is approximately equal to $\phi^{*}$.
You tell the model what attributes each piece of data has and it simply remembers it and does nothing more with it.
If you have biased data, then regardless of having the correct number of parameters and infinite training, the final learnt model would have error.
But if there really is no pattern, then you're out of luck and all you can hope for is a look-up table to tell you what you know is possible.
What got me to understand the problem about overfitting was by imagining what the most overfit model possible would be.
Finally, to avoid overfitting you could regularize the model or use cross validation.
(1) Over-fitting is bad in machine learning because it is impossible to collect a truly unbiased sample of population of any data.
You are erroneously conflating two different entities: (1) bias-variance and (2) model complexity.
If you keep growing your decision tree bigger and bigger, eventually you'll wind up with a tree in which every leaf node is based on exactly one data point.
Now  find the mean model and then use the variance formula to compute how much each model varies from the mean.
You get predictive power by assuming that there is some underlying function and that if you can determine what that function is, you can predict the outcome of events.
No one seems to have posted the XKCD overfitting comic yet.
But the point of machine learning isn't to tell you what happened, it's to understand the patterns and use those patterns to predict what's going on.
If you give it a piece of data that it's seen before, it looks it up and simply regurgitates what you told it earlier.
For overfitted models, this variance will be really high.
That's why overfitting is bad and we need to fit the model somewhere in the middle.
This is because, each model would have estimated parameters which are very specific to the small dataset that we fed to it.
You've just found a backdoor way of creating a look-up table.
Let's say you have training data with you, which you divide into N parts.
$E[|y-M(\hat{\phi})|] \rightarrow e_{M} \mbox{ as } n \rightarrow \infty$, where $e_{M}$ is some regression fit error bounding value
Overfitting happens when your variance is too high and bias is too low.