The exception is S3 which would end up larger most likely since compressing a compressed file adds overheads but cannot compress.
In terms of the difference between s1 and s2, I would say that it depends on how you are most likely to use the archive in the future and how big they end up.
Really big archives are a pain to handle (moving, opening, etc) and this is likely to be more important than saving a few kB.
If you want better compression, look for newer archiving tools that have better algorithms.
I doubt that the different schemes would make a lot of difference to be honest since the compression algorithms typically only look forward a limited amount in order to control memory use.
You might however look at something like RAR which allows redundancy and split archives.
A small error in a large archive can be devastating.
Loosing one project is probably much better than loosing them all.
Additionally, when thinking of long-term storage, don't ignore "bit-rot".
You create multiple archive files each of which has built in redundancy so that you can loose a file and still recreate the original data.