This script is only minimally tested, but it has worked reasonably enough for me.
You can choose the minimum file size you want to have checked, and it can eliminate all but one.
I've written a simple python script FileDupeFinder.py that will find file duplicate files on OSX (and probably Linux/*nix) systems.
It will ignore files that are hard links of each other.
Doesn't seem to be able to create hard links or symlinks, though, which I'd want.
I personally scan for duplicates just to assess how much of my data is redundant (currently 3/50 GB, or 6% of the data), but I rarely delete anything in case it breaks the internal magic of my file chaos.
The most convenient GUI that I have found for this task is Gemini, but a script might be enough for your needs.
Right now, you pass it a directory root path and a minimum file size and it will walk the directory tree and output a list of files that it believes to be duplicates.
There is Araxis Find Duplicate Files which is pretty convenient.
You can tell it to exclude directories (right now it just excludes Backups.backupdb, as this script isn't ready to handle Time Machine wackiness) and tell it not to span filesystems (though these aren't currently paramaterized, they are easy enough to set in the script itself.
i recommended dupeGuru by Hardcoded Software, it's Open Source Fairware and free to use