The authors use this conjecture to show that a number of other fine-grained complexity assumptions are unlikely to be reducible to each other.
(For this approach, a fast co-nondeterministic algorithm for satisfibility is sufficient, as you are postulating.)
It would not directly contradict ETH because that's for deterministic algorithms.
If you use Williams' approach, tightened here, you get a lower bound of $n^{1+\Omega(1)}$ for a function on $n$ bits in the class E$^{NP}$.
Since you are making a pretty strong assumption this could follow from the seminal work by Impagliazzo, Kabanets, and Wigderson, I haven't checked.
Specifically, the latter paper shows that a lower bound for size $s$ follows from a satisfiability algorithm for circuits of size $O(s)$, which we can translate to a 3SAT instance with $O(s)$ variables by Cook-Levin.
Are there interesting consequences of such containment?
NSETH is of course an even stronger assumption than "NETH" which you ask about, and still appears to be consistent with everything we know so far.
introduces the Nondeterministic Strong Exponential Time Hypothesis (NSETH) which makes the conjecture that there are no $\text{NTIME}[2^{(1-\varepsilon) n}]$ algorithms for DNF-TAUT.
Would it contradict the Exponential Time Hypothesis?
Is it possible that $\overline{SAT} \in NTIME(\exp(n^{0.9}))$ ?