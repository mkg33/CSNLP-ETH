https://developer.mozilla.org/en-US/docs/Tools/Web_Console
Some web pages fetch images only when the page is scrolled.
Using the "Webpage, Complete" option via the Save option of the browser should get most of the  page elements in a folder with the same name as the web page title.
Wireshark is a free program for Windows, Mac OS, and Linux that can monitor and record all network packets to and from your machine including browser and other network traffic.
Packets can be viewed and filtered/unfiltered during active capture as well as after you stop, including if you save the capture after previously saving it.
I know of web page "leaching" tools that would get all static files for offline access but they may be ineffective in cases where content of the web page is fetched dynamically
For dynamically generated web pages, you may get a different set of files related to the main web page, with each viewing.
To use in its simplest form simply select your network interface and start capture.
Firefox and other browsers have web developer tools.
For such conditional rendering of page elements, I doubt if any tool exists that would get ALL associated files.