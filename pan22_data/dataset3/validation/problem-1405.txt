When features are on the various scales, it is also fine.
Therefore, as a rule of thumb, SVM is hardly scalable beyond 10^5 points.
Large number of features (homogeneous features with meaningful distance, pixel of image would be a perfect example) is generally not a problem.
SVM will generally perform better on linear dependencies, otherwise you need nonlinear kernel and choice of kernel may change results.
Indeed as @Ianenok, SVMs tend to be unusable beyond 10 000 data points.
However, SVMs are known to perform better on some specific datasets (images, microarray data...).
For multiclass problem you will need to reduce it into multiple binary classification problems.
It is up to you to decide if "distance" is meaningful.
For example in document classification you may have thousands, even tens of thousands of features and in any given document vector only a small fraction of these features may have a value greater than zero.
Random Forest works well with a mixture of numerical and categorical features.
It really depends what you want to achieve, what your data look like and etc.
SVM maximizes the "margin" and thus relies on the concept of "distance" between different points.
I would also try Logistic Regression - great interpretable classifier)
There are probably other differences between them, but this is what I found for my problems.
Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class.
To sum it up - the rule of thumb is try anything and compare what gives you best results/interpretation.
Also, SVM are less interpretable - for e.g if you want to explain why the classification was like it was - it will be non-trivial.
Further, min-max or other scaling is highly recommended at preprocessing step.
Also they train faster than SVM in general, but they have tendency to overfit...
So, once again, cross validation is indeed the best way to know which method performs best.
For a classification problem Random Forest gives you probability of belonging to class.
SVM models perform better on sparse data than does trees in general.
For those problems, where SVM applies, it generally performs better than Random Forest.
I would say, the choice depends very much on what data you have and what is your purpose.
If you have data with $n$ points and $m$ features, an intermediate step in SVM is constructing an $n\times n$ matrix (think about memory requirements for storage) by calculating $n^2$ dot products (computational complexity).
Besides, the way algorithms are implemented (and for theoretical reasons) random forests are usually much faster than (non linear) SVMs.
From Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?
As a consequence, one-hot encoding for categorical features is a must-do.
Decision trees have better interpretability, they work faster and if you have categorical/numerical variables its fine, moreover: non-linear dependencies are handled well (given N large enough).
SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability.
SVM gives you "support vectors", that is points in each class closest to the boundary between classes.
random forests are more likely to achieve a better performance than random forests.
Roughly speaking, with Random Forest you can use data as they are.
They may be of interest by themselves for interpretation.