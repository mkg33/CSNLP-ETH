DVI-D is constrained by the spec of the standard, altouugh dual-link varieties will go up to 2560x1600 and some 2x dual-link screens were produced at one point that supported a resolution of 3840x2400 if you had a compatible video card.
SVGA signals could go up to that resolution, depending on the speed of the DAC in the video card.
A dual link DVI-D/I cable has two sets of data and can be used to run larger monitors such as the HP LP3065 or Apple 30" display (at 2560x1600).
Since DVI-D is all digital and DVI-I is digital and analog, I am wondering if DVI-I sacrifices any digital quality in order to also support analog.
DVI-I has two sets of signals: a digital signal and an analogue one that is in the same format as VGA.
If you have a monitor and card with a DVI-I connector then the monitor can use the analogue or digital signal.
SVGA will support nominally higher resolutions than DVI-D, which maxes out at 1920x1200 in its single link varieties.
A DVI-VGA cable just hooks up the analogue signals.
In theory it is supposed to prefer the digital signal if it is available.
If it is using the analog signal then it will be equivalent to the VGA signal your card would generate if it had a VGA connector.
I found this to be very flaky in practice with KVM switches and had a lot of trouble getting DVI-D (and USB for that matter) to work with Gefen, Avocent and Startech KVM switches before I gave up.
Note that this is something different from the DMS59-Dual DVI adaptor cables used with some video cards.
There is also a single and dual link variant of DVI-I, but most cables are dual link.
In answer to your question, if the monitor is using the digital signal then it is just the DVI signal.
You can run a VGA monitor from a video card that supports this type of signal.
Some higher resolution CRT monitors would do 2048x1536 (not counting exotic monochrome medical imaging systems).