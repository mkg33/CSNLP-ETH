I know theoretically that it is possible, but I've never encountered it practically.
The basic idea is to break a task up into smaller task and have every node process one of the small task.
My question is, how is he able to do that, whether it is a capability in the software he uses, or can it be applied it to any task?
I heard a 3D modeller say that when he renders a scene he uses multiple machines to do it because simply the hardware is not powerful enough and he is using the collective computational power of more than one machine.
When every node is done, the final result is assembled.
You then break up the image into smaller areas and have every node render one of the areas.
When every node is done, you assemble the full picture and present it as a result.
In the case of rendering, when you render an image, you're basically trying to determine the color of every pixel in the image through raytracing.
All the nodes need is a copy of the scene and to be told which areas of the final image they should produce.