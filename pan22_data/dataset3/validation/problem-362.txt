Also, the propagation delay may vary because of things like congestion.
Also, remember that when measuring "delay" between two nodes, you are really measuring the latency of the link, plus the processing time of whatever operating system the node happens to be running in it's control plane, even when you are pinging an interface address on a device.
I have seen numerous examples over many years where pinging a router or switch returns a higher latency than pinging a device that is physically beyond it.
Eg: If I ping the device I'm attached to I get an RTT of 5ms, but pinging a host on the other side of a campus reachable via the same device still reports an RTT of 1ms or less.
It is an acceptable value to use, however there would need to be significant variance for it to be useable across a large network.
In fact, the mode (most commonly occurring RTT) would probably be a better number to use than the average as this would significantly reduce the effect of variance due to congestion and QoS as Ron describes.
Most people do this with ping which is a very poor measure of the propagation delay.
In graph theory, the length of an edge is mostly represented by a numerical value to indicate the distance between the two end nodes.
I think the best you can do is come up with an average propagation delay, and understand that it could be much faster or slower than that based on other factors.
In fact, I see this sometimes in video tutorials but I wonder if it's really a an accepting value to be used or not.
Thus, in the field of networking, is it acceptable if someone uses the propagation delay between the two nodes to represent the value of the link between them?
Eg: you might want to take an average over a period of time for each device and multiply it by 10 or 100 in order to differentiate a lot of devices that may have a variance of less than 5ms between them.
It depends on how the propagation delay is measured.