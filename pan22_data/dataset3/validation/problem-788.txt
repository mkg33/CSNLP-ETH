I have chosen the values 999, 1000, and 1001 because 1000 is a point where the number of digits steps up.
The implication is clear: using the LOG function would give me an incorrect count for some values, so I should use the LOG10 function.
Why does the floor function produce different values from their input?
The value of all three expressions is visibly the same, and looks correct.
There are several ways to construct an equivalent expression using built-in functions, but some of them give incorrect results.
If we added 1 to each value, we would have a count of 3 digits in 999 and a count of 4 digits in 1001.
But the value of each log expression itself is identical and correct.
The values of each expression for 999 and 1001 are equal and correct.
If we added 1 to each value, we would have a count of 4 digits in 1000 if we used the LOG10 function, and a count of 3 digits if we used the LOG function in either form.
You can take the floor of each log in the previous example using a query like this:
If you prefer one function for all logarithms you can use the LOG function and specify base 10 with the second parameter.
Mathematically, the number of decimal digits in an integer n is 1 + floor(log(n)), where log is the common logarithm (base 10).
Prior to 2012, SQL Server's LOG function would calculate only the natural log (base e=2.71828...).
I'm trying to find a reliable, efficient expression to calculate how many decimal digits it takes to write a positive integer.
You can calculate the log to an arbitrary base of a number by dividing the natural logarithm of the number by the natural logarithm of the base.
The following query calculates all three expressions for some example values:
The simplest way to calculate the common logarithm is to use the LOG10 function.