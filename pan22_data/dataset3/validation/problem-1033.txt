Something in the script or how its set up is causing the script to fail.
It is easy to forget who is supposed to free memory that comes out of a function.
If this doesn't solve your problem, and/or you want a way to monitor the process over a long period of time, have a look at plog.
My best guess is a memory leak or a variable overflowing.
Why are you increasing a variable ever 30 seconds or so?
Especially with C code involved, my mind jumps to memory leaks.
Just some knowledge that I ran by a few weeks ago.
The other stuff is input/output redirection (the output you probably actually want to send to a log instead of /dev/null).
Not quite an answer but a guess, since this is a pretty vague question.
If/when you are starting it from a login (including ssh), simply back-grounding something is not sturdy enough.
I have seen people with similar problems when it comes to creating a hydroponics system.
When you run a program for an extended period of time, I have seen that in a few cases, this can be the issue.
See man setsid -- this ensures the forked process will be re-parented by init.
It turned out that there were variables that were being incremented that went past the memory allocation for its type.
I'm presuming something you are starting with the intent of having it run for years is also intended to outlast the login session which started it -- unless you start it via the init system, which you don't refer to in the question.
Unless you post it, anything we say is just a guess.
I think (as a work-around) they used the "long long" type or an "unsigned int" so that the increment values can store a larger number before it crashes.
Wouldn't it be easier just to calculate the value when you need it?