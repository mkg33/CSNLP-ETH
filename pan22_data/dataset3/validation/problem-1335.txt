Asking what their plans are if all their paper files go up in flames is an example here.
Aiming for as close to "none and none" as you can reasonably get is good, but you'll need to be able to recognise when the point of diminishing returns gets in.
Something that I have read about is agencies sharing the cost of maintaining a hot site for when the big one does hit.
It may seem obvious, but to go along with the offsite documentation above, make sure you have offsite (preferably out of the region) backups.
One approach to use on customers, if you find that they're reluctant to get involved, is to ask them DR questions from a non-IT angle.
In an ideal world the answers would be "none and none", but a DR scenario is an exceptional circumstance.
An approach focussed around scale and spread of the disaster is more likely to yield results.
This could be an online storage service or a place to take tapes to.
With DR the basic things are your RTOs (Recovery Time Objectives) and RPOs (Recovery Point Objectives), which roughly translate as "how much time is acceptable to have to spend getting it back, and how much data can we afford to lose".
If you lose a server you need to get it back, irrespective of whether it was hit by lightning, accidentally formatted, or whatever.
These two factors might be different at different times of the year, and different on different systems.
With DR the incident has already happened, and specifics of what it was are less relevant (except perhaps in terms of affecting availability of DR facilities).
http://sqlserverpedia.com/blog/sql-server-backup-and-restore/disaster-recovery-basics-tutorial/
I like the more well-rounded approach; it's tempting to list out the events that can lead to a DR scenario, but these really belong more to a risk ananlysis/mitigation exercise.
It's no good having a beautiful DR plan that looks great on paper but that doesn't meet it's objectives.
They enact plans for restoring both companies' mission critical to the hot site using virtualization and such, and then share staffing on the level of make-sure-all-the-lights-are-blinking.
Its all good to have your backup in a safety deposit box at the bank, until your bank is under liquid hot magma (/Dr.
This can help with getting them more involved in the broader DR thing, and can feed useful info into your own plans.
I say preferably out of the region because I come from an area where we don't have many natural disasters annually, but, if/when we do have one, it is on a regional scale with mass destruction (earthquakes, volcanoes).
For books there's Disaster Recovery Planning by Jon William Toigo, now in it's 3rd edition, with a 4th edition blook (blog+book) on the horizon.
Make sure you have off-line/remote documentation of your network
I understand there's bunch out there to follow, but some of us have specific priorities when it comes to recovery.
These really should be driven by your customers, but since you're starting from the IT angle you can make best guesses, but be prepared to adjust up or down as required.
Finally testing your plan regularly is crucial to success.
If we add our ideas we could create a nice wiki from this post once everyone has had added their own ideas.
Here is a link from SQLServerPedia that gives out basics of DR.