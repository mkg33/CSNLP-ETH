IMO, the duplication of data will lead to change of distribution of overall data.
So you can get back the result on happiness of having a million more/less people.
Your data is country wide, duplicating data by any measure will only skew your results and produce fake "accuracy".
By assigning 300 variables the same values, you're effectively saying that each of these million people have the average economic output (i.e., if you're using GDPpercapita) for all the people in your dataset.
Now coming to regression part, it will depend what kind of techniques you are using for performing regression.
Question: While analyzing country happiness data via OLS regression, should I duplicate observations based on country population?
I don`t know exactly what you are modelling but let's say an example could be predicting happiness based on economic data (GDP, etc.
the same entries over multiple years for each country).
The standard practice is remove the duplicate data.
That's a pretty misleading understanding of the distribution of that variable, and will lead to poor analysis.
X: GDPPerCapita, InfantMortalityRatePer1000, UnemploymentPercent, GiniIndex, etc.
Example: If duplicating per million, the U.S. would have 327 observations and Denmark would have 6.
I see what you're doing, and understand the thinking behind it, but don't think it makes sense with this problem.
If you do not have enough countries/records for modelling maybe think about enriching the data set with a time series (e.g.
In this case each country is exactly one record and duplicating data would be absolutely wrong here.
Multiplying by 327 will fake model accuracy and absolutely overfit to boot.
If you're looking to do a regression where those are incorporated into your analyses, using the unscaled population data as one of your features will include that information as one of the coefficients you return.
There aren't 327 countries with the statistics and happiness of the USA, there is only one.
Function to duplicate observations based on population: