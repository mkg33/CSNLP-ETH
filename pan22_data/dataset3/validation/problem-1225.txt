3) control of the variance of the data: if you have skewed features, and you don't transform them, you risk that the model will simply ignore the elements in the tail of the distributions.
feature transformations (like Random Forest), in general it's a good practice to transform the features in order to have better performance in a ML model.
In practice, this means that the numerical behavior in the range [0.0, 1.0] is not the same of the range [1'000'000.0, 1'000'001.0].
This can lead to other instabilities: some values of learning rate (LR) can be too small for one feature (and so the convergence will be slow) but too big for the second feature (and so you jump over the optimal values).
So they use a representation based on Floating Point arithmetic.
And so, at the end of the training process you will have a sub-optimal model.
And in some cases, the tails are much more informative than the bulk of the distributions.
1) Numerical stability: computers cannot represent every number, because the electronic which make them exist deals with binaries (zeros and ones).
2) Control of the gradient: imagine that you have a feature that spans in a range [-1, 1], and another one that spans in a range [-1'000'000, 1'000'000]: the weights associated to the first feature are much more sensitive to small variations, and so their gradient will become much more variable in the direction described by that feature.
So having two features that have very different scales can lead to numerical instability, and finally to a model unable to learn anything.