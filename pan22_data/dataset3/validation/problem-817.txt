Boosting trees is about building multiple decision trees.
For some machine learning methods it is recommended to use feature normalization to use features that are on the same scale, especially for distance based methods like k-means or when using regularization.
Other techniques often require data normalization....
My rather limited experience with scaling of features suggests that it has virtually no impact on xgboost results.
However, in my experience, boosting tree regression works less well when I use normalized features, for some strange reason.
I suppose by normalisation you mean subtracting the mean and then dividing by standard deviation.
How is your experience using feature normalization with boosted trees does it in general improve our models?
According to my understanding of xgboost, the correctly performed scaling should have no impact on the performance.
Decision tree doesn't require feature normalization, that's because the model only needs the absolute values for branching.
However, it's always a good idea to normalize your features because:
I suggest you double check your implementation or provide more details on how you do it, preferably with including a reproducible example.
If you calculated the statistics based on entire dataset (including holdout) you would get data leakage, which might indeed, at least theoretically, degrade the performance on holdout.