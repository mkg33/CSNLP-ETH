Or is there another reason why congestion control of TCP would decrease the performance of this setup?
Transmission Control Protocol (TCP) uses a network congestion-avoidance algorithm but this does not necessarily result in equal division of link bandwidth.
I was wondering why the performance would decrease when streaming a high quality video to my iphone over a long distance, when you use an end-end congestion control with TCP?
http://intronetworks.cs.luc.edu/current/html/dynamics.html
All else being equal, longer distance means higher RTT and therefore less bandwidth.
I was thinking, TCP has bandwidth control and divides the bandwidth equally over the current connections.
In case of adaptive bitrate streaming, this quality may change mid stream as the network load changes.
TCP bandwidth sharing has a well known bias towards favoring streams with low RTTs.
This is one benefit of a CDN (they reduce RTT through geographic diversity).
The codec, the format, and the underlying network determine the quality of the video played out to your viewport/monitor.
So, when a video travels over a long distance, there will probably be a connection where there is too much bandwidth used and therefore the streamed video will lose some of its data.
I'll focus on the long-distance with TCP part of your question and specifically how you say "divide the bandwidth equally".